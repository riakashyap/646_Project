# -*- coding: utf-8 -*-
"""646_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Utm3P7OXdrl4wzOTeXaqAb1jD3UaigMS
"""

# Clean out partial builds
!pip uninstall -y mim mmcv
!pip install -U pip setuptools wheel
!pip install "torch>=2.1" "mmcv==2.1.0" "mmdet==3.3.0" --no-cache-dir

import json

dataset_path = '/content/sample_data/1hop.json'

with open(dataset_path, 'r') as f:
    data = json.load(f)

print(f"Loaded {len(data)} entries.")
print("\nExample entry structure:\n")
print(json.dumps(data[0], indent=2)[:1000])  # Print first entry (truncated)

import json
import os
from PIL import Image
from torch.utils.data import Dataset
from torchvision import transforms

class OneHopDataset(Dataset):
    def __init__(self, json_path, img_dir, transform=None):
        self.img_dir = img_dir
        self.transform = transform or transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor()
        ])

        # Load JSON
        with open(json_path, 'r') as f:
            self.data = json.load(f)

        # Filter or validate entries if needed
        print(f"Loaded {len(self.data)} entries from {json_path}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # --- Parse fields ---
        question = item.get("question", "")
        answer = item.get("answer", "")
        image_filename = item.get("image", None)

        # --- Load image if available ---
        if image_filename:
            image_path = os.path.join(self.img_dir, image_filename)
            try:
                image = Image.open(image_path).convert("RGB")
            except FileNotFoundError:
                image = Image.new("RGB", (224, 224), color=(0, 0, 0))
        else:
            image = Image.new("RGB", (224, 224), color=(0, 0, 0))

        image = self.transform(image)

        return {
            "question": question,
            "answer": answer,
            "image": image
        }

from torch.utils.data import DataLoader

dataset = OneHopDataset(json_path='/content/sample_data/1hop.json', img_dir='/content/images')
loader = DataLoader(dataset, batch_size=2, shuffle=True)

for batch in loader:
    print(batch['image'].shape)
    print(batch['question'])
    break

from mmcv.transforms import Compose

pipeline = Compose([
    dict(type='Resize', scale=(1333, 800)),
    dict(type='RandomFlip', prob=0.5)
])

dataset = OneHopDataset('/content/sample_data/1hop.json', '/content/images', transform=pipeline)

import json

# Load the dataset
dataset_path = '/content/sample_data/1hop.json'
with open(dataset_path, 'r') as f:
    data = json.load(f)

# Filter: keep entries that have non-empty text evidence
filtered_data = [entry for entry in data if entry.get('text_evidence')]

print(f"Original entries: {len(data)}")
print(f"Entries with text evidence: {len(filtered_data)}")

# Save the filtered dataset
filtered_path = '/content/sample_data/1hop_text_only.json'
with open(filtered_path, 'w') as f:
    json.dump(filtered_data, f, indent=2)

!pip install sentence-transformers faiss-cpu

from sentence_transformers import SentenceTransformer
import faiss
import json
import numpy as np

# Load your filtered dataset
dataset_path = '/content/sample_data/1hop_text_only.json'
with open(dataset_path, 'r') as f:
    data = json.load(f)

# Combine all text_evidence into a single list per entry
corpus = []
corpus_ids = []  # To track which entry each text belongs to
for idx, entry in enumerate(data):
    for text in entry['text_evidence']:
        corpus.append(text)
        corpus_ids.append(idx)

# Step 1: Create embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(corpus, show_progress_bar=True, convert_to_numpy=True)

# Step 2: Build FAISS index
embedding_dim = embeddings.shape[1]
index = faiss.IndexFlatIP(embedding_dim)  # Inner product for cosine similarity
faiss.normalize_L2(embeddings)  # Normalize for cosine similarity
index.add(embeddings)

print(f"Retriever built with {len(corpus)} text evidences.")

def retrieve_topk(claim, model, index, corpus, corpus_ids, k=5):
    # Encode the query claim
    query_emb = model.encode([claim], convert_to_numpy=True)
    faiss.normalize_L2(query_emb)

    # Search FAISS index
    D, I = index.search(query_emb, k)  # D = scores, I = indices

    # Get the actual text using indices
    retrieved_texts = [corpus[i]['wiki_context'] if isinstance(corpus[i], dict) else corpus[i] for i in I[0]]
    retrieved_entry_ids = [corpus_ids[i] for i in I[0]]

    return retrieved_texts, retrieved_entry_ids, D[0]

# Example claim
claim = "Airport security measures include baggage screening and passenger checks."

top_texts, top_ids, scores = retrieve_topk(claim, model, index, corpus, corpus_ids, k=5)

# Print top texts with scores
for i, (text, score) in enumerate(zip(top_texts, scores)):
    print(f"{i+1}. Score: {score:.4f}")
    print(f"Text evidence: {text[:500]}...\n")  # show first 500 characters for readability

import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# -------------------------------
# Step 0: Load your cleaned 1hop dataset
# -------------------------------
dataset_path = '/content/sample_data/1hop.json'
with open(dataset_path, 'r') as f:
    data = json.load(f)

# Filter entries that have text evidence
data = [entry for entry in data if entry.get('text_evidence')]

print(f"Number of entries with text evidence: {len(data)}")

# -------------------------------
# Step 1: Build corpus and ID mapping
# -------------------------------
corpus = []      # actual wiki_context texts
corpus_ids = []  # corresponding text IDs (first ID in text_evidence)

for entry in data:
    for te_id in entry['text_evidence']:
        corpus.append(entry['wiki_context'])  # use full wiki_context or split if needed
        corpus_ids.append(te_id)

print(f"Corpus size: {len(corpus)}")

# -------------------------------
# Step 2: Encode texts with SentenceTransformer
# -------------------------------
model = SentenceTransformer('all-MiniLM-L6-v2')
corpus_embeddings = model.encode(corpus, convert_to_numpy=True, show_progress_bar=True)

# Normalize embeddings for cosine similarity
faiss.normalize_L2(corpus_embeddings)

# -------------------------------
# Step 3: Build FAISS index
# -------------------------------
dimension = corpus_embeddings.shape[1]
index = faiss.IndexFlatIP(dimension)  # Inner Product = cosine similarity (after normalization)
index.add(corpus_embeddings)
print(f"FAISS index built with {index.ntotal} vectors")

# -------------------------------
# Step 4: Retrieve top-k texts for a claim
# -------------------------------
def retrieve_topk(claim, model, index, corpus, corpus_ids, k=5):
    # Encode the query claim
    query_emb = model.encode([claim], convert_to_numpy=True)
    faiss.normalize_L2(query_emb)

    # Search FAISS index
    D, I = index.search(query_emb, k)  # D = similarity scores, I = indices
    retrieved_texts = [corpus[i] for i in I[0]]
    retrieved_entry_ids = [corpus_ids[i] for i in I[0]]

    return retrieved_texts, retrieved_entry_ids, D[0]

# Example usage
claim = "Airport security measures include baggage screening and passenger checks."
top_texts, top_ids, scores = retrieve_topk(claim, model, index, corpus, corpus_ids, k=5)

for i, (text, score, te_id) in enumerate(zip(top_texts, scores, top_ids)):
    print(f"{i+1}. Score: {score:.4f}, Text Evidence ID: {te_id}\n{text}\n")

from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

# 1️⃣ Embedding model for retrieval
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# 2️⃣ LLM model for reasoning
llm_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(llm_name)
llm_model = AutoModelForSeq2SeqLM.from_pretrained(llm_name)
llm_pipe = pipeline("text2text-generation", model=llm_model, tokenizer=tokenizer)

# Now retrieve top-k evidence using the embedding model
claim = "Airport security measures include baggage screening and passenger checks."
top_texts, top_ids, scores = retrieve_topk(claim, embedding_model, index, corpus, corpus_ids, k=5)

# Pass retrieved evidence to LLM
prompt = build_prompt(claim, top_texts)
output = llm_pipe(prompt, max_length=256, do_sample=False)
print(output[0]['generated_text'])