{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e072dbc",
   "metadata": {},
   "source": [
    "# MADR prompt evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bf871",
   "metadata": {},
   "source": [
    "This notebook is inspired by [this research](https://cacm.acm.org/research/from-prompt-engineering-to-prompt-science-with-humans-in-the-loop/) on prompt engineering with humans in the loop.\n",
    "\n",
    "We are faced with the issue: the existing prompts in the MADR paper are ill-defined and intended for a different complexity model. While fine-tuning is an enticing option, we lack a dataset of questions, evidence, veracity claim, and target feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3384cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822d5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_jupyter_notebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        return shell == 'ZMQInteractiveShell'\n",
    "    except NameError:\n",
    "        return False\n",
    "IN_JUPYTER = in_jupyter_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581f57b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_clients import LlamaCppClient\n",
    "from src import config\n",
    "from src.madr import run_madr\n",
    "from src.parsers import parse_ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41daea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dir = config.PROMPTS_DIR / \"custom\"\n",
    "client = LlamaCppClient(prompts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b7a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corag_run = None\n",
    "relative_path = os.path.join('assets', '20251123T192328--metrics__100.json')\n",
    "with open(relative_path, 'r', encoding='utf-8') as f:\n",
    "    corag_run = json.load(f)[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc4d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e91e1d",
   "metadata": {},
   "source": [
    "## An initial codebook for Debater 1, 2, Cross-Agent, Judge, Refiner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3bf95c",
   "metadata": {},
   "source": [
    "A 'codebook' (or criteria) in the above research encapsulates the desired outcomes for generation. It systematically allows any human-in-the-loop to quickly and methodically conclude when a generated response does or does not meet the specifications.\n",
    "\n",
    "The starting criteria, validated by multiple researchers for each model, is defined below:\n",
    "\n",
    "1. **Debater 1, given an explanation and evidence:**\n",
    "- Does distinguish between the following types of errors in the explanation:\n",
    "  1) misrepresentations of factual details in the evidence\n",
    "  2) introduces or misrepresents events not present in the evidence\n",
    "  3) introduces or misrepresents noun phrases in the evidence, changing semantic meaning\n",
    "  4) logic/reasoning inconsistencies in the explanation\n",
    "  5) includes information not relevant to the claim/evidence or beyond what the evidence supports\n",
    "  6) does not adequately justify its position\n",
    "- Provides actionable feedback, rather than attempting to correct errors in the explanation to align with evidence.\n",
    "- ONLY considers errors in the explanation, not the claim or Q/A pairs.\n",
    "- Identifies ALL such errors\n",
    "- Does NOT output non-existent errors.\n",
    "- Does NOT merely restate the types of errors without context.\n",
    "- Does NOT merely restate the types of errors without context.\n",
    "- Does NOT merely restate the types of errors without context.\n",
    "- Does NOT merely restate the types of errors without context.\n",
    "- Does NOT merely restate the types of errors without context.\n",
    "- Labels all errors with the typology.\n",
    "- Answers tersely\n",
    "\n",
    "*note, it is NOT a requirement the model names the error types.*\n",
    "\n",
    "2. **Debater 2, given an explanation and evidence:**\n",
    "- Identifies general weaknesses which reduce the faithfulness of an explanation to the claim/evidence.\n",
    "- Generates explanations for why identified issues are unfaithful:\n",
    "  1) Factually inaccurate\n",
    "  2) logically inaccurate\n",
    "  3) irrelevant\n",
    "  4) incoherent\n",
    "  5) incomplete\n",
    "- Provides actionable feedback, rather than attempting to correct errors in the explanation to align with evidence.\n",
    "- ONLY considers errors in the explanation, not the claim or Q/A pairs.\n",
    "- Does not include a restatement of the explanation\n",
    "- Avoids logical assumptions about the claim\n",
    "- Considers the evidence as the source of ground truth\n",
    "- Does provide actionable feedback\n",
    "- Identifies ALL such errors\n",
    "- Does NOT output non-existent errors.\n",
    "- Answers tersely\n",
    "\n",
    "3. **Cross-Agent, given the original claim, evidence, explanation, and a primary and secondary set of feedback:**\n",
    "- uses the claim, evidence, and explanation to:\n",
    "  1) recognize invalid feedback in the primary feedback, and incorporate corrections from the secondary feedback.\n",
    "  2) does NOT include invalid feedback from the secondary feedback.\n",
    "- outputs ONLY the modified primary feedback\n",
    "- does NOT merely copy the secondary feedback\n",
    "\n",
    "*note, the input to this agent is up to debate. I am making the assumption that only including the explanation rather than the full claim and Q/A pairs balances the agent's ability to refine the feedback given a source of ground truth while keeping the prompt context small.*\n",
    "\n",
    "4. **Judge, given two sets of feedback:**\n",
    "- identifies major discrepancies between the input feedback:\n",
    "  1) identifies different errors or issues\n",
    "  2) provides inconsistent error descriptions\n",
    "  3) whether any suggested fixes are not equivalent\n",
    "- if discrepancies exist, output contains the word 'FALSE'\n",
    "- else, output contains the word 'TRUE'\n",
    "\n",
    "5. **Refinement Agent, given two sets of feedback and an explanation:**\n",
    "- rewrites the explanation to align with both sets of feedback\n",
    "- Does NOT output anything other than the refined explanation\n",
    "- Does NOT remove the component containing the verdict in the original explanation, but can 'flip' it, i.e.: true -> false."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba9d78",
   "metadata": {},
   "source": [
    "## Initial Custom Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d3757f",
   "metadata": {},
   "source": [
    "Because it is already established the original prompts do not meet these requirements, we will start by creating our initial custom prompts.\n",
    "\n",
    "To do this, we simply feed the codebook itself into an LLM to generate the prompt. I will use this boilerplate:\n",
    "\n",
    "> You are tasked with taking a set of requirements and phrasing it as a concise prompt for an LLM. In this initial generation, the codebook serves as the primary material for construction, while a previous prompt is provided purely to fill in any gaps left by the codebook. Extra output besides the prompt is unnecessary and unwarranted.\n",
    "\n",
    "For further refinement:\n",
    "\n",
    "> You are tasked with taking a set of requirements and phrasing it as a concise prompt for an LLM. In this initial generation, the codebook serves as the primary material for construction. A repeated point stresses emphasis. Extra output besides the prompt is unnecessary and unwarranted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "090aa541",
   "metadata": {},
   "outputs": [],
   "source": [
    "madr_init_fb1_system_prompt = \"\"\"Evaluate the following explanation against the given evidence by identifying all errors it contains, using these labels:\n",
    "\n",
    "- misrepresented factual details;\n",
    "- introduced/misrepresented events not in evidence;\n",
    "- introduced/misrepresented noun phrases altering meaning;\n",
    "- logic/reasoning inconsistencies;\n",
    "- irrelevant or unsupported information;\n",
    "- inadequate justification.\n",
    "\n",
    "Only analyze the explanation for errors (not the claim or Q/A). Do not invent errors. Provide actionable feedback without rewriting or correcting the explanation. Be terse and avoid restating the error types without context.\"\"\"\n",
    "\n",
    "madr_init_fb2_system_prompt = \"\"\"Given the following claim, explanation, and QA-pair evidence, identify all weaknesses in the explanation that reduce the faithfulness to the claim/evidence.\n",
    "\n",
    "For each problematic sentence in the explanation, quote it (inline, no need to restate the full explanation), explain why it is unfaithful (factually inaccurate, logically inaccurate, irrelevant, incoherent, or incomplete), and provide actional feedback without correcting or rewriting the explanation.\n",
    "\n",
    "Only analyze the explanation for errors (not the claim or Q/A). Only consider the available evidence as the ground truth. Do not invent issues. Keep responses terse.\"\"\"\n",
    "\n",
    "madr_cross_fb_system_prompt = \"\"\"Given the following explanation plus primary and secondary feedback, revise the primary feedback using the explanation as ground truth.\n",
    "\n",
    "Remove any invalid points in the primary feedback, add only valid corrections supported by the secondary feedback, and ignore invalid secondary feedback.\n",
    "\n",
    "Do not copy secondary feedback verbatim. Output only the corrected primary feedback.\"\"\"\n",
    "\n",
    "madr_cross_fb_user_prompt = \"\"\"Claim: {}\n",
    "\n",
    "Question-Answer Pairs:\n",
    "{}\n",
    "\n",
    "Explanation:\n",
    "{}\n",
    "\n",
    "Primary feedback:\n",
    "{}\n",
    "\n",
    "Secondary feedback:\n",
    "{}\"\"\"\n",
    "\n",
    "madr_judge_system_prompt = \"\"\"Given the following sets of feedback, compare them for major discrepancies: whether they identify idfferent errors, give inconsistent error descriptions, or propose nonequivalent fixes.\n",
    "\n",
    "If any discrepancy exists, output FALSE; otherwise output TRUE. Output only this word.\"\"\"\n",
    "\n",
    "\n",
    "madr_revise_system_prompt = \"\"\"Given the following explanation and two sets of feedback, rewrite the explanation so it aligns with both feedback sets.\n",
    "\n",
    "Keep the verdict component but allow flipping its valud. Output only the refined explanation and nothing else.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b93230d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_prompt(key, system_prompt_str):\n",
    "    client._prompts[key] = (\n",
    "        client._prompts[key][0],\n",
    "        system_prompt_str\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb3d8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_prompt(\"madr_init_fb1\", madr_init_fb1_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8d31088",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Claim: {}\\n\\nQuestion-Answer Pairs:\\n{}\\n\\nExplanation: {}\\n\\nFeedback:',\n",
       " 'Evaluate the following explanation against the given evidence by identifying all errors it contains, using these labels:\\n\\n- misrepresented factual details;\\n- introduced/misrepresented events not in evidence;\\n- introduced/misrepresented noun phrases altering meaning;\\n- logic/reasoning inconsistencies;\\n- irrelevant or unsupported information;\\n- inadequate justification.\\n\\nOnly analyze the explanation for errors (not the claim or Q/A). Do not invent errors. Provide actionable feedback without rewriting or correcting the explanation. Be terse and avoid restating the error types without context.')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client._prompts[\"madr_init_fb1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59cef96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_prompt(\"madr_init_fb2\", madr_init_fb2_system_prompt)\n",
    "replace_prompt(\"madr_judge\", madr_judge_system_prompt)\n",
    "replace_prompt(\"madr_revise\", madr_revise_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27d60f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "client._prompts[\"madr_cross_fb\"] = (\n",
    "    madr_cross_fb_user_prompt,\n",
    "    madr_cross_fb_system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921dac7",
   "metadata": {},
   "source": [
    "## Refining Base MADR Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45973ad8",
   "metadata": {},
   "source": [
    "I have a set of 100 claims run through the baseline pipeline. When evaluating the MADR pipeline *I will assume that the answers generated by the CoRAG answering agent are always factual.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6793a1d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim_id': 0,\n",
       "  'claim': 'In 2008, Jeff Ament released a solo record.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [[\"When was Jeff Ament's solo record released?\",\n",
       "    'September 16, 2008.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'True.  \\nInconclusive'},\n",
       " {'claim_id': 1,\n",
       "  'claim': 'Franklin D. Roosevelt had a family that originated in New York.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [[\"What is the origin of Franklin D. Roosevelt's family?\",\n",
       "    \"Franklin D. Roosevelt's family originated from Dutch settlers in New York, with his ancestors tracing back to the 17th century. His direct lineage includes prominent figures such as his father, James Roosevelt, and his mother, Eleanor Roosevelt, who were part of influential American families.\"]],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'True. The question-answer pairs fully confirm the claim that Franklin D. Roosevelt had a family that originated in New York.'},\n",
       " {'claim_id': 2,\n",
       "  'claim': 'A phylogenetic tree is a branching diagram.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [['What is the primary purpose of a phylogenetic tree according to the statement?',\n",
       "    'The primary purpose of a phylogenetic tree is to represent a hypothesis about the evolutionary ancestry of a set of genes, species, or other taxa.'],\n",
       "   ['What type of diagram is a phylogenetic tree described as in the statement?',\n",
       "    'A phylogenetic tree is described as a branching diagram or \"tree\" in the statement.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'The question-answer pairs fully confirm the claim.  \\nTrue'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corag_run[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f337828",
   "metadata": {},
   "source": [
    "### Debater 1 Freespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56cf0dc",
   "metadata": {},
   "source": [
    "This marks our first iteration of refining the codebook. To evaluate the debater, we need a reasonable number of responses. In this case, I will generate 10, for 5 random claims of each type (TRUE/FALSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a52daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_sample(data, count=5):\n",
    "    supports = [i for i in data if i[\"true_label\"] == \"SUPPORTS\"]\n",
    "    refutes = [i for i in data if i[\"true_label\"] == \"REFUTES\"]\n",
    "\n",
    "    selection_supports = random.sample(supports, count)\n",
    "    selection_refutes = random.sample(refutes, count)\n",
    "\n",
    "    return selection_supports + selection_refutes\n",
    "sample = select_random_sample(corag_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81fd0b",
   "metadata": {},
   "source": [
    "Now I'll generate and cache the debater's responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "366129a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate():\n",
    "    for c in sample:\n",
    "        claim = c[\"claim\"]\n",
    "        qa_pairs = c[\"qa_pairs\"]\n",
    "        verdict_raw = c[\"verdict_raw\"]\n",
    "        fb = client.send_prompt(\"madr_init_fb1\", [claim, qa_pairs, verdict_raw])\n",
    "        c[\"r\"] = fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb724ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim_id': 40,\n",
       "  'claim': 'Jun Ji-hyun is in the South Korean film called Windstruck.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [['Is Jun Ji-hyun featured in the South Korean film \"Windstruck\"?',\n",
       "    'Answer: Yes.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'True.',\n",
       "  'r': '- inadequate justification.'},\n",
       " {'claim_id': 7,\n",
       "  'claim': 'A subtype of anti-nuclear antibodies are anti-Ro antibodies.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [['Is the statement accurate that a subtype of anti-nuclear antibodies specifically includes anti-Ro antibodies?',\n",
       "    'Answer: Yes.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'True.  \\nInconclusive',\n",
       "  'r': '- misrepresented factual details: The explanation labels the statement as \"Inconclusive\" but the answer is \"Yes,\" indicating the statement is accurate.  \\n- logic/reasoning inconsistencies: The explanation provides no justification for its \"Inconclusive\" rating when the answer is explicitly \"Yes.\"  \\n- inadequate justification: The explanation fails to provide any reasoning or evidence supporting its \"Inconclusive\" assessment.'},\n",
       " {'claim_id': 1,\n",
       "  'claim': 'Franklin D. Roosevelt had a family that originated in New York.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [[\"What is the origin of Franklin D. Roosevelt's family?\",\n",
       "    \"Franklin D. Roosevelt's family originated from Dutch settlers in New York, with his ancestors tracing back to the 17th century. His direct lineage includes prominent figures such as his father, James Roosevelt, and his mother, Eleanor Roosevelt, who were part of influential American families.\"]],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'True. The question-answer pairs fully confirm the claim that Franklin D. Roosevelt had a family that originated in New York.',\n",
       "  'r': '- misrepresented factual details: The explanation assumes the question-answer pair fully confirms the claim without verifying the accuracy of the provided information. The answer mentions Dutch settlers and 17th-century origins, which may not be supported by evidence in the question-answer pairs themselves.  \\n- inadequate justification: The explanation does not provide evidence or reasoning based on the question-answer pairs to support the assertion that the claim is confirmed.  \\n- introduced/misrepresented events not in evidence: The explanation infers the origin of the family from Dutch settlers in New York without explicit confirmation from the provided question-answer pairs.'},\n",
       " {'claim_id': 47,\n",
       "  'claim': 'Taal was romantic.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'NOT ENOUGH INFO',\n",
       "  'qa_pairs': [['What historical evidence supports the assertion that Taal was romantic?',\n",
       "    'question too specific'],\n",
       "   ['When did Taal exhibit romantic behavior?', 'question too specific'],\n",
       "   ['Who is considered to have been romantic in the claim about Taal?',\n",
       "    'question too specific']],\n",
       "  'verdict_int': 2,\n",
       "  'verdict_raw': 'Inconclusive',\n",
       "  'r': '- misrepresented factual details: Explanation labels the response as \"Inconclusive\" without providing specific reasoning based on the evidence.  \\n- inadequate justification: The feedback does not explain why the response is inconclusive, failing to address the claim or the evidence provided.  \\n- logic/reasoning inconsistencies: The explanation does not engage with the specific questions posed, leading to an unsatisfactory conclusion.'},\n",
       " {'claim_id': 17,\n",
       "  'claim': 'Bill Cosby was a comedian who did stand-up.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'NOT ENOUGH INFO',\n",
       "  'qa_pairs': [['Who is considered a comedian who did stand-up according to the statement?',\n",
       "    'Gabriel Iglesias is considered a comedian who did stand-up.'],\n",
       "   ['When did Bill Cosby start performing stand-up comedy?',\n",
       "    'The provided sources do not specify when Bill Cosby started performing stand-up comedy. Therefore, the answer is \"question too specific.\"']],\n",
       "  'verdict_int': 2,\n",
       "  'verdict_raw': \"The question-answer pairs do not fully confirm or disprove the claim as they only mention Gabriel Iglesias as a stand-up comedian and lack specific information about Bill Cosby's start in stand-up comedy.  \\nInconclusive\",\n",
       "  'r': '- misrepresented factual details: The explanation misrepresents the evidence by implying the question-answer pairs \"only mention Gabriel Iglesias,\" when the first pair actually asks about a comedian who did stand-up and cites Gabriel Iglesias as the answer, while the second pair is about Bill Cosby.  \\n- logic/reasoning inconsistencies: The explanation concludes \"Inconclusive\" without properly addressing the claim that Bill Cosby was a stand-up comedian, instead focusing on the limitations of the question-answer pairs.  \\n- irrelevant or unsupported information: The explanation introduces the term \"Inconclusive\" as a conclusion without clearly explaining why the evidence is inconclusive in relation to the claim.'},\n",
       " {'claim_id': 65,\n",
       "  'claim': 'Peter Cetera was entirely absent from the album Chicago Transit Authority.',\n",
       "  'true_label': 'REFUTES',\n",
       "  'predicted_label': 'REFUTES',\n",
       "  'qa_pairs': [[\"Is Peter Cetera's absence from the album Chicago Transit Authority accurate?\",\n",
       "    \"No, Peter Cetera's absence from the album *Chicago Transit Authority* is not accurate. He was a member of the band at the time and contributed to the album, though his role as a bassist and vocalist is more prominently featured on later albums.\"]],\n",
       "  'verdict_int': 0,\n",
       "  'verdict_raw': 'False. The question-answer pair directly refutes the claim by stating that Peter Cetera was not entirely absent from the album *Chicago Transit Authority* and was a member of the band at the time, contributing to the album.',\n",
       "  'r': '- misrepresented factual details: The explanation misrepresents the claim by implying the question-answer pair \"refutes\" the claim, when in fact the question-answer pair contradicts the claim.  \\n- logic/reasoning inconsistencies: The explanation fails to address the specific assertion in the claim about Cetera\\'s \"entirely absent\" status and instead focuses on his role as a member and contributor.  \\n- introduced/misrepresented noun phrases altering meaning: The explanation uses \"refutes\" in a way that mischaracterizes the relationship between the claim and the question-answer pair.  \\n- irrelevant or unsupported information: The explanation includes details about Cetera\\'s role on \"later albums,\" which are not relevant to the specific claim about *Chicago Transit Authority*.'},\n",
       " {'claim_id': 64,\n",
       "  'claim': 'Gustave Eiffel has yet to retire from engineering.',\n",
       "  'true_label': 'REFUTES',\n",
       "  'predicted_label': 'REFUTES',\n",
       "  'qa_pairs': [['When did Gustave Eiffel retire from engineering?',\n",
       "    'Gustave Eiffel retired from engineering after his contributions to meteorology and aerodynamics, but the exact date of his retirement is not specified in the provided sources.'],\n",
       "   [\"What was Gustave Eiffel's role in meteorology and aerodynamics after retiring from engineering?\",\n",
       "    'After retiring from engineering, Gustave Eiffel focused on research into meteorology and aerodynamics, making significant contributions in both fields.'],\n",
       "   [\"What was Gustave Eiffel's primary occupation before he retired from engineering?\",\n",
       "    \"Gustave Eiffel's primary occupation before he retired from engineering was a civil engineer and architect.\"]],\n",
       "  'verdict_int': 0,\n",
       "  'verdict_raw': 'The question-answer pairs do not provide a definitive date for Gustave Eiffel\\'s retirement, but they confirm he retired from engineering and transitioned to meteorology and aerodynamics, making it impossible to confirm the claim that he \"has yet to retire.\"  \\nVerdict: False',\n",
       "  'r': '- misrepresented factual details: The explanation assumes the claim is about current retirement status without explicit evidence from the Q&A pairs.  \\n- introduced/misrepresented events not in evidence: The explanation infers that Gustave Eiffel \"has yet to retire\" based on the lack of a specified date, but the Q&A only states that retirement is not specified, not that he is still active.  \\n- logic/reasoning inconsistencies: The conclusion that the claim is \"false\" is based on the absence of a retirement date, but the claim is about whether he \"has yet to retire,\" which is not directly addressed by the Q&A.  \\n- irrelevant or unsupported information: The explanation discusses the impossibility of confirming the claim, but the Q&A does not provide information about his current employment status.'},\n",
       " {'claim_id': 58,\n",
       "  'claim': 'Tequila is made from a plant primarily found around the woods.',\n",
       "  'true_label': 'REFUTES',\n",
       "  'predicted_label': 'REFUTES',\n",
       "  'qa_pairs': [['What plant is tequila primarily made from?',\n",
       "    'The plant tequila is primarily made from is the blue agave plant.']],\n",
       "  'verdict_int': 0,\n",
       "  'verdict_raw': 'False. The blue agave plant is not primarily found around the woods; it is primarily found in the desert regions of Mexico.  \\nInconclusive',\n",
       "  'r': '- misrepresented factual details: The explanation incorrectly states that the blue agave plant is not primarily found around the woods, which is a factual mischaracterization of the plant\\'s habitat.  \\n- introduced/misrepresented noun phrases altering meaning: \"Woods\" is misused to describe the habitat of the blue agave, which is not accurate.  \\n- logic/reasoning inconsistencies: The explanation contradicts the known habitat of the blue agave by associating it with woods instead of desert regions.  \\n- irrelevant or unsupported information: The mention of \"woods\" is irrelevant and unsupported by evidence, as blue agave is not found there.'},\n",
       " {'claim_id': 97,\n",
       "  'claim': 'The People vs. Larry Flynt is an unproduced screenplay.',\n",
       "  'true_label': 'REFUTES',\n",
       "  'predicted_label': 'REFUTES',\n",
       "  'qa_pairs': [['Is \"The People vs. Larry Flynt\" actually an unproduced screenplay or has it been produced in some form?',\n",
       "    'Answer: \"The People vs. Larry Flynt\" has been produced in some form; it is a 1996 American biographical drama film.']],\n",
       "  'verdict_int': 0,\n",
       "  'verdict_raw': 'False. The question-answer pairs directly disprove the claim by stating that \"The People vs. Larry Flynt\" has been produced as a 1996 film.',\n",
       "  'r': '- misrepresented factual details: The explanation misstates the claim as \"false\" when the claim is about the status of the work as an unproduced screenplay, and the answer correctly identifies it as a produced film. The explanation does not directly address the claim\\'s accuracy but instead focuses on the answer\\'s content.  \\n- logic/reasoning inconsistencies: The explanation assumes the claim is false without proper contextual analysis of the claim and answer.  \\n- introduced/misrepresented events not in evidence: The explanation does not introduce new events but rather misinterprets the relationship between the claim and the answer.  \\n- irrelevant or unsupported information: The explanation does not provide additional information beyond what is in the answer and claim.  \\n- inadequate justification: The explanation does not adequately justify why the claim is false, instead relying on a direct contradiction without contextual clarification.'},\n",
       " {'claim_id': 56,\n",
       "  'claim': 'Matt Damon is not an actor.',\n",
       "  'true_label': 'REFUTES',\n",
       "  'predicted_label': 'NOT ENOUGH INFO',\n",
       "  'qa_pairs': [['Who is considered an actor in the context of this statement?',\n",
       "    'The term \"actor\" in the context of this statement refers to individuals or entities involved in the process of problematization, particularly within the framework of actor-network theory (ANT). These actors are participants in the network and are involved in the interdefinition of actors and the definition of the problem/topic/action program. Specific actors are not named in the provided text.'],\n",
       "   ['What is the definition of \"actor\" used in the statement?',\n",
       "    'question too specific'],\n",
       "   ['What is the basis for the assertion that Matt Damon is not an actor?',\n",
       "    'question too specific']],\n",
       "  'verdict_int': 2,\n",
       "  'verdict_raw': 'Inconclusive',\n",
       "  'r': '- misrepresented factual details: The explanation labels the claim as \"Inconclusive\" without addressing the actual content of the claim or providing any analysis based on the evidence provided.  \\n- logic/reasoning inconsistencies: The feedback fails to engage with the question-answer pairs or the claim, leading to an unsupported conclusion.  \\n- irrelevant or unsupported information: The feedback does not reference the specific evidence or reasoning provided in the question-answer pairs, making the assessment arbitrary.  \\n- inadequate justification: The explanation does not justify why the claim is \"Inconclusive\" based on the given evidence, leaving the assessment without sufficient foundation.'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regenerate()\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68659246",
   "metadata": {},
   "source": [
    "We now need a section of code to pretty-print each claim out, making it easy to judge the effectiveness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be0c38ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_results(sample):\n",
    "    for i, s in enumerate(sample):\n",
    "        claim = s[\"claim\"]\n",
    "        qa_pairs = s[\"qa_pairs\"]\n",
    "        explanation = s[\"verdict_raw\"]\n",
    "        madr_agent = s[\"r\"].strip()\n",
    "        print(f\"{i+1}. {claim}\\n\")\n",
    "        print(f\"Available evidence for this claim, in the form of Q/A pairs:\")\n",
    "        for j, e in enumerate(qa_pairs):\n",
    "            print(f\"{j+1}.\\tQuestion: {e[0]}\\n\\t{e[1]}\")\n",
    "        print()\n",
    "        print(f\"The original verdict: {explanation}\\n\")\n",
    "        print(f\"This agent's feedback:\\n{madr_agent}\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "pprint_results(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a717ebe",
   "metadata": {},
   "source": [
    "## Debater 2 Freespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfaac8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate():\n",
    "    for c in sample:\n",
    "        claim = c[\"claim\"]\n",
    "        qa_pairs = c[\"qa_pairs\"]\n",
    "        verdict_raw = c[\"verdict_raw\"]\n",
    "        fb = client.send_prompt(\"madr_init_fb2\", [claim, qa_pairs, verdict_raw])\n",
    "        c[\"r\"] = fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e734a821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Birth of the Dragon's principal photography began in Vancouver, Canada.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Where was the principal photography for *Birth of the Dragon* actually conducted?\n",
      "\tThe principal photography for *Birth of the Dragon* was conducted in Vancouver.\n",
      "\n",
      "The original verdict: True.  \n",
      "Inconclusive\n",
      "\n",
      "This agent's feedback:\n",
      "Explanation: True.  \n",
      "Inconclusive\n",
      "\n",
      "Feedback: The explanation is unfaithful because it states \"True.\" without providing any supporting reasoning or evidence, which is inconsistent with the claim and the evidence provided. The explanation should either confirm the claim with reasoning or state that it is inconclusive with proper justification.\n",
      "\n",
      "\n",
      "\n",
      "2. Taal was romantic.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What historical evidence supports the assertion that Taal was romantic?\n",
      "\tquestion too specific\n",
      "2.\tQuestion: When did Taal exhibit romantic behavior?\n",
      "\tquestion too specific\n",
      "3.\tQuestion: Who is considered to have been romantic in the claim about Taal?\n",
      "\tquestion too specific\n",
      "\n",
      "The original verdict: Inconclusive\n",
      "\n",
      "This agent's feedback:\n",
      "Explanation: Inconclusive\n",
      "\n",
      "Feedback: The explanation \"Inconclusive\" is factually inaccurate. The evidence provided indicates that the questions are too specific, but the explanation does not address this directly. Instead, it fails to provide any analysis or reasoning based on the evidence, making it incomplete and unfaithful to the claim and evidence.\n",
      "\n",
      "\n",
      "\n",
      "3. Jet Li was a person.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Is the statement about Jet Li being a person accurate?\n",
      "\tquestion too specific\n",
      "2.\tQuestion: What is the basis for the assertion that Jet Li was a person?\n",
      "\tquestion too specific\n",
      "3.\tQuestion: When was Jet Li considered a person according to the statement?\n",
      "\tquestion too specific\n",
      "\n",
      "The original verdict: Inconclusive\n",
      "\n",
      "This agent's feedback:\n",
      "Explanation: Inconclusive\n",
      "\n",
      "Feedback: The explanation \"Inconclusive\" is incomplete. The evidence provided indicates that the question is too specific, but the explanation does not address the claim that Jet Li was a person. The explanation fails to provide any factual basis or reasoning related to the claim, making it unfaithful to the evidence and the claim.\n",
      "\n",
      "\n",
      "\n",
      "4. Dangal (film) is set in Haryana.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Where is the film \"Dangal\" primarily set?\n",
      "\tquestion too specific\n",
      "2.\tQuestion: What region in Haryana is the film \"Dangal\" set in?\n",
      "\tquestion too specific\n",
      "3.\tQuestion: What is the significance of the setting in \"Dangal\" regarding Haryana?\n",
      "\tThe setting in \"Bharat Kesri Dangal\" is significant regarding Haryana as it is a freestyle wrestling competition organized by the Haryana Sports and Youth Affairs department to commemorate the martyrdom day of Shaheed Bhagat Singh, Rajguru, and Sukhdev.\n",
      "\n",
      "The original verdict: The question-answer pairs confirm that the film \"Dangal\" is set in Haryana, as they mention the setting is significant to Haryana and reference a wrestling competition organized by the Haryana Sports and Youth Affairs department.  \n",
      "True.\n",
      "\n",
      "This agent's feedback:\n",
      "\"The question-answer pairs confirm that the film 'Dangal' is set in Haryana, as they mention the setting is significant to Haryana and reference a wrestling competition organized by the Haryana Sports and Youth Affairs department.\"\n",
      "\n",
      "- This sentence is unfaithful because it incorrectly attributes the significance of the setting to the film \"Dangal\" when the evidence only references the wrestling competition in the context of Haryana's significance, not the film's setting. The evidence does not state that the film is set in Haryana, only that the setting is significant to Haryana. The explanation conflates the significance of the setting with the film's location.\n",
      "\n",
      "\n",
      "\n",
      "5. Julia Louis-Dreyfus has a job.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What is the nature of Julia Louis-Dreyfus's job?\n",
      "\tJulia Louis-Dreyfus is an actress, most known for her role as Elaine Benes on the sitcom *Seinfeld*.\n",
      "\n",
      "The original verdict: True. The question-answer pair confirms that Julia Louis-Dreyfus is an actress, which constitutes having a job.\n",
      "\n",
      "This agent's feedback:\n",
      "\"True. The question-answer pair confirms that Julia Louis-Dreyfus is an actress, which constitutes having a job.\"\n",
      "\n",
      "- This sentence is unfaithful because it uses the word \"True\" without justification or evidence from the provided Q/A pair. The explanation should not assert truth without supporting reasoning based on the given evidence.\n",
      "- The statement \"which constitutes having a job\" is logically inaccurate. While being an actress is a job, the Q/A pair does not explicitly state that being an actress is a \"job,\" and the explanation assumes this without direct support from the evidence.\n",
      "\n",
      "\n",
      "\n",
      "6. Independence Day: Resurgence is a drama film.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What genre does Independence Day: Resurgence belong to?\n",
      "\tquestion too specific\n",
      "2.\tQuestion: What type of film is Independence Day: Resurgence?\n",
      "\tScience fiction action film.\n",
      "\n",
      "The original verdict: The question-answer pairs provide a direct answer stating that *Independence Day: Resurgence* is a \"science fiction action film,\" which contradicts the claim that it is a \"drama film.\"  \n",
      "False.\n",
      "\n",
      "This agent's feedback:\n",
      "The explanation states: \"The question-answer pairs provide a direct answer stating that *Independence Day: Resurgence* is a 'science fiction action film,' which contradicts the claim that it is a 'drama film.' False.\"\n",
      "\n",
      "This sentence is unfaithful because it incorrectly labels the explanation as \"False\" when the explanation is actually accurate. The explanation correctly points out the contradiction between the claim and the evidence, and the final \"False\" is not part of the explanation's reasoning but an external assertion.\n",
      "\n",
      "\n",
      "\n",
      "7. John Wick: Chapter 2 was theatrically released in AMC Theatres.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: When was John Wick: Chapter 2 theatrically released?\n",
      "\tFebruary 10, 2017.\n",
      "\n",
      "The original verdict: The question-answer pair confirms the theatrical release date of John Wick: Chapter 2 but does not specifically mention AMC Theatres, so it does not fully confirm the claim.  \n",
      "Inconclusive.\n",
      "\n",
      "This agent's feedback:\n",
      "The explanation is unfaithful because it incorrectly states \"Inconclusive\" as feedback when the evidence clearly indicates that the claim is not fully confirmed by the Q/A pair. The explanation fails to acknowledge that the Q/A pair confirms the release date but not the theater. The sentence \"The question-answer pair confirms the theatrical release date of John Wick: Chapter 2 but does not specifically mention AMC Theatres, so it does not fully confirm the claim.\" is factually accurate, but the conclusion \"Inconclusive\" is not supported by the evidence. Feedback: \"Inconclusive\" is not a valid conclusion based on the provided evidence.\n",
      "\n",
      "\n",
      "\n",
      "8. Appian Way Productions is a film direction agency.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What is the primary function or focus of Appian Way Productions?\n",
      "\tThe primary function or focus of Appian Way Productions is film production.\n",
      "\n",
      "The original verdict: The question-answer pair confirms the claim that Appian Way Productions is a film direction agency, as it states the primary function is film production.  \n",
      "True.\n",
      "\n",
      "This agent's feedback:\n",
      "The explanation contains the sentence: \"The question-answer pair confirms the claim that Appian Way Productions is a film direction agency, as it states the primary function is film production.\"\n",
      "\n",
      "This sentence is unfaithful because it inaccurately categorizes Appian Way Productions as a \"film direction agency\" based on the evidence that its primary function is \"film production.\" The explanation conflates \"film production\" with \"film direction agency,\" which are distinct roles. The evidence does not support the claim that it is a direction agency, only that it is involved in production.\n",
      "\n",
      "\n",
      "\n",
      "9. Ronaldo Maczinski's birthday is May 11.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: When was Ronaldo Maczinski actually born?\n",
      "\tAnswer: 11 September 1980.\n",
      "\n",
      "The original verdict: False.  \n",
      "Inconclusive\n",
      "\n",
      "This agent's feedback:\n",
      "Explanation: False.  \n",
      "Inconclusive\n",
      "\n",
      "Feedback: \"False.\" is unfaithful because it contradicts the claim that Ronaldo Maczinski's birthday is May 11. \"Inconclusive\" is unfaithful because it fails to address the evidence provided, which clearly states the birth date as 11 September 1980.\n",
      "\n",
      "\n",
      "\n",
      "10. Big Boi has yet to create music.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: When was Big Boi last known to have released a music album or single?\n",
      "\tBig Boi's last known release was his second studio album, *Vicious Lies and Dangerous Rumors*, which was released on December 11, 2012.\n",
      "\n",
      "The original verdict: The question-answer pairs disprove the claim.  \n",
      "Verdict: False\n",
      "\n",
      "This agent's feedback:\n",
      "\"The question-answer pairs disprove the claim.\"  \n",
      "This sentence is unfaithful because it is factually inaccurate. The explanation incorrectly states that the question-answer pairs disprove the claim, when in fact they support the claim that Big Boi has yet to create music, as the last known release was in 2012 and no subsequent releases are mentioned.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = select_random_sample(corag_run)\n",
    "regenerate()\n",
    "pprint_results(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce506752",
   "metadata": {},
   "source": [
    "## Cross-Reference Freespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eebc07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate the debaters\n",
    "# for c in corag_run:\n",
    "#     claim = c[\"claim\"]\n",
    "#     qa_pairs = c[\"qa_pairs\"]\n",
    "#     verdict_raw = c[\"verdict_raw\"]\n",
    "#     fb1 = client.send_prompt(\"madr_init_fb1\", [claim, qa_pairs, verdict_raw])\n",
    "#     fb2 = client.send_prompt(\"madr_init_fb2\", [claim, qa_pairs, verdict_raw])\n",
    "#     c[\"fb1\"] = fb1\n",
    "#     c[\"fb2\"] = fb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cff7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "incomp_relative_path = os.path.join('assets', '20251201T170345--metrics_incomp__100.json')\n",
    "#with open(relative_path, 'w') as f:\n",
    "#    json.dump(corag_run, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32788368",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(relative_path, 'r', encoding='utf-8') as f:\n",
    "    incomp_run = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7215c636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim_id': 0,\n",
       "  'claim': 'In 2008, Jeff Ament released a solo record.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [[\"When was Jeff Ament's solo record released?\",\n",
       "    'September 16, 2008.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'True.  \\nInconclusive',\n",
       "  'fb1': '- misrepresented factual details: Explanation states \"True. Inconclusive\" which is inconsistent and misleading.  \\n- logic/reasoning inconsistencies: The explanation provides no justification for the conclusion.  \\n- irrelevant or unsupported information: The phrase \"Inconclusive\" is presented as a conclusion without support.  \\n- inadequate justification: The explanation fails to provide any reasoning or evidence to support its claim.',\n",
       "  'fb2': 'Explanation: True.  \\nInconclusive\\n\\nFeedback: \"Inconclusive\" is unfaithful because it contradicts the evidence provided in the QA-pair, which clearly states that Jeff Ament\\'s solo record was released on September 16, 2008. The explanation should be \"True\" based on the evidence, but it is labeled as \"Inconclusive,\" which is factually inaccurate.'},\n",
       " {'claim_id': 1,\n",
       "  'claim': 'Franklin D. Roosevelt had a family that originated in New York.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [[\"What is the origin of Franklin D. Roosevelt's family?\",\n",
       "    \"Franklin D. Roosevelt's family originated from Dutch settlers in New York, with his ancestors tracing back to the 17th century. His direct lineage includes prominent figures such as his father, James Roosevelt, and his mother, Eleanor Roosevelt, who were part of influential American families.\"]],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'True. The question-answer pairs fully confirm the claim that Franklin D. Roosevelt had a family that originated in New York.',\n",
       "  'fb1': '- misrepresented factual details: The explanation assumes the question-answer pair fully confirms the claim without verifying the specific historical accuracy of the stated origin of the family. The answer mentions Dutch settlers in New York and 17th-century ancestry, which may not be supported by the evidence provided.  \\n- inadequate justification: The explanation does not address or reference the specific evidence in the question-answer pair to support the claim, merely stating that it \"fully confirms\" the claim without analysis.',\n",
       "  'fb2': 'The explanation is unfaithful because it makes a claim without directly referencing the evidence provided in the QA-pair. The sentence \"The question-answer pairs fully confirm the claim...\" is factually inaccurate as it implies the QA-pair confirms the claim without explicitly stating how the evidence supports the claim. The explanation should instead reference the specific evidence from the QA-pair, such as the mention of Dutch settlers in New York and the 17th-century origin.'},\n",
       " {'claim_id': 2,\n",
       "  'claim': 'A phylogenetic tree is a branching diagram.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [['What is the primary purpose of a phylogenetic tree according to the statement?',\n",
       "    'The primary purpose of a phylogenetic tree is to represent a hypothesis about the evolutionary ancestry of a set of genes, species, or other taxa.'],\n",
       "   ['What type of diagram is a phylogenetic tree described as in the statement?',\n",
       "    'A phylogenetic tree is described as a branching diagram or \"tree\" in the statement.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'The question-answer pairs fully confirm the claim.  \\nTrue',\n",
       "  'fb1': '- misrepresented factual details: The explanation assumes the question-answer pairs \"confirm\" the claim without addressing whether the answers align with the claim. The claim is about the definition of a phylogenetic tree as a \"branching diagram,\" while the answers describe its purpose and describe it as a \"branching diagram.\" The explanation fails to critically assess whether the answers directly support the claim.  \\n- logic/reasoning inconsistencies: The explanation concludes \"True\" without providing justification based on the content of the question-answer pairs. It assumes confirmation without explicit alignment between the answers and the claim.  \\n- inadequate justification: The feedback does not explain why the question-answer pairs \"confirm\" the claim, nor does it address whether the answers accurately reflect the claim\\'s definition.',\n",
       "  'fb2': 'The explanation is unfaithful because it incorrectly states that the question-answer pairs \"fully confirm the claim\" without providing specific evidence from the Q/A pairs to support this assertion. The explanation fails to analyze the content of the Q/A pairs in relation to the claim, making the justification irrelevant and incomplete.'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incomp_run[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a704260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Explanation:\\n{}\\n\\nPrimary feedback:\\n{}\\n\\nSecondary feedback:\\n{}',\n",
       " 'Given the following explanation plus primary and secondary feedback, revise the primary feedback using the explanation as ground truth.\\n\\nRemove any invalid points in the primary feedback, add only valid corrections supported by the secondary feedback, and ignore invalid secondary feedback.\\n\\nMake minimal edits; do not copy secondary feedback verbatim. Output only the corrected primary feedback.')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the user prompt include explanation now\n",
    "client._prompts[\"madr_cross_fb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff5082fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_cross():\n",
    "    for i, c in enumerate(sample):\n",
    "        fb1 = c[\"fb1\"]\n",
    "        fb2 = c[\"fb2\"]\n",
    "        claim = c[\"claim\"]\n",
    "        qa_pairs = c[\"qa_pairs\"]\n",
    "        verdict_raw = c[\"verdict_raw\"]\n",
    "        if i >= 5:\n",
    "            # try opposite send order\n",
    "            cross = client.send_prompt(\"madr_cross_fb\", [claim, qa_pairs, verdict_raw, fb2, fb1])\n",
    "        else:\n",
    "            cross = client.send_prompt(\"madr_cross_fb\", [claim, qa_pairs, verdict_raw, fb1, fb2])\n",
    "        c[\"cross\"] = cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87765239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_results_cross(sample):\n",
    "    for i, s in enumerate(sample):\n",
    "        claim = s[\"claim\"]\n",
    "        qa_pairs = s[\"qa_pairs\"]\n",
    "        explanation = s[\"verdict_raw\"]\n",
    "        fb1 = s[\"fb1\"]\n",
    "        fb2 = s[\"fb2\"]\n",
    "        cross_agent = s[\"cross\"].strip()\n",
    "        print(f\"{i+1}. {claim}\")\n",
    "        print(f\"Available evidence for this claim, in the form of Q/A pairs:\")\n",
    "        for j, e in enumerate(qa_pairs):\n",
    "            print(f\"{j+1}.\\tQuestion: {e[0]}\\n\\t{e[1]}\")\n",
    "        print(f\"Explanation: {explanation}\\n\")\n",
    "        print(f\"Feedback for this explanation:\")\n",
    "        print(f\"\\ta. {fb1}\")\n",
    "        print(f\"\\tb. {fb2}\")\n",
    "        print()\n",
    "        print(f\"This agent's rewrite:\\n{cross_agent}\")\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47e2953b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Jared Padalecki grew up in the US.\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Where was Jared Padalecki born and raised?\n",
      "\tJared Padalecki was born and raised in Texas.\n",
      "Explanation: True.  \n",
      "Inconclusive.\n",
      "\n",
      "Feedback for this explanation:\n",
      "\ta. - misrepresented factual details: Explanation states \"True. Inconclusive,\" which is contradictory and misrepresents the evaluation of the claim.  \n",
      "- logic/reasoning inconsistencies: The explanation provides no justification for its conclusion, leading to an inconsistent and unsupported response.\n",
      "\tb. Explanation: True.  \n",
      "Inconclusive.\n",
      "\n",
      "Feedback: \"True.\" is unfaithful because it incorrectly asserts the explanation is true when the evidence states Jared Padalecki was born and raised in Texas, but the explanation does not provide any supporting details or reasoning. \"Inconclusive\" is unfaithful because it contradicts the evidence, which directly states that Jared Padalecki was born and raised in Texas, making the explanation conclusive, not inconclusive.\n",
      "\n",
      "This agent's rewrite:\n",
      "- misrepresented factual details: The explanation states \"True. Inconclusive,\" which is contradictory and misrepresents the evaluation of the claim. The evidence directly states that Jared Padalecki was born and raised in Texas, making the claim true.  \n",
      "- logic/reasoning inconsistencies: The explanation provides no justification for its conclusion, leading to an inconsistent and unsupported response.\n",
      "\n",
      "\n",
      "\n",
      "2. The People vs. Larry Flynt is directed by Milos Forman.\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Is the movie \"The People vs. Larry Flynt\" directed by Milos Forman?\n",
      "\tAnswer: Yes.\n",
      "Explanation: True.\n",
      "\n",
      "Feedback for this explanation:\n",
      "\ta. - inadequate justification.\n",
      "\tb. Explanation: True.\n",
      "\n",
      "Feedback: The explanation is too vague and does not address the claim or provide any reasoning based on the evidence. It fails to explain why the statement is true or reference the evidence provided.\n",
      "\n",
      "This agent's rewrite:\n",
      "Feedback: The explanation should clearly state that the movie \"The People vs. Larry Flynt\" was indeed directed by Milos Forman, based on the provided answer and evidence.\n",
      "\n",
      "\n",
      "\n",
      "3. J. R. R. Tolkien created Gimli.\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Who is credited with creating the character Gimli in the context of J. R. R. Tolkien's works?\n",
      "\tJ. R. R. Tolkien is credited with creating the character Gimli in the context of his works.\n",
      "Explanation: True.  \n",
      "Inconclusive\n",
      "\n",
      "Feedback for this explanation:\n",
      "\ta. - misrepresented factual details: The explanation incorrectly states \"Inconclusive\" when the evidence clearly supports that J. R. R. Tolkien is credited with creating Gimli.  \n",
      "- inadequate justification: The explanation provides no reasoning for its conclusion, merely stating \"True\" and \"Inconclusive\" without support.\n",
      "\tb. Explanation: True.  \n",
      "Inconclusive\n",
      "\n",
      "Feedback: \"Inconclusive\" is unfaithful because it contradicts the evidence provided in the QA-pair, which explicitly states that J. R. R. Tolkien is credited with creating Gimli. The explanation should be more specific and accurate rather than inconclusive.\n",
      "\n",
      "This agent's rewrite:\n",
      "Primary feedback:\n",
      "- misrepresented factual details: The explanation incorrectly states \"Inconclusive\" when the evidence clearly supports that J. R. R. Tolkien is credited with creating Gimli.  \n",
      "- inadequate justification: The explanation provides no reasoning for its conclusion, merely stating \"True\" and \"Inconclusive\" without support.\n",
      "\n",
      "\n",
      "\n",
      "4. Priyanka Chopra advocates various causes.\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What specific causes does Priyanka Chopra advocate for?\n",
      "\tquestion too specific\n",
      "2.\tQuestion: When did Priyanka Chopra start advocating for various causes?\n",
      "\tquestion too specific\n",
      "3.\tQuestion: What organizations is Priyanka Chopra associated with in her advocacy work?\n",
      "\tquestion too specific\n",
      "Explanation: Inconclusive\n",
      "\n",
      "Feedback for this explanation:\n",
      "\ta. - misrepresented factual details: The explanation labels the responses as \"inconclusive\" without providing evidence or reasoning to support that conclusion.  \n",
      "- inadequate justification: The feedback does not explain why the responses are considered \"inconclusive,\" lacking proper justification based on the evidence.  \n",
      "- logic/reasoning inconsistencies: The explanation fails to address the specific questions asked, leading to a lack of logical connection between the claim and the feedback.\n",
      "\tb. Explanation: Inconclusive\n",
      "\n",
      "Feedback: The explanation \"Inconclusive\" is unfaithful because it fails to address the claim that Priyanka Chopra advocates various causes. The explanation does not provide any specific information or reasoning based on the evidence, which indicates that the question is too specific and the available evidence does not support a definitive answer. The explanation is incomplete and does not align with the provided evidence.\n",
      "\n",
      "This agent's rewrite:\n",
      "- misrepresented factual details: The explanation \"Inconclusive\" does not provide specific evidence or reasoning to support the conclusion, making it unfaithful to the provided evidence.  \n",
      "- inadequate justification: The explanation lacks detailed reasoning for why the responses are considered \"inconclusive,\" failing to align with the evidence provided.  \n",
      "- logic/reasoning inconsistencies: The explanation does not address the specific questions asked or provide a logical connection between the claim and the conclusion.\n",
      "\n",
      "\n",
      "\n",
      "5. Dangal (film) is set in Haryana.\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Where is the film \"Dangal\" primarily set?\n",
      "\tquestion too specific\n",
      "2.\tQuestion: What region in Haryana is the film \"Dangal\" set in?\n",
      "\tquestion too specific\n",
      "3.\tQuestion: What is the significance of the setting in \"Dangal\" regarding Haryana?\n",
      "\tThe setting in \"Bharat Kesri Dangal\" is significant regarding Haryana as it is a freestyle wrestling competition organized by the Haryana Sports and Youth Affairs department to commemorate the martyrdom day of Shaheed Bhagat Singh, Rajguru, and Sukhdev.\n",
      "Explanation: The question-answer pairs confirm that the film \"Dangal\" is set in Haryana, as they mention the setting is significant to Haryana and reference a wrestling competition organized by the Haryana Sports and Youth Affairs department.  \n",
      "True.\n",
      "\n",
      "Feedback for this explanation:\n",
      "\ta. - misrepresented factual details: The explanation assumes the film is set in Haryana based on the mention of a Haryana-organized event, but the evidence only states the setting is significant to Haryana without confirming the film's location.  \n",
      "- introduced/misrepresented events not in evidence: The explanation references \"Bharat Kesri Dangal\" as the film, which is not mentioned in the provided evidence.  \n",
      "- inadequate justification: The conclusion that the film is set in Haryana is not directly supported by the evidence, which only discusses the significance of the setting to Haryana.\n",
      "\tb. The explanation is faithful to the claim and evidence. There are no weaknesses in the explanation that reduce faithfulness to the claim or evidence.\n",
      "\n",
      "This agent's rewrite:\n",
      "- misrepresented factual details: The explanation correctly identifies the film as set in Haryana based on the evidence that the setting is significant to Haryana and references a Haryana-organized event.  \n",
      "- introduced/misrepresented events not in evidence: The explanation correctly does not reference \"Bharat Kesri Dangal\" as the film, as this is not mentioned in the provided evidence.  \n",
      "- inadequate justification: The conclusion that the film is set in Haryana is directly supported by the evidence, which states the setting is significant to Haryana.\n",
      "\n",
      "\n",
      "\n",
      "6. Gustave Eiffel has yet to retire from engineering.\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: When did Gustave Eiffel retire from engineering?\n",
      "\tGustave Eiffel retired from engineering after his contributions to meteorology and aerodynamics, but the exact date of his retirement is not specified in the provided sources.\n",
      "2.\tQuestion: What was Gustave Eiffel's role in meteorology and aerodynamics after retiring from engineering?\n",
      "\tAfter retiring from engineering, Gustave Eiffel focused on research into meteorology and aerodynamics, making significant contributions in both fields.\n",
      "3.\tQuestion: What was Gustave Eiffel's primary occupation before he retired from engineering?\n",
      "\tGustave Eiffel's primary occupation before he retired from engineering was a civil engineer and architect.\n",
      "Explanation: The question-answer pairs do not provide a definitive date for Gustave Eiffel's retirement, but they confirm he retired from engineering and transitioned to meteorology and aerodynamics, making it impossible to confirm the claim that he \"has yet to retire.\"  \n",
      "Verdict: False\n",
      "\n",
      "Feedback for this explanation:\n",
      "\ta. - misrepresented factual details: The explanation assumes the claim is about whether Gustave Eiffel is still actively working in engineering, but the claim is about him \"has yet to retire from engineering,\" which is a statement about his current status, not about his retirement date.  \n",
      "- logic/reasoning inconsistencies: The explanation concludes the claim is false based on the absence of a retirement date, but the claim's truth value depends on whether he is still employed in engineering, not on the availability of a retirement date.  \n",
      "- introduced/misrepresented noun phrases altering meaning: The explanation misrepresents the claim as being about current employment status, whereas the claim is specifically about retirement status.\n",
      "\tb. \"The question-answer pairs do not provide a definitive date for Gustave Eiffel's retirement, but they confirm he retired from engineering and transitioned to meteorology and aerodynamics, making it impossible to confirm the claim that he 'has yet to retire.'\"  \n",
      "- This sentence is unfaithful because it incorrectly implies that the absence of a definitive date means he has retired, which is not supported by the evidence. The evidence only states that the exact date is not specified, not that he has retired.  \n",
      "- The phrase \"making it impossible to confirm the claim that he 'has yet to retire'\" is logically inaccurate because the claim is that he \"has yet to retire,\" and the evidence does not confirm he has retired, so it is possible to confirm the claim.\n",
      "\n",
      "This agent's rewrite:\n",
      "\"The question-answer pairs do not provide a definitive date for Gustave Eiffel's retirement, but they confirm he retired from engineering and transitioned to meteorology and aerodynamics, making it impossible to confirm the claim that he 'has yet to retire.'\"  \n",
      "- The sentence incorrectly implies that the absence of a definitive date means he has retired, which is not supported by the evidence. The evidence only states that the exact date is not specified, not that he has retired.  \n",
      "- The phrase \"making it impossible to confirm the claim that he 'has yet to retire'\" is logically inaccurate because the claim is that he \"has yet to retire,\" and the evidence does not confirm he has retired, so it is possible to confirm the claim.\n",
      "\n",
      "\n",
      "\n",
      "7. Matt Damon is not an actor.\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Who is considered an actor in the context of this statement?\n",
      "\tThe term \"actor\" in the context of this statement refers to individuals or entities involved in the process of problematization, particularly within the framework of actor-network theory (ANT). These actors are participants in the network and are involved in the interdefinition of actors and the definition of the problem/topic/action program. Specific actors are not named in the provided text.\n",
      "2.\tQuestion: What is the definition of \"actor\" used in the statement?\n",
      "\tquestion too specific\n",
      "3.\tQuestion: What is the basis for the assertion that Matt Damon is not an actor?\n",
      "\tquestion too specific\n",
      "Explanation: Inconclusive\n",
      "\n",
      "Feedback for this explanation:\n",
      "\ta. - misrepresented factual details: The explanation labels the claim as \"inconclusive\" without addressing the actual content of the claim or providing any analysis of the evidence presented.  \n",
      "- introduced/misrepresented events not in evidence: The explanation does not engage with the provided evidence or the definitions given in the question-answer pairs.  \n",
      "- logic/reasoning inconsistencies: The explanation fails to provide any logical reasoning or justification for its conclusion, making it inconsistent with the requirement to evaluate the explanation against the evidence.  \n",
      "- irrelevant or unsupported information: The explanation is overly brief and does not offer any meaningful feedback based on the provided evidence or the context of the claim.\n",
      "\tb. Explanation: Inconclusive\n",
      "\n",
      "Feedback: The sentence \"Inconclusive\" is factually inaccurate. The explanation should address the claim based on the provided evidence, but instead, it fails to provide any analysis or reasoning that connects to the claim or evidence. It is incomplete and does not demonstrate faithfulness to the claim or evidence.\n",
      "\n",
      "This agent's rewrite:\n",
      "Explanation: The explanation is factually inaccurate and fails to address the claim or provide analysis based on the evidence. It does not engage with the definitions provided in the question-answer pairs or offer reasoning related to the claim that Matt Damon is not an actor.\n",
      "\n",
      "\n",
      "\n",
      "8. Gunday was written by Aditya Chopra.\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Who is the actual author of the book \"Gunday\"?\n",
      "\tThe question is too specific.\n",
      "2.\tQuestion: When was the book \"Gunday\" written?\n",
      "\tquestion too specific\n",
      "3.\tQuestion: What is the correct author of the book \"Gunday\"?\n",
      "\tquestion too specific\n",
      "Explanation: Inconclusive\n",
      "\n",
      "Feedback for this explanation:\n",
      "\ta. - misrepresented factual details: The explanation states \"Inconclusive\" without providing any reasoning or evidence to support that conclusion.  \n",
      "- inadequate justification: The feedback does not explain why the answer is inconclusive, failing to justify the assessment based on the provided evidence.  \n",
      "- irrelevant or unsupported information: The feedback does not address the specific questions or evidence provided, making it irrelevant to the task.\n",
      "\tb. Explanation: Inconclusive\n",
      "\n",
      "Feedback: The explanation \"Inconclusive\" is unfaithful because it fails to address the claim directly. The explanation does not provide any factual information about the author of \"Gunday\" and does not reference the evidence that the questions are too specific. It is irrelevant to the claim and does not demonstrate any logical connection to the provided evidence.\n",
      "\n",
      "This agent's rewrite:\n",
      "Explanation: Inconclusive  \n",
      "Feedback: The explanation \"Inconclusive\" does not address the claim directly or provide reasoning based on the evidence that the questions are too specific. It fails to explain why the answer is inconclusive and does not reference the specific questions or evidence provided.\n",
      "\n",
      "\n",
      "\n",
      "9. We Found Love is a romance novel.\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What genre does the book \"We Found Love\" belong to?\n",
      "\tquestion too specific\n",
      "2.\tQuestion: What is the main theme of \"We Found Love\"?\n",
      "\tquestion too specific\n",
      "3.\tQuestion: What is the author of \"We Found Love\"?\n",
      "\tquestion too specific\n",
      "Explanation: Inconclusive\n",
      "\n",
      "Feedback for this explanation:\n",
      "\ta. - misrepresented factual details: The explanation labels the question-answer pairs as \"question too specific\" without providing evidence or reasoning to support this judgment.  \n",
      "- logic/reasoning inconsistencies: The explanation fails to address the actual content of the questions or the evidence presented, leading to an inconclusive response.  \n",
      "- inadequate justification: The conclusion \"Inconclusive\" is not supported by any analysis of the provided evidence or reasoning.\n",
      "\tb. Explanation: Inconclusive\n",
      "\n",
      "Feedback: The explanation \"Inconclusive\" is factually inaccurate. The claim states that \"We Found Love\" is a romance novel, and the evidence includes Q/A pairs that indicate the question about the genre is too specific, which does not contradict the claim. The explanation fails to address the claim directly and is incomplete.\n",
      "\n",
      "This agent's rewrite:\n",
      "Explanation: Inconclusive\n",
      "\n",
      "Feedback: The explanation \"Inconclusive\" is factually inaccurate. The claim states that \"We Found Love\" is a romance novel, and the evidence includes Q/A pairs that indicate the question about the genre is too specific, which does not contradict the claim. The explanation fails to address the claim directly and is incomplete.\n",
      "\n",
      "\n",
      "\n",
      "10. Friendship has been studied only in sociology.\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Who conducted the studies on friendship mentioned in the claim?\n",
      "\tquestion too specific\n",
      "2.\tQuestion: What disciplines have studied friendship besides sociology?\n",
      "\tCommunication, social psychology, anthropology, and philosophy.\n",
      "Explanation: The question-answer pairs disprove the claim by showing that friendship has been studied in disciplines beyond sociology.  \n",
      "False\n",
      "\n",
      "Feedback for this explanation:\n",
      "\ta. - misrepresented factual details: The explanation misrepresents the claim by implying the question-answer pairs directly disprove the claim, when the claim is about the scope of studies, not the existence of specific studies.  \n",
      "- logic/reasoning inconsistencies: The explanation fails to address the specific content of the question-answer pairs and instead makes a broad assertion without connecting the pairs to the claim.  \n",
      "- irrelevant or unsupported information: The explanation concludes with \"False\" without providing a valid reasoning chain that links the question-answer pairs to the claim's truth value.\n",
      "\tb. The explanation is unfaithful because it incorrectly states \"False\" as feedback for the claim, which is not supported by the evidence. The explanation does not address the specific weaknesses in the reasoning provided.  \n",
      "\n",
      "Problematic sentence: \"The question-answer pairs disprove the claim by showing that friendship has been studied in disciplines beyond sociology.\"  \n",
      "Reason: This sentence is factually accurate based on the evidence, but the explanation fails to recognize that the claim is about \"only\" sociology, and the QA pairs show that other disciplines have also studied friendship, thus disproving the claim. However, the explanation does not explicitly point out this logical incoherence in the claim's wording.  \n",
      "\n",
      "Problematic sentence: \"False\"  \n",
      "Reason: This is an incorrect feedback statement. It does not explain why the explanation is unfaithful to the claim or evidence, and it is not a proper response to the explanation's reasoning.\n",
      "\n",
      "This agent's rewrite:\n",
      "The explanation is unfaithful because it incorrectly states \"False\" as feedback for the claim, which is not supported by the evidence. The explanation does not address the specific weaknesses in the reasoning provided.  \n",
      "\n",
      "Problematic sentence: \"The question-answer pairs disprove the claim by showing that friendship has been studied in disciplines beyond sociology.\"  \n",
      "Reason: This sentence is factually accurate based on the evidence, but the explanation fails to recognize that the claim is about \"only\" sociology, and the QA pairs show that other disciplines have also studied friendship, thus disproving the claim. However, the explanation does not explicitly point out this logical incoherence in the claim's wording.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = select_random_sample(incomp_run)\n",
    "regenerate_cross()\n",
    "pprint_results_cross(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ba69e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "/gnu/store/zmxsy7mhaq0yq9kh5w4yc6sq2g8kzb2x-python-3.11.14/bin/python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
