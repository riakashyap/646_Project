{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e072dbc",
   "metadata": {},
   "source": [
    "# MADR prompt evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bf871",
   "metadata": {},
   "source": [
    "This notebook is inspired by [this research](https://cacm.acm.org/research/from-prompt-engineering-to-prompt-science-with-humans-in-the-loop/) on prompt engineering with humans in the loop.\n",
    "\n",
    "We are faced with the issue: the existing prompts in the MADR paper are ill-defined and intended for a different complexity model. While fine-tuning is an enticing option, we lack a dataset of questions, evidence, veracity claim, and target feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3384cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822d5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_jupyter_notebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        return shell == 'ZMQInteractiveShell'\n",
    "    except NameError:\n",
    "        return False\n",
    "IN_JUPYTER = in_jupyter_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581f57b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_clients import LlamaCppClient\n",
    "from src import config\n",
    "from src.madr import run_madr\n",
    "from src.parsers import parse_ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41daea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dir = config.PROMPTS_DIR / \"custom\"\n",
    "client = LlamaCppClient(prompts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b7a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corag_run = None\n",
    "relative_path = os.path.join('assets', '20251123T192328--metrics__100.json')\n",
    "with open(relative_path, 'r', encoding='utf-8') as f:\n",
    "    corag_run = json.load(f)[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc4d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e91e1d",
   "metadata": {},
   "source": [
    "## An initial codebook for Debater 1, 2, Cross-Agent, Judge, Refiner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3bf95c",
   "metadata": {},
   "source": [
    "A 'codebook' (or criteria) in the above research encapsulates the desired outcomes for generation. It systematically allows any human-in-the-loop to quickly and methodically conclude when a generated response does or does not meet the specifications.\n",
    "\n",
    "The starting criteria, validated by multiple researchers for each model, is defined below:\n",
    "\n",
    "1. **Debater 1, given an explanation and evidence:**\n",
    "- Does distinguish between the following types of errors in the explanation:\n",
    "  1) misrepresentations of factual details in the evidence\n",
    "  2) introduces or misrepresents events not present in the evidence\n",
    "  3) introduces or misrepresents noun phrases in the evidence, changing semantic meaning\n",
    "  4) logic/reasoning inconsistencies in the explanation\n",
    "  5) includes information not relevant to the claim/evidence or beyond what the evidence supports\n",
    "- Does NOT attempt to correct errors in the explanation to align with evidence.\n",
    "- Identifies ALL such errors\n",
    "- Does NOT output non-existent errors.\n",
    "- Answers tersely\n",
    "\n",
    "*note, it is NOT a requirement the model names the error types.*\n",
    "\n",
    "2. **Debater 2, given an explanation and evidence:**\n",
    "- Identifies general weaknesses which reduce the faithfulness of an explanation to the claim/evidence.\n",
    "- Generates explanations for why identified issues are unfaithful:\n",
    "  1) Factually inaccurate\n",
    "  2) logically inaccurate\n",
    "  3) irrelevant\n",
    "  4) incomplete\n",
    "  5) incoherent\n",
    "- Does NOT attempt to correct errors in the explanation to align with evidence.\n",
    "- Does provide actionable feedback\n",
    "- Identifies ALL such errors\n",
    "- Does NOT output non-existent errors.\n",
    "- Answers tersely\n",
    "\n",
    "3. **Cross-Agent, given the original explanation, and a primary and secondary set of feedback:**\n",
    "- uses the original explanation to:\n",
    "  1) recognize invalid feedback in the primary feedback, and incorporate corrections from the secondary feedback.\n",
    "  2) does NOT include invalid feedback from the secondary feedback.\n",
    "- outputs ONLY the modified primary feedback\n",
    "- makes MINIMAL changes, rather than copying the secondary feedback\n",
    "\n",
    "*note, the input to this agent is up to debate. I am making the assumption that only including the explanation rather than the full claim and Q/A pairs balances the agent's ability to refine the feedback given a source of ground truth while keeping the prompt context small.*\n",
    "\n",
    "4. **Judge, given two sets of feedback:**\n",
    "- identifies major discrepancies between the input feedback:\n",
    "  1) identifies different errors or issues\n",
    "  2) provides inconsistent error descriptions\n",
    "  3) whether any suggested fixes are not equivalent\n",
    "- if discrepancies exist, output contains the word 'FALSE'\n",
    "- else, output contains the word 'TRUE'\n",
    "\n",
    "5. **Refinement Agent, given two sets of feedback and an explanation:**\n",
    "- rewrites the explanation to align with both sets of feedback\n",
    "- Does NOT output anything other than the refined explanation\n",
    "- Does NOT remove the component containing the verdict in the original explanation, but can 'flip' it, i.e.: true -> false."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba9d78",
   "metadata": {},
   "source": [
    "## Initial Custom Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d3757f",
   "metadata": {},
   "source": [
    "Because it is already established the original prompts do not meet these requirements, we will start by creating our initial custom prompts.\n",
    "\n",
    "To do this, we simply feed the codebook itself into an LLM to generate the prompt. I will use this boilerplate:\n",
    "\n",
    "> You are tasked with taking a set of requirements and phrasing it as a concise prompt for an LLM. In this initial generation, the codebook serves as the primary material for construction, while a previous prompt is provided purely to fill in any gaps left by the codebook. Extra output besides the prompt is unnecessary and unwarranted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "090aa541",
   "metadata": {},
   "outputs": [],
   "source": [
    "madr_init_fb1_system_prompt = \"\"\"Given the following claim, explanation, and QA-pair evidence, identify all errors in the explanation using this error typology:\n",
    "\n",
    "1. misrepresented factual details;\n",
    "2. introduced or misrepresented events;\n",
    "3. introduced or misrepresented noun phrases;\n",
    "4. reasoning/logic inconsistencies;\n",
    "5. irrelevant or unsupported information.\n",
    "\n",
    "Do not attempt to correct the explanation. Do not invent errors.\n",
    "Output a terse list of all real errors, each labeled with its category.\"\"\"\n",
    "\n",
    "madr_init_fb2_system_prompt = \"\"\"Given the following claim, explanation, and QA-pair evidence, identify all weaknesses in the explanation that reduce the faithfulness to the claim/evidence.\n",
    "\n",
    "For each problematic sentence in the explanation, quote it, explain why it is unfaithful (factually inaccurate, logically inaccurate, irrelevant, incomplete, or incoherent), and provide actional feedback without correcting or rewriting the explanation.\n",
    "\n",
    "Do not invent issues. Keep responses terse.\"\"\"\n",
    "\n",
    "madr_cross_fb_system_prompt = \"\"\"Given the following explanation plus primary and secondary feedback, revise the primary feedback using the explanation as ground truth.\n",
    "\n",
    "Remove any invalid points in the primary feedback, add only valid corrections supported by the secondary feedback, and ignore invalid secondary feedback.\n",
    "\n",
    "Make minimal edits; do not copy secondary feedback verbatim. Output only the corrected primary feedback.\"\"\"\n",
    "\n",
    "madr_cross_fb_user_prompt = \"\"\"Primary feedback:\n",
    "{}\n",
    "\n",
    "Secondary feedback:\n",
    "{}\"\"\"\n",
    "\n",
    "madr_judge_system_prompt = \"\"\"Given the following sets of feedback, compare them for major discrepancies: whether they identify idfferent errors, give inconsistent error descriptions, or propose nonequivalent fixes.\n",
    "\n",
    "If any discrepancy exists, output FALSE; otherwise output TRUE. Output only this word.\"\"\"\n",
    "\n",
    "\n",
    "madr_revise_system_prompt = \"\"\"Given the following explanation and two sets of feedback, rewrite the explanation so it aligns with both feedback sets.\n",
    "\n",
    "Keep the verdict component but allow flipping its valud. Output only the refined explanation and nothing else.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b93230d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_prompt(key, system_prompt_str):\n",
    "    client._prompts[key] = (\n",
    "        client._prompts[key][0],\n",
    "        system_prompt_str\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb3d8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_prompt(\"madr_init_fb1\", madr_init_fb1_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8d31088",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Claim: {}\\n\\nQuestion-Answer Pairs:\\n{}\\n\\nExplanation: {}\\n\\nFeedback:',\n",
       " 'Given the following claim, explanation, and QA-pair evidence, identify all errors in the explanation using this error typology:\\n\\n1. misrepresented factual details;\\n2. introduced or misrepresented events;\\n3. introduced or misrepresented noun phrases;\\n4. reasoning/logic inconsistencies;\\n5. irrelevant or unsupported information.\\n\\nDo not attempt to correct the explanation. Do not invent errors.\\nOutput a terse list of all real errors, each labeled with its category.')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client._prompts[\"madr_init_fb1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59cef96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_prompt(\"madr_init_fb2\", madr_init_fb2_system_prompt)\n",
    "replace_prompt(\"madr_judge\", madr_judge_system_prompt)\n",
    "replace_prompt(\"madr_revise\", madr_revise_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27d60f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "client._prompts[\"madr_cross_fb\"] = (\n",
    "    madr_cross_fb_user_prompt,\n",
    "    madr_cross_fb_system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921dac7",
   "metadata": {},
   "source": [
    "## Refining Base MADR Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45973ad8",
   "metadata": {},
   "source": [
    "I have a set of 100 claims run through the baseline pipeline. When evaluating the MADR pipeline *I will assume that the answers generated by the CoRAG answering agent are always factual.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6793a1d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim_id': 0,\n",
       "  'claim': 'In 2008, Jeff Ament released a solo record.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [[\"When was Jeff Ament's solo record released?\",\n",
       "    'September 16, 2008.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'True.  \\nInconclusive'},\n",
       " {'claim_id': 1,\n",
       "  'claim': 'Franklin D. Roosevelt had a family that originated in New York.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [[\"What is the origin of Franklin D. Roosevelt's family?\",\n",
       "    \"Franklin D. Roosevelt's family originated from Dutch settlers in New York, with his ancestors tracing back to the 17th century. His direct lineage includes prominent figures such as his father, James Roosevelt, and his mother, Eleanor Roosevelt, who were part of influential American families.\"]],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'True. The question-answer pairs fully confirm the claim that Franklin D. Roosevelt had a family that originated in New York.'},\n",
       " {'claim_id': 2,\n",
       "  'claim': 'A phylogenetic tree is a branching diagram.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [['What is the primary purpose of a phylogenetic tree according to the statement?',\n",
       "    'The primary purpose of a phylogenetic tree is to represent a hypothesis about the evolutionary ancestry of a set of genes, species, or other taxa.'],\n",
       "   ['What type of diagram is a phylogenetic tree described as in the statement?',\n",
       "    'A phylogenetic tree is described as a branching diagram or \"tree\" in the statement.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'The question-answer pairs fully confirm the claim.  \\nTrue'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corag_run[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f337828",
   "metadata": {},
   "source": [
    "### Debater 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56cf0dc",
   "metadata": {},
   "source": [
    "This marks our first iteration of refining the codebook. To evaluate the debater, we need a reasonable number of responses. In this case, I will generate 10, for 5 random claims of each type (TRUE/FALSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a52daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_sample(data, count=5):\n",
    "    supports = [i for i in data if i[\"true_label\"] == \"SUPPORTS\"]\n",
    "    refutes = [i for i in data if i[\"true_label\"] == \"REFUTES\"]\n",
    "\n",
    "    selection_supports = random.sample(supports, count)\n",
    "    selection_refutes = random.sample(refutes, count)\n",
    "\n",
    "    return selection_supports + selection_refutes\n",
    "sample = select_random_sample(corag_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81fd0b",
   "metadata": {},
   "source": [
    "Now I'll generate and cache the debater's responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "366129a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in sample:\n",
    "    claim = c[\"claim\"]\n",
    "    qa_pairs = c[\"qa_pairs\"]\n",
    "    verdict_raw = c[\"verdict_raw\"]\n",
    "    fb = client.send_prompt(\"madr_init_fb1\", [claim, qa_pairs, verdict_raw])\n",
    "    c[\"r\"] = fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb724ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim_id': 40,\n",
       "  'claim': 'Jun Ji-hyun is in the South Korean film called Windstruck.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [['Is Jun Ji-hyun featured in the South Korean film \"Windstruck\"?',\n",
       "    'Answer: Yes.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'True.',\n",
       "  'fb1': '\\n\\n[]'},\n",
       " {'claim_id': 7,\n",
       "  'claim': 'A subtype of anti-nuclear antibodies are anti-Ro antibodies.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [['Is the statement accurate that a subtype of anti-nuclear antibodies specifically includes anti-Ro antibodies?',\n",
       "    'Answer: Yes.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'True.  \\nInconclusive',\n",
       "  'fb1': '\\n\\n- **Introduced or misrepresented noun phrases**: \"Inconclusive\" misrepresents the explanation\\'s adequacy.'},\n",
       " {'claim_id': 1,\n",
       "  'claim': 'Franklin D. Roosevelt had a family that originated in New York.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [[\"What is the origin of Franklin D. Roosevelt's family?\",\n",
       "    \"Franklin D. Roosevelt's family originated from Dutch settlers in New York, with his ancestors tracing back to the 17th century. His direct lineage includes prominent figures such as his father, James Roosevelt, and his mother, Eleanor Roosevelt, who were part of influential American families.\"]],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'True. The question-answer pairs fully confirm the claim that Franklin D. Roosevelt had a family that originated in New York.',\n",
       "  'fb1': '\\n\\n- **Introduced or misrepresented noun phrases**: \"Dutch settlers in New York\" is not a recognized historical term for Franklin D. Roosevelt\\'s family origin.  \\n- **Reasoning/logic inconsistencies**: The explanation assumes the answer to the question \"What is the origin of Franklin D. Roosevelt\\'s family?\" fully confirms the claim, but the answer provided does not directly confirm the claim in a factual, unambiguous way.'},\n",
       " {'claim_id': 47,\n",
       "  'claim': 'Taal was romantic.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'NOT ENOUGH INFO',\n",
       "  'qa_pairs': [['What historical evidence supports the assertion that Taal was romantic?',\n",
       "    'question too specific'],\n",
       "   ['When did Taal exhibit romantic behavior?', 'question too specific'],\n",
       "   ['Who is considered to have been romantic in the claim about Taal?',\n",
       "    'question too specific']],\n",
       "  'verdict_int': 2,\n",
       "  'verdict_raw': 'Inconclusive',\n",
       "  'fb1': '\\n\\n[]'},\n",
       " {'claim_id': 17,\n",
       "  'claim': 'Bill Cosby was a comedian who did stand-up.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'NOT ENOUGH INFO',\n",
       "  'qa_pairs': [['Who is considered a comedian who did stand-up according to the statement?',\n",
       "    'Gabriel Iglesias is considered a comedian who did stand-up.'],\n",
       "   ['When did Bill Cosby start performing stand-up comedy?',\n",
       "    'The provided sources do not specify when Bill Cosby started performing stand-up comedy. Therefore, the answer is \"question too specific.\"']],\n",
       "  'verdict_int': 2,\n",
       "  'verdict_raw': \"The question-answer pairs do not fully confirm or disprove the claim as they only mention Gabriel Iglesias as a stand-up comedian and lack specific information about Bill Cosby's start in stand-up comedy.  \\nInconclusive\",\n",
       "  'fb1': '\\n\\n- **Introduced or misrepresented noun phrases**: \"Gabriel Iglesias is considered a comedian who did stand-up\" is not directly relevant to the claim about Bill Cosby.  \\n- **Reasoning/logic inconsistencies**: The explanation incorrectly assumes that the absence of specific information about Bill Cosby\\'s start in stand-up comedy means the question-answer pairs do not confirm or disprove the claim, when the claim is about Bill Cosby being a stand-up comedian, not about when he started.'},\n",
       " {'claim_id': 65,\n",
       "  'claim': 'Peter Cetera was entirely absent from the album Chicago Transit Authority.',\n",
       "  'true_label': 'REFUTES',\n",
       "  'predicted_label': 'REFUTES',\n",
       "  'qa_pairs': [[\"Is Peter Cetera's absence from the album Chicago Transit Authority accurate?\",\n",
       "    \"No, Peter Cetera's absence from the album *Chicago Transit Authority* is not accurate. He was a member of the band at the time and contributed to the album, though his role as a bassist and vocalist is more prominently featured on later albums.\"]],\n",
       "  'verdict_int': 0,\n",
       "  'verdict_raw': 'False. The question-answer pair directly refutes the claim by stating that Peter Cetera was not entirely absent from the album *Chicago Transit Authority* and was a member of the band at the time, contributing to the album.',\n",
       "  'fb1': '\\n\\n- misrepresented factual details (Claim states Cetera was \"entirely absent,\" but the explanation and QA-pair state he was a member and contributed to the album)'},\n",
       " {'claim_id': 64,\n",
       "  'claim': 'Gustave Eiffel has yet to retire from engineering.',\n",
       "  'true_label': 'REFUTES',\n",
       "  'predicted_label': 'REFUTES',\n",
       "  'qa_pairs': [['When did Gustave Eiffel retire from engineering?',\n",
       "    'Gustave Eiffel retired from engineering after his contributions to meteorology and aerodynamics, but the exact date of his retirement is not specified in the provided sources.'],\n",
       "   [\"What was Gustave Eiffel's role in meteorology and aerodynamics after retiring from engineering?\",\n",
       "    'After retiring from engineering, Gustave Eiffel focused on research into meteorology and aerodynamics, making significant contributions in both fields.'],\n",
       "   [\"What was Gustave Eiffel's primary occupation before he retired from engineering?\",\n",
       "    \"Gustave Eiffel's primary occupation before he retired from engineering was a civil engineer and architect.\"]],\n",
       "  'verdict_int': 0,\n",
       "  'verdict_raw': 'The question-answer pairs do not provide a definitive date for Gustave Eiffel\\'s retirement, but they confirm he retired from engineering and transitioned to meteorology and aerodynamics, making it impossible to confirm the claim that he \"has yet to retire.\"  \\nVerdict: False',\n",
       "  'fb1': '\\n\\n- **Introduced or misrepresented events**: The explanation claims it is \"impossible to confirm the claim that he \\'has yet to retire\\'\" based on the absence of a retirement date, which is a misrepresentation of the claim. The claim is about present status, not past events.  \\n- **Reasoning/logic inconsistencies**: The explanation incorrectly infers that the absence of a retirement date implies he has not retired, which is a logical inconsistency.  \\n- **Irrelevant or unsupported information**: The explanation introduces the idea that the lack of a retirement date makes it impossible to confirm the claim, which is not supported by the evidence provided.'},\n",
       " {'claim_id': 58,\n",
       "  'claim': 'Tequila is made from a plant primarily found around the woods.',\n",
       "  'true_label': 'REFUTES',\n",
       "  'predicted_label': 'REFUTES',\n",
       "  'qa_pairs': [['What plant is tequila primarily made from?',\n",
       "    'The plant tequila is primarily made from is the blue agave plant.']],\n",
       "  'verdict_int': 0,\n",
       "  'verdict_raw': 'False. The blue agave plant is not primarily found around the woods; it is primarily found in the desert regions of Mexico.  \\nInconclusive',\n",
       "  'fb1': '\\n\\n- misrepresented factual details (Claim says tequila is made from a plant \"primarily found around the woods\"; Explanation correctly states that blue agave is found in desert regions of Mexico, thus refuting the claim)  \\n- introduced or misrepresented noun phrases (\"the plant tequila is primarily made from is the blue agave plant\" is accurate, but the Explanation does not directly address this part of the claim)  \\n- reasoning/logic inconsistencies (The Explanation correctly refutes the claim by showing that blue agave is not found around the woods, but the claim\\'s main point about the plant being used for tequila is not directly addressed)'},\n",
       " {'claim_id': 97,\n",
       "  'claim': 'The People vs. Larry Flynt is an unproduced screenplay.',\n",
       "  'true_label': 'REFUTES',\n",
       "  'predicted_label': 'REFUTES',\n",
       "  'qa_pairs': [['Is \"The People vs. Larry Flynt\" actually an unproduced screenplay or has it been produced in some form?',\n",
       "    'Answer: \"The People vs. Larry Flynt\" has been produced in some form; it is a 1996 American biographical drama film.']],\n",
       "  'verdict_int': 0,\n",
       "  'verdict_raw': 'False. The question-answer pairs directly disprove the claim by stating that \"The People vs. Larry Flynt\" has been produced as a 1996 film.',\n",
       "  'fb1': '\\n\\n- misrepresented factual details: The explanation incorrectly states that the claim is false based on the QA pair, but the QA pair only provides evidence that the film was produced, not that the screenplay was unproduced.'},\n",
       " {'claim_id': 56,\n",
       "  'claim': 'Matt Damon is not an actor.',\n",
       "  'true_label': 'REFUTES',\n",
       "  'predicted_label': 'NOT ENOUGH INFO',\n",
       "  'qa_pairs': [['Who is considered an actor in the context of this statement?',\n",
       "    'The term \"actor\" in the context of this statement refers to individuals or entities involved in the process of problematization, particularly within the framework of actor-network theory (ANT). These actors are participants in the network and are involved in the interdefinition of actors and the definition of the problem/topic/action program. Specific actors are not named in the provided text.'],\n",
       "   ['What is the definition of \"actor\" used in the statement?',\n",
       "    'question too specific'],\n",
       "   ['What is the basis for the assertion that Matt Damon is not an actor?',\n",
       "    'question too specific']],\n",
       "  'verdict_int': 2,\n",
       "  'verdict_raw': 'Inconclusive',\n",
       "  'fb1': '\\n\\n- Introduced or misrepresented noun phrases: \"actor\" is redefined in the explanation to refer to entities in actor-network theory, which is not the standard definition of \"actor\" in the context of Matt Damon.'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68659246",
   "metadata": {},
   "source": [
    "We now need a section of code to pretty-print each claim out, making it easy to judge the effectiveness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be0c38ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Jun Ji-hyun is in the South Korean film called Windstruck.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Is Jun Ji-hyun featured in the South Korean film \"Windstruck\"?\n",
      "\tAnswer: Yes.\n",
      "\n",
      "The original verdict: True.\n",
      "\n",
      "This agent's feedback:\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "2. A subtype of anti-nuclear antibodies are anti-Ro antibodies.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Is the statement accurate that a subtype of anti-nuclear antibodies specifically includes anti-Ro antibodies?\n",
      "\tAnswer: Yes.\n",
      "\n",
      "The original verdict: True.  \n",
      "Inconclusive\n",
      "\n",
      "This agent's feedback:\n",
      "- **Introduced or misrepresented noun phrases**: \"Inconclusive\" misrepresents the explanation's adequacy.\n",
      "\n",
      "\n",
      "\n",
      "3. Franklin D. Roosevelt had a family that originated in New York.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What is the origin of Franklin D. Roosevelt's family?\n",
      "\tFranklin D. Roosevelt's family originated from Dutch settlers in New York, with his ancestors tracing back to the 17th century. His direct lineage includes prominent figures such as his father, James Roosevelt, and his mother, Eleanor Roosevelt, who were part of influential American families.\n",
      "\n",
      "The original verdict: True. The question-answer pairs fully confirm the claim that Franklin D. Roosevelt had a family that originated in New York.\n",
      "\n",
      "This agent's feedback:\n",
      "- **Introduced or misrepresented noun phrases**: \"Dutch settlers in New York\" is not a recognized historical term for Franklin D. Roosevelt's family origin.  \n",
      "- **Reasoning/logic inconsistencies**: The explanation assumes the answer to the question \"What is the origin of Franklin D. Roosevelt's family?\" fully confirms the claim, but the answer provided does not directly confirm the claim in a factual, unambiguous way.\n",
      "\n",
      "\n",
      "\n",
      "4. Taal was romantic.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What historical evidence supports the assertion that Taal was romantic?\n",
      "\tquestion too specific\n",
      "2.\tQuestion: When did Taal exhibit romantic behavior?\n",
      "\tquestion too specific\n",
      "3.\tQuestion: Who is considered to have been romantic in the claim about Taal?\n",
      "\tquestion too specific\n",
      "\n",
      "The original verdict: Inconclusive\n",
      "\n",
      "This agent's feedback:\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "5. Bill Cosby was a comedian who did stand-up.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Who is considered a comedian who did stand-up according to the statement?\n",
      "\tGabriel Iglesias is considered a comedian who did stand-up.\n",
      "2.\tQuestion: When did Bill Cosby start performing stand-up comedy?\n",
      "\tThe provided sources do not specify when Bill Cosby started performing stand-up comedy. Therefore, the answer is \"question too specific.\"\n",
      "\n",
      "The original verdict: The question-answer pairs do not fully confirm or disprove the claim as they only mention Gabriel Iglesias as a stand-up comedian and lack specific information about Bill Cosby's start in stand-up comedy.  \n",
      "Inconclusive\n",
      "\n",
      "This agent's feedback:\n",
      "- **Introduced or misrepresented noun phrases**: \"Gabriel Iglesias is considered a comedian who did stand-up\" is not directly relevant to the claim about Bill Cosby.  \n",
      "- **Reasoning/logic inconsistencies**: The explanation incorrectly assumes that the absence of specific information about Bill Cosby's start in stand-up comedy means the question-answer pairs do not confirm or disprove the claim, when the claim is about Bill Cosby being a stand-up comedian, not about when he started.\n",
      "\n",
      "\n",
      "\n",
      "6. Peter Cetera was entirely absent from the album Chicago Transit Authority.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Is Peter Cetera's absence from the album Chicago Transit Authority accurate?\n",
      "\tNo, Peter Cetera's absence from the album *Chicago Transit Authority* is not accurate. He was a member of the band at the time and contributed to the album, though his role as a bassist and vocalist is more prominently featured on later albums.\n",
      "\n",
      "The original verdict: False. The question-answer pair directly refutes the claim by stating that Peter Cetera was not entirely absent from the album *Chicago Transit Authority* and was a member of the band at the time, contributing to the album.\n",
      "\n",
      "This agent's feedback:\n",
      "- misrepresented factual details (Claim states Cetera was \"entirely absent,\" but the explanation and QA-pair state he was a member and contributed to the album)\n",
      "\n",
      "\n",
      "\n",
      "7. Gustave Eiffel has yet to retire from engineering.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: When did Gustave Eiffel retire from engineering?\n",
      "\tGustave Eiffel retired from engineering after his contributions to meteorology and aerodynamics, but the exact date of his retirement is not specified in the provided sources.\n",
      "2.\tQuestion: What was Gustave Eiffel's role in meteorology and aerodynamics after retiring from engineering?\n",
      "\tAfter retiring from engineering, Gustave Eiffel focused on research into meteorology and aerodynamics, making significant contributions in both fields.\n",
      "3.\tQuestion: What was Gustave Eiffel's primary occupation before he retired from engineering?\n",
      "\tGustave Eiffel's primary occupation before he retired from engineering was a civil engineer and architect.\n",
      "\n",
      "The original verdict: The question-answer pairs do not provide a definitive date for Gustave Eiffel's retirement, but they confirm he retired from engineering and transitioned to meteorology and aerodynamics, making it impossible to confirm the claim that he \"has yet to retire.\"  \n",
      "Verdict: False\n",
      "\n",
      "This agent's feedback:\n",
      "- **Introduced or misrepresented events**: The explanation claims it is \"impossible to confirm the claim that he 'has yet to retire'\" based on the absence of a retirement date, which is a misrepresentation of the claim. The claim is about present status, not past events.  \n",
      "- **Reasoning/logic inconsistencies**: The explanation incorrectly infers that the absence of a retirement date implies he has not retired, which is a logical inconsistency.  \n",
      "- **Irrelevant or unsupported information**: The explanation introduces the idea that the lack of a retirement date makes it impossible to confirm the claim, which is not supported by the evidence provided.\n",
      "\n",
      "\n",
      "\n",
      "8. Tequila is made from a plant primarily found around the woods.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What plant is tequila primarily made from?\n",
      "\tThe plant tequila is primarily made from is the blue agave plant.\n",
      "\n",
      "The original verdict: False. The blue agave plant is not primarily found around the woods; it is primarily found in the desert regions of Mexico.  \n",
      "Inconclusive\n",
      "\n",
      "This agent's feedback:\n",
      "- misrepresented factual details (Claim says tequila is made from a plant \"primarily found around the woods\"; Explanation correctly states that blue agave is found in desert regions of Mexico, thus refuting the claim)  \n",
      "- introduced or misrepresented noun phrases (\"the plant tequila is primarily made from is the blue agave plant\" is accurate, but the Explanation does not directly address this part of the claim)  \n",
      "- reasoning/logic inconsistencies (The Explanation correctly refutes the claim by showing that blue agave is not found around the woods, but the claim's main point about the plant being used for tequila is not directly addressed)\n",
      "\n",
      "\n",
      "\n",
      "9. The People vs. Larry Flynt is an unproduced screenplay.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Is \"The People vs. Larry Flynt\" actually an unproduced screenplay or has it been produced in some form?\n",
      "\tAnswer: \"The People vs. Larry Flynt\" has been produced in some form; it is a 1996 American biographical drama film.\n",
      "\n",
      "The original verdict: False. The question-answer pairs directly disprove the claim by stating that \"The People vs. Larry Flynt\" has been produced as a 1996 film.\n",
      "\n",
      "This agent's feedback:\n",
      "- misrepresented factual details: The explanation incorrectly states that the claim is false based on the QA pair, but the QA pair only provides evidence that the film was produced, not that the screenplay was unproduced.\n",
      "\n",
      "\n",
      "\n",
      "10. Matt Damon is not an actor.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Who is considered an actor in the context of this statement?\n",
      "\tThe term \"actor\" in the context of this statement refers to individuals or entities involved in the process of problematization, particularly within the framework of actor-network theory (ANT). These actors are participants in the network and are involved in the interdefinition of actors and the definition of the problem/topic/action program. Specific actors are not named in the provided text.\n",
      "2.\tQuestion: What is the definition of \"actor\" used in the statement?\n",
      "\tquestion too specific\n",
      "3.\tQuestion: What is the basis for the assertion that Matt Damon is not an actor?\n",
      "\tquestion too specific\n",
      "\n",
      "The original verdict: Inconclusive\n",
      "\n",
      "This agent's feedback:\n",
      "- Introduced or misrepresented noun phrases: \"actor\" is redefined in the explanation to refer to entities in actor-network theory, which is not the standard definition of \"actor\" in the context of Matt Damon.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pprint_results(sample):\n",
    "    for i, s in enumerate(sample):\n",
    "        claim = s[\"claim\"]\n",
    "        qa_pairs = s[\"qa_pairs\"]\n",
    "        explanation = s[\"verdict_raw\"]\n",
    "        madr_agent = s[\"fb1\"].strip()\n",
    "        print(f\"{i+1}. {claim}\\n\")\n",
    "        print(f\"Available evidence for this claim, in the form of Q/A pairs:\")\n",
    "        for j, e in enumerate(qa_pairs):\n",
    "            print(f\"{j+1}.\\tQuestion: {e[0]}\\n\\t{e[1]}\")\n",
    "        print()\n",
    "        print(f\"The original verdict: {explanation}\\n\")\n",
    "        print(f\"This agent's feedback:\\n{madr_agent}\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "pprint_results(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99729e",
   "metadata": {},
   "source": [
    "As a reviewer, I note:\n",
    "1. the debater's response to claim 5 and 6 points out a noun phrase inconsistency in the Q/A pairs, not the explanation. The codebook should be modified to ignore errors in the Q/A pairs.\n",
    "2. the debater in fails to identify the introduction of evidence by the explanation.\n",
    "3. the debater fails in cases where the CoRAG pipeline also clearly failed (what to do about this?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c1f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "/gnu/store/zmxsy7mhaq0yq9kh5w4yc6sq2g8kzb2x-python-3.11.14/bin/python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
