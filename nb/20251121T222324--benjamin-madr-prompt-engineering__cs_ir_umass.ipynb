{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e072dbc",
   "metadata": {},
   "source": [
    "# MADR prompt evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bf871",
   "metadata": {},
   "source": [
    "This notebook is inspired by [this research](https://cacm.acm.org/research/from-prompt-engineering-to-prompt-science-with-humans-in-the-loop/) on prompt engineering with humans in the loop.\n",
    "\n",
    "We are faced with the issue: the existing prompts in the MADR paper are ill-defined and intended for a different complexity model. While fine-tuning is an enticing option, we lack a dataset of questions, evidence, veracity claim, and target feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3384cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822d5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_jupyter_notebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        return shell == 'ZMQInteractiveShell'\n",
    "    except NameError:\n",
    "        return False\n",
    "IN_JUPYTER = in_jupyter_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581f57b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_clients import LlamaCppClient\n",
    "from src import config\n",
    "from src.madr import run_madr\n",
    "from src.utils import parse_ternary, get_prompt_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41daea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = get_prompt_files(config.MADR_DIR)\n",
    "client = LlamaCppClient(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b7a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corag_run = None\n",
    "relative_path = os.path.join('assets', '20251123T192328--metrics__100.json')\n",
    "with open(relative_path, 'r', encoding='utf-8') as f:\n",
    "    corag_run = json.load(f)[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc4d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e91e1d",
   "metadata": {},
   "source": [
    "## An initial codebook for Debater 1, 2, Cross-Agent, Judge, Refiner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3bf95c",
   "metadata": {},
   "source": [
    "A 'codebook' (or criteria) in the above research encapsulates the desired outcomes for generation. It systematically allows any human-in-the-loop to quickly and methodically conclude when a generated response does or does not meet the specifications.\n",
    "\n",
    "The starting criteria, validated by multiple researchers for each model, is defined below:\n",
    "\n",
    "1. **Debater 1, given an explanation and evidence:**\n",
    "- Does distinguish between the following types of errors in the explanation:\n",
    "  1) misrepresentations of factual details in the evidence\n",
    "  2) introduces or misrepresents events not present in the evidence\n",
    "  3) introduces or misrepresents noun phrases in the evidence, changing semantic meaning\n",
    "  4) logic/reasoning inconsistencies in the explanation\n",
    "  5) includes information not relevant to the claim/evidence or beyond what the evidence supports\n",
    "  6) does not adequately justify its position\n",
    "- Provides actionable feedback, rather than attempting to correct errors in the explanation to align with evidence.\n",
    "- ONLY considers errors in the explanation, not the claim or Q/A pairs.\n",
    "- Identifies ALL such errors\n",
    "- Does NOT output non-existent errors.\n",
    "- Does NOT merely restate the types of errors without context.\n",
    "- Does NOT merely restate the types of errors without context.\n",
    "- Does NOT merely restate the types of errors without context.\n",
    "- Does NOT merely restate the types of errors without context.\n",
    "- Does NOT merely restate the types of errors without context.\n",
    "- Labels all errors with the typology.\n",
    "- Answers tersely\n",
    "\n",
    "*note, it is NOT a requirement the model names the error types.*\n",
    "\n",
    "2. **Debater 2, given an explanation and evidence:**\n",
    "- Identifies general weaknesses which reduce the faithfulness of an explanation to the claim/evidence.\n",
    "- Generates explanations for why identified issues are unfaithful:\n",
    "  1) Factually inaccurate\n",
    "  2) logically inaccurate\n",
    "  3) irrelevant\n",
    "  4) incoherent\n",
    "  5) incomplete\n",
    "- Provides actionable feedback, rather than attempting to correct errors in the explanation to align with evidence.\n",
    "- ONLY considers errors in the explanation, not the claim or Q/A pairs.\n",
    "- Does not include a restatement of the explanation\n",
    "- Avoids logical assumptions about the claim\n",
    "- Considers the evidence as the source of ground truth\n",
    "- Does provide actionable feedback\n",
    "- Identifies ALL such errors\n",
    "- Does NOT output non-existent errors.\n",
    "- Answers tersely\n",
    "\n",
    "3. **Cross-Agent, given the original claim, evidence, explanation, and a primary and secondary set of feedback:**\n",
    "- uses the claim, evidence, and explanation to:\n",
    "  1) recognize invalid feedback in the primary feedback, and incorporate corrections from the secondary feedback.\n",
    "  2) does NOT include invalid feedback from the secondary feedback.\n",
    "- outputs ONLY the modified primary feedback\n",
    "- does NOT merely copy the secondary feedback\n",
    "\n",
    "*note, the input to this agent is up to debate. I am making the assumption that only including the explanation rather than the full claim and Q/A pairs balances the agent's ability to refine the feedback given a source of ground truth while keeping the prompt context small.*\n",
    "\n",
    "4. **Judge, given two sets of feedback:**\n",
    "- identifies major discrepancies between the input feedback:\n",
    "  1) identifies different errors or issues\n",
    "  2) provides inconsistent error descriptions\n",
    "  3) whether any suggested fixes are not equivalent\n",
    "- if discrepancies exist, output contains the word 'FALSE'\n",
    "- else, output contains the word 'TRUE'\n",
    "\n",
    "5. **Refinement Agent, given two sets of feedback and an explanation:**\n",
    "- rewrites the explanation to align with both sets of feedback\n",
    "- Does NOT output anything other than the refined explanation\n",
    "- Does NOT remove the component containing the verdict in the original explanation, but can 'flip' it, i.e.: true -> false."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba9d78",
   "metadata": {},
   "source": [
    "## Initial Custom Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d3757f",
   "metadata": {},
   "source": [
    "Because it is already established the original prompts do not meet these requirements, we will start by creating our initial custom prompts.\n",
    "\n",
    "To do this, we simply feed the codebook itself into an LLM to generate the prompt. I will use this boilerplate:\n",
    "\n",
    "> You are tasked with taking a set of requirements and phrasing it as a concise prompt for an LLM. In this initial generation, the codebook serves as the primary material for construction, while a previous prompt is provided purely to fill in any gaps left by the codebook. Extra output besides the prompt is unnecessary and unwarranted.\n",
    "\n",
    "For further refinement:\n",
    "\n",
    "> You are tasked with taking a set of requirements and phrasing it as a concise prompt for an LLM. In this initial generation, the codebook serves as the primary material for construction. A repeated point stresses emphasis. Extra output besides the prompt is unnecessary and unwarranted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "090aa541",
   "metadata": {},
   "outputs": [],
   "source": [
    "madr_init_fb1_system_prompt = \"\"\"Evaluate the following explanation against the given evidence by identifying all errors it contains, using these labels:\n",
    "\n",
    "- misrepresented factual details;\n",
    "- introduced/misrepresented events not in evidence;\n",
    "- introduced/misrepresented noun phrases altering meaning;\n",
    "- logic/reasoning inconsistencies;\n",
    "- irrelevant or unsupported information;\n",
    "- inadequate justification.\n",
    "\n",
    "Only analyze the explanation for errors (not the claim or Q/A). Do not invent errors. Provide actionable feedback without rewriting or correcting the explanation. Avoid restating the error types without context.\"\"\"\n",
    "\n",
    "madr_init_fb2_system_prompt = \"\"\"Given the following claim, explanation, and QA-pair evidence, identify all weaknesses in the explanation that reduce the faithfulness to the claim/evidence.\n",
    "\n",
    "For each problematic sentence in the explanation, quote it (inline, no need to restate the full explanation), explain why it is unfaithful (factually inaccurate, logically inaccurate, irrelevant, incoherent, or incomplete), and provide actional feedback without correcting or rewriting the explanation.\n",
    "\n",
    "Only analyze the explanation for errors (not the claim or Q/A). Only consider the available evidence as the ground truth. Do not invent issues.\"\"\"\n",
    "\n",
    "madr_cross_fb_system_prompt = \"\"\"Given the following claim, QA-pairs, explanation, and primary and secondary feedback, revise the primary feedback.\n",
    "\n",
    "Remove any invalid points in the primary feedback, add only valid corrections supported by the secondary feedback, and ignore invalid secondary feedback. Use the provided claim, QA-pairs, and explanation as context.\n",
    "\n",
    "Do not copy secondary feedback verbatim. Output only the corrected primary feedback.\"\"\"\n",
    "\n",
    "madr_cross_fb_user_prompt = \"\"\"Claim: {}\n",
    "\n",
    "Question-Answer Pairs:\n",
    "{}\n",
    "\n",
    "Explanation:\n",
    "{}\n",
    "\n",
    "Primary feedback:\n",
    "{}\n",
    "\n",
    "Secondary feedback:\n",
    "{}\"\"\"\n",
    "\n",
    "madr_judge_system_prompt = \"\"\"Given the following sets of feedback, compare them for major discrepancies: whether they identify different errors, give inconsistent error descriptions, or propose nonequivalent fixes.\n",
    "\n",
    "If any discrepancy exists, output FALSE; otherwise output TRUE. Output only one word.\"\"\"\n",
    "\n",
    "\n",
    "madr_revise_system_prompt = \"\"\"Given the following explanation and two sets of feedback, rewrite the explanation so it aligns with both feedback sets.\n",
    "Your revised feedback must always conclude with either \"True\", \"False\", or \"Inconclusive\", depending on the truthfulness of the claim with regard to the rewritten explanation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b93230d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_prompt(key, system_prompt_str):\n",
    "    client._prompts[key] = (\n",
    "        client._prompts[key][0],\n",
    "        system_prompt_str\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb3d8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_prompt(\"init_fb1\", madr_init_fb1_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8d31088",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Claim: {}\\n\\nQuestion-Answer Pairs:\\n{}\\n\\nExplanation: {}\\n\\nFeedback:',\n",
       " 'Evaluate the following explanation against the given evidence by identifying all errors it contains, using these labels:\\n\\n- misrepresented factual details;\\n- introduced/misrepresented events not in evidence;\\n- introduced/misrepresented noun phrases altering meaning;\\n- logic/reasoning inconsistencies;\\n- irrelevant or unsupported information;\\n- inadequate justification.\\n\\nOnly analyze the explanation for errors (not the claim or Q/A). Do not invent errors. Provide actionable feedback without rewriting or correcting the explanation. Avoid restating the error types without context.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client._prompts[\"init_fb1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59cef96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_prompt(\"init_fb2\", madr_init_fb2_system_prompt)\n",
    "replace_prompt(\"judge\", madr_judge_system_prompt)\n",
    "replace_prompt(\"revise\", madr_revise_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27d60f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "client._prompts[\"cross_fb\"] = (\n",
    "    madr_cross_fb_user_prompt,\n",
    "    madr_cross_fb_system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921dac7",
   "metadata": {},
   "source": [
    "## Refining Base MADR Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45973ad8",
   "metadata": {},
   "source": [
    "I have a set of 100 claims run through the baseline pipeline. When evaluating the MADR pipeline *I will assume that the answers generated by the CoRAG answering agent are always factual.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6793a1d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim_id': 0,\n",
       "  'claim': 'In 2008, Jeff Ament released a solo record.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [[\"When was Jeff Ament's solo record released?\",\n",
       "    'September 16, 2008.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'The question-answer pair directly confirms the claim that Jeff Ament released a solo record in 2008, as it specifies the release date as September 16, 2008. Therefore, the claim is supported by the provided information. True.'},\n",
       " {'claim_id': 1,\n",
       "  'claim': 'Franklin D. Roosevelt had a family that originated in New York.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [[\"What is the origin of Franklin D. Roosevelt's family?\",\n",
       "    \"The origin of Franklin D. Roosevelt's family can be traced back to the Roosevelt family, a prominent American family with roots in Dutch heritage. The family's history in the United States dates back to the 17th century, with early ancestors arriving in the New World. The Roosevelt family became well-known in American politics and society, with Franklin D. Roosevelt being one of its most notable members.\"]],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': \"The question-answer pair directly supports the claim by stating that the Roosevelt family, including Franklin D. Roosevelt, has roots in Dutch heritage and a history in the United States dating back to the 17th century, with early ancestors arriving in the New World. This confirms that Franklin D. Roosevelt's family originated in New York, as New York was a significant settlement for early Dutch settlers. True.\"},\n",
       " {'claim_id': 2,\n",
       "  'claim': 'A phylogenetic tree is a branching diagram.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [['What is the primary purpose of a phylogenetic tree according to the statement?',\n",
       "    'The primary purpose of a phylogenetic tree is to represent a hypothesis about the evolutionary ancestry of a set of genes, species, or other taxa.'],\n",
       "   ['What type of diagram is a phylogenetic tree described as in the statement?',\n",
       "    'Answer: A branching diagram or \"tree.\"']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'The question-answer pairs confirm the claim that a phylogenetic tree is a branching diagram. The second answer explicitly states that a phylogenetic tree is described as a \"branching diagram or \\'tree\\'\". This directly supports the claim. The first question-answer pair provides context about the purpose of a phylogenetic tree, which is supplementary but does not contradict the claim. Therefore, the claim is fully confirmed. True.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corag_run[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f337828",
   "metadata": {},
   "source": [
    "### Debater 1 Freespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56cf0dc",
   "metadata": {},
   "source": [
    "This marks our first iteration of refining the codebook. To evaluate the debater, we need a reasonable number of responses. In this case, I will generate 10, for 5 random claims of each type (TRUE/FALSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a52daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_sample(data, count=5):\n",
    "    supports = [i for i in data if i[\"true_label\"] == \"SUPPORTS\"]\n",
    "    refutes = [i for i in data if i[\"true_label\"] == \"REFUTES\"]\n",
    "\n",
    "    selection_supports = random.sample(supports, count)\n",
    "    selection_refutes = random.sample(refutes, count)\n",
    "\n",
    "    return selection_supports + selection_refutes\n",
    "sample = select_random_sample(corag_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81fd0b",
   "metadata": {},
   "source": [
    "Now I'll generate and cache the debater's responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "366129a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate():\n",
    "    for c in sample:\n",
    "        claim = c[\"claim\"]\n",
    "        qa_pairs = c[\"qa_pairs\"]\n",
    "        verdict_raw = c[\"verdict_raw\"]\n",
    "        fb = client.send_prompt(\"init_fb1\", [claim, qa_pairs, verdict_raw])\n",
    "        c[\"r\"] = fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb724ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim_id': 40,\n",
       "  'claim': 'Jun Ji-hyun is in the South Korean film called Windstruck.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [['Is Jun Ji-hyun featured in the South Korean film \"Windstruck\"?',\n",
       "    'Answer: Yes.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'The question-answer pair directly confirms that Jun Ji-hyun is featured in the South Korean film \"Windstruck,\" which aligns with the claim. Therefore, the claim is supported by the provided information. True.',\n",
       "  'r': '- logic/reasoning inconsistencies: The explanation assumes that the question-answer pair directly confirms the claim without providing specific evidence or details about Jun Ji-hyun\\'s role or presence in the film \"Windstruck.\" The reasoning is based on the assertion that the answer \"Yes\" supports the claim, but this does not constitute a logical or evidential justification beyond the answer itself.'},\n",
       " {'claim_id': 7,\n",
       "  'claim': 'A subtype of anti-nuclear antibodies are anti-Ro antibodies.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [['Is the statement about anti-Ro antibodies being a subtype of anti-nuclear antibodies accurate?',\n",
       "    'Answer: Yes.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'The question-answer pair directly supports the claim by affirming that anti-Ro antibodies are indeed a subtype of anti-nuclear antibodies. Since the answer confirms the accuracy of the statement, the claim is validated. True.',\n",
       "  'r': 'The explanation does not contain any errors in terms of the labels provided. However, it is somewhat redundant and does not provide actionable feedback as requested. The explanation confirms the validity of the claim based on the question-answer pair but does not identify any specific errors in the explanation itself. Therefore, no errors of the specified types are present in the explanation.'},\n",
       " {'claim_id': 1,\n",
       "  'claim': 'Franklin D. Roosevelt had a family that originated in New York.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [[\"What is the origin of Franklin D. Roosevelt's family?\",\n",
       "    \"The origin of Franklin D. Roosevelt's family can be traced back to the Roosevelt family, a prominent American family with roots in Dutch heritage. The family's history in the United States dates back to the 17th century, with early ancestors arriving in the New World. The Roosevelt family became well-known in American politics and society, with Franklin D. Roosevelt being one of its most notable members.\"]],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': \"The question-answer pair directly supports the claim by stating that the Roosevelt family, including Franklin D. Roosevelt, has roots in Dutch heritage and a history in the United States dating back to the 17th century, with early ancestors arriving in the New World. This confirms that Franklin D. Roosevelt's family originated in New York, as New York was a significant settlement for early Dutch settlers. True.\",\n",
       "  'r': '- misrepresented factual details: The explanation assumes that the Dutch heritage and 17th-century origins necessarily mean the family originated in New York, which is not explicitly stated in the evidence. The evidence mentions Dutch heritage and early ancestors arriving in the New World, but does not specify that New York was the origin or that the family originated there.  \\n\\n- introduced/misrepresented noun phrases altering meaning: The explanation rephrases \"early ancestors arriving in the New World\" as \"the family originated in New York,\" which alters the meaning by assuming a specific location without explicit support from the evidence.  \\n\\n- logic/reasoning inconsistencies: The explanation draws a direct connection between the 17th-century arrival in the New World and the family originating in New York, which is a logical leap not supported by the evidence. The evidence does not specify the exact location of the family\\'s origin within the New World.'},\n",
       " {'claim_id': 47,\n",
       "  'claim': 'Taal was romantic.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'NOT ENOUGH INFO',\n",
       "  'qa_pairs': [[\"What specific aspects of Taal's behavior or actions indicate a romantic nature?\",\n",
       "    'question too specific'],\n",
       "   ['When did Taal display romantic behavior?', 'question too specific'],\n",
       "   ['Who is Taal, and what context is being referred to in the statement?',\n",
       "    'question too specific'],\n",
       "   ['What cultural or historical context is referenced in the statement about Taal being romantic?',\n",
       "    'question too specific'],\n",
       "   ['What is the basis for the assertion that Taal was romantic?',\n",
       "    'The basis for the assertion that *Taal* was romantic is that it is a Bollywood musical romantic drama film.']],\n",
       "  'verdict_int': 2,\n",
       "  'verdict_raw': 'The question-answer pairs provide context that the claim \"Taal was romantic\" refers to *Taal*, a Bollywood musical romantic drama film. The basis for the assertion is that it is a romantic film, which supports the claim. However, the pairs do not directly confirm or refute the claim with specific evidence about the film\\'s romantic nature beyond its genre classification. Therefore, the question-answer pairs partially support the claim but do not fully confirm or disprove it. Inconclusive.',\n",
       "  'r': '- misrepresented factual details: The explanation implies that the question-answer pairs \"do not directly confirm or refute the claim with specific evidence about the film\\'s romantic nature beyond its genre classification,\" which is accurate. However, it misrepresents the role of genre classification as insufficient evidence for the claim, which is a valid critique but not a factual misrepresentation of the evidence itself.\\n\\n- introduced/misrepresented events not in evidence: The explanation does not introduce or misrepresent events outside the evidence; it correctly focuses on the genre classification of the film as the basis for the claim.\\n\\n- introduced/misrepresented noun phrases altering meaning: The explanation does not alter the meaning of noun phrases from the evidence. It correctly refers to \"Taal\" as a Bollywood musical romantic drama film.\\n\\n- logic/reasoning inconsistencies: There is no significant logic or reasoning inconsistency in the explanation. It acknowledges the partial support from the question-answer pairs and the limitations of genre classification as evidence.\\n\\n- irrelevant or unsupported information: The explanation does not introduce irrelevant or unsupported information. It focuses on the relationship between the claim, the question-answer pairs, and the genre classification.\\n\\n- inadequate justification: The explanation provides a reasonable justification for its conclusion that the evidence is inconclusive, based on the limitations of genre classification as evidence for the romantic nature of the film. However, it could have more explicitly acknowledged the potential sufficiency of genre classification in this context.'},\n",
       " {'claim_id': 17,\n",
       "  'claim': 'Bill Cosby was a comedian who did stand-up.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [['Who is being referred to as a comedian who did stand-up in the statement?',\n",
       "    'Ian Harvie'],\n",
       "   ['When did Bill Cosby start doing stand-up comedy?',\n",
       "    'The provided sources do not specify when Bill Cosby started doing stand-up comedy. Therefore, the answer is: question too specific.'],\n",
       "   [\"What is the significance of Bill Cosby's stand-up comedy career?\",\n",
       "    'Bill Cosby\\'s stand-up comedy career is significant for pioneering a conversational style of comedy that influenced the sitcom \"The Cosby Show,\" and for producing works like the 1983 film \"Bill Cosby: Himself,\" regarded as a landmark in stand-up comedy. His routines also contributed to the evolution of comedy albums and live performance films.'],\n",
       "   [\"What is the relationship between Bill Cosby's stand-up comedy and his role as a comedian?\",\n",
       "    \"Bill Cosby's stand-up comedy is directly related to his role as a comedian, as it showcases his comedic routines and conversational style, which are central to his career as a comedian.\"]],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'The question-answer pairs confirm that Bill Cosby was a comedian who did stand-up, as they describe his stand-up comedy career, its significance, and its relationship to his role as a comedian. While one question about the start date of his career is unanswered, the overall information supports the claim. Verdict: True.',\n",
       "  'r': '- misrepresented factual details: The explanation assumes that the question-answer pairs confirm Bill Cosby was a comedian who did stand-up without directly addressing the claim. The evidence provided in the Q&A pairs does not explicitly state that Bill Cosby was a comedian who did stand-up, but rather discusses his stand-up comedy career and its significance. The explanation conflates the discussion of his career with the assertion that he was a comedian who did stand-up.\\n\\n- introduced/misrepresented events not in evidence: The explanation references \"the provided sources do not specify when Bill Cosby started doing stand-up comedy\" as an unanswered question, but this is not an event or fact that is misrepresented. However, the explanation does not clarify that the lack of specificity does not negate the truth of the claim, which is a logical inconsistency.\\n\\n- logic/reasoning inconsistencies: The explanation concludes that the claim is \"True\" based on the Q&A pairs, but the evidence does not directly confirm the claim that Bill Cosby was a comedian who did stand-up. The Q&A pairs focus on his stand-up career and its significance, not on his role as a comedian in general. The conclusion assumes that the discussion of his stand-up career equates to him being a comedian who did stand-up, which may not be explicitly stated in the evidence.\\n\\n- irrelevant or unsupported information: The explanation does not address the specific claim that Bill Cosby was a comedian who did stand-up, but instead focuses on the discussion of his stand-up career. While the Q&A pairs do discuss his stand-up comedy, they do not directly state that he was a comedian who did stand-up, which is the core of the claim. The explanation therefore relies on inferred information rather than direct confirmation from the evidence.'},\n",
       " {'claim_id': 65,\n",
       "  'claim': 'Peter Cetera was entirely absent from the album Chicago Transit Authority.',\n",
       "  'true_label': 'REFUTES',\n",
       "  'predicted_label': 'REFUTES',\n",
       "  'qa_pairs': [['Did Peter Cetera contribute to any tracks on the album Chicago Transit Authority?',\n",
       "    'Yes, Peter Cetera contributed to tracks on the album *Chicago Transit Authority* by providing lead vocals on the song \"Questions 67 and 68.\"']],\n",
       "  'verdict_int': 0,\n",
       "  'verdict_raw': 'The question-answer pair directly contradicts the claim by confirming that Peter Cetera did contribute to the album *Chicago Transit Authority*, specifically by providing lead vocals on the song \"Questions 67 and 68.\" Therefore, the claim that Peter Cetera was entirely absent from the album is false. False.',\n",
       "  'r': 'The explanation does not contain any errors in terms of the labels provided. The explanation correctly identifies the contradiction between the claim and the question-answer pair, and it accurately states that the claim is false based on the evidence. There is no misrepresented factual detail, no introduction of events not in evidence, no misrepresentation of noun phrases, no logic or reasoning inconsistencies, no irrelevant or unsupported information, and no inadequate justification. The explanation is consistent with the evidence and properly addresses the contradiction.'},\n",
       " {'claim_id': 64,\n",
       "  'claim': 'Gustave Eiffel has yet to retire from engineering.',\n",
       "  'true_label': 'REFUTES',\n",
       "  'predicted_label': 'NOT ENOUGH INFO',\n",
       "  'qa_pairs': [['When did Gustave Eiffel retire from engineering?',\n",
       "    'Gustave Eiffel retired from engineering after his contributions to the Eiffel Tower and the Statue of Liberty, but the exact date of his retirement is not specified in the provided sources.'],\n",
       "   [\"What was Gustave Eiffel's role in the construction of the Statue of Liberty?\",\n",
       "    \"Gustave Eiffel contributed to the construction of the Statue of Liberty by designing the internal structure of the statue's framework.\"],\n",
       "   [\"What was Gustave Eiffel's role in the construction of the Eiffel Tower?\",\n",
       "    'Gustave Eiffel was the designer of the Eiffel Tower.'],\n",
       "   [\"What is the current status of Gustave Eiffel's retirement from engineering?\",\n",
       "    'question too specific'],\n",
       "   ['When did Gustave Eiffel retire from engineering?',\n",
       "    'Gustave Eiffel retired from engineering after his contributions to the construction of the Eiffel Tower and the Statue of Liberty, but the exact date of his retirement is not specified in the provided sources.']],\n",
       "  'verdict_int': 2,\n",
       "  'verdict_raw': 'The question-answer pairs do not fully confirm or disprove the claim that Gustave Eiffel has yet to retire from engineering. The provided information states that Gustave Eiffel retired from engineering after his contributions to the Eiffel Tower and the Statue of Liberty, but the exact date of his retirement is unspecified. Additionally, one answer indicates that the current status of his retirement is too specific to answer, suggesting a lack of available information. Since the claim refers to his current status, and the sources do not provide a definitive answer, the question-answer pairs are insufficient in explaining the claim. Inconclusive.',\n",
       "  'r': '- misrepresented factual details: The explanation misrepresents the content of the question-answer pairs by implying that the sources state Gustave Eiffel \"retired from engineering,\" when the sources only state that he \"retired from engineering after his contributions\" without confirming that he has \"yet to retire\" as claimed. The explanation conflates the retirement date being unspecified with the claim that he has \"yet to retire.\"\\n\\n- introduced/misrepresented events not in evidence: The explanation introduces the idea that the current status of his retirement is \"too specific to answer,\" which is a direct quote from the question-answer pairs. However, it misinterprets this as suggesting a lack of available information, rather than acknowledging that the question is too specific for an answer.\\n\\n- logic/reasoning inconsistencies: The explanation concludes that the question-answer pairs are \"insufficient in explaining the claim\" because the sources do not provide a definitive answer. However, the claim is about whether he \"has yet to retire,\" and the sources do not confirm or deny this, making the conclusion logically consistent with the evidence.\\n\\n- irrelevant or unsupported information: The explanation does not address the specific claim that Gustave Eiffel \"has yet to retire from engineering\" directly. Instead, it focuses on the lack of a specific retirement date, which is not the same as the current status of his retirement.'},\n",
       " {'claim_id': 58,\n",
       "  'claim': 'Tequila is made from a plant primarily found around the woods.',\n",
       "  'true_label': 'REFUTES',\n",
       "  'predicted_label': 'REFUTES',\n",
       "  'qa_pairs': [['What plant is tequila primarily made from, and where is it typically found?',\n",
       "    'Tequila is primarily made from the blue agave plant, which is typically found in the state of Jalisco, Mexico.']],\n",
       "  'verdict_int': 0,\n",
       "  'verdict_raw': 'The question-answer pair directly addresses the claim by specifying that tequila is made from the blue agave plant, which is typically found in Jalisco, Mexico. This confirms that the plant is not primarily found around general woods but in a specific region. Therefore, the claim is false.  \\nVerdict: False',\n",
       "  'r': '- misrepresented factual details: The explanation refers to \"general woods\" in the claim but does not explicitly state that the claim misrepresents the plant\\'s typical habitat. The explanation correctly identifies Jalisco as the region where blue agave is found, but it does not directly address the misrepresentation of \"woods\" as a general term.  \\n\\n- introduced/misrepresented events not in evidence: The explanation does not introduce any events not present in the evidence. It correctly focuses on the geographical location of the blue agave plant.  \\n\\n- introduced/misrepresented noun phrases altering meaning: The explanation does not alter the meaning of noun phrases from the evidence. It correctly identifies \"blue agave\" and \"Jalisco, Mexico\" as the specific plant and region.  \\n\\n- logic/reasoning inconsistencies: There are no logic or reasoning inconsistencies in the explanation. The reasoning flows logically from the evidence provided.  \\n\\n- irrelevant or unsupported information: The explanation does not include any irrelevant or unsupported information. It directly addresses the claim and provides a factual response based on the evidence.  \\n\\n- inadequate justification: The explanation provides a sufficient justification by directly addressing the claim and citing the specific plant and region, thereby refuting the claim that the plant is primarily found around \"woods.\"'},\n",
       " {'claim_id': 97,\n",
       "  'claim': 'The People vs. Larry Flynt is an unproduced screenplay.',\n",
       "  'true_label': 'REFUTES',\n",
       "  'predicted_label': 'REFUTES',\n",
       "  'qa_pairs': [['Is \"The People vs. Larry Flynt\" considered unproduced or has it been produced in any form?',\n",
       "    'Answer: \"The People vs. Larry Flynt\" has been produced; it is a 1996 American biographical drama film.']],\n",
       "  'verdict_int': 0,\n",
       "  'verdict_raw': 'The question-answer pair directly addresses the claim by stating that \"The People vs. Larry Flynt\" has been produced as a 1996 film, thereby disproving the claim that it is an unproduced screenplay. \\n\\nFalse.',\n",
       "  'r': 'The explanation contains the following errors:\\n\\n- **introduced/misrepresented events not in evidence**: The explanation states that the film is a \"1996 American biographical drama film,\" but the evidence only provides the answer that it has been produced as a film without specifying the year or genre. The explanation adds details not present in the evidence.\\n\\n- **introduced/misrepresented noun phrases altering meaning**: The explanation refers to the work as a \"film,\" while the original claim refers to it as a \"screenplay.\" The explanation alters the meaning by misrepresenting the nature of the work.\\n\\n- **inadequate justification**: The explanation does not adequately justify why the film being a \"1996 American biographical drama film\" disproves the claim that it is an \"unproduced screenplay.\" The justification is based on an assumption rather than a direct comparison of the terms used in the claim and the answer.'},\n",
       " {'claim_id': 56,\n",
       "  'claim': 'Matt Damon is not an actor.',\n",
       "  'true_label': 'REFUTES',\n",
       "  'predicted_label': 'REFUTES',\n",
       "  'qa_pairs': [['Is Matt Damon recognized as an actor in the entertainment industry?',\n",
       "    'Answer: Yes.']],\n",
       "  'verdict_int': 0,\n",
       "  'verdict_raw': 'The question-answer pair directly addresses the claim by confirming that Matt Damon is recognized as an actor in the entertainment industry. This directly contradicts the claim that Matt Damon is not an actor. Therefore, the claim is false. \\n\\nVerdict: False',\n",
       "  'r': '- misrepresented factual details: The explanation assumes that the claim \"Matt Damon is not an actor\" is false based on the question-answer pair, but it does not provide any evidence beyond the question-answer pair to support this conclusion. The explanation relies on the answer to the question rather than independent verification of Matt Damon\\'s status as an actor.\\n\\n- logic/reasoning inconsistencies: The explanation concludes that the claim is false based on the answer to a specific question, but it does not logically establish that the answer is definitive or that the claim is necessarily false without further context or evidence.\\n\\n- inadequate justification: The explanation does not provide any external or additional evidence to support the conclusion that the claim is false, other than the answer to the question itself. This makes the reasoning insufficient to determine the truth value of the original claim.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regenerate()\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68659246",
   "metadata": {},
   "source": [
    "We now need a section of code to pretty-print each claim out, making it easy to judge the effectiveness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be0c38ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Jun Ji-hyun is in the South Korean film called Windstruck.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Is Jun Ji-hyun featured in the South Korean film \"Windstruck\"?\n",
      "\tAnswer: Yes.\n",
      "\n",
      "The original verdict: The question-answer pair directly confirms that Jun Ji-hyun is featured in the South Korean film \"Windstruck,\" which aligns with the claim. Therefore, the claim is supported by the provided information. True.\n",
      "\n",
      "This agent's feedback:\n",
      "- logic/reasoning inconsistencies: The explanation assumes that the question-answer pair directly confirms the claim without providing specific evidence or details about Jun Ji-hyun's role or presence in the film \"Windstruck.\" The reasoning is based on the assertion that the answer \"Yes\" supports the claim, but this does not constitute a logical or evidential justification beyond the answer itself.\n",
      "\n",
      "\n",
      "\n",
      "2. A subtype of anti-nuclear antibodies are anti-Ro antibodies.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Is the statement about anti-Ro antibodies being a subtype of anti-nuclear antibodies accurate?\n",
      "\tAnswer: Yes.\n",
      "\n",
      "The original verdict: The question-answer pair directly supports the claim by affirming that anti-Ro antibodies are indeed a subtype of anti-nuclear antibodies. Since the answer confirms the accuracy of the statement, the claim is validated. True.\n",
      "\n",
      "This agent's feedback:\n",
      "The explanation does not contain any errors in terms of the labels provided. However, it is somewhat redundant and does not provide actionable feedback as requested. The explanation confirms the validity of the claim based on the question-answer pair but does not identify any specific errors in the explanation itself. Therefore, no errors of the specified types are present in the explanation.\n",
      "\n",
      "\n",
      "\n",
      "3. Franklin D. Roosevelt had a family that originated in New York.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What is the origin of Franklin D. Roosevelt's family?\n",
      "\tThe origin of Franklin D. Roosevelt's family can be traced back to the Roosevelt family, a prominent American family with roots in Dutch heritage. The family's history in the United States dates back to the 17th century, with early ancestors arriving in the New World. The Roosevelt family became well-known in American politics and society, with Franklin D. Roosevelt being one of its most notable members.\n",
      "\n",
      "The original verdict: The question-answer pair directly supports the claim by stating that the Roosevelt family, including Franklin D. Roosevelt, has roots in Dutch heritage and a history in the United States dating back to the 17th century, with early ancestors arriving in the New World. This confirms that Franklin D. Roosevelt's family originated in New York, as New York was a significant settlement for early Dutch settlers. True.\n",
      "\n",
      "This agent's feedback:\n",
      "- misrepresented factual details: The explanation assumes that the Dutch heritage and 17th-century origins necessarily mean the family originated in New York, which is not explicitly stated in the evidence. The evidence mentions Dutch heritage and early ancestors arriving in the New World, but does not specify that New York was the origin or that the family originated there.  \n",
      "\n",
      "- introduced/misrepresented noun phrases altering meaning: The explanation rephrases \"early ancestors arriving in the New World\" as \"the family originated in New York,\" which alters the meaning by assuming a specific location without explicit support from the evidence.  \n",
      "\n",
      "- logic/reasoning inconsistencies: The explanation draws a direct connection between the 17th-century arrival in the New World and the family originating in New York, which is a logical leap not supported by the evidence. The evidence does not specify the exact location of the family's origin within the New World.\n",
      "\n",
      "\n",
      "\n",
      "4. Taal was romantic.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What specific aspects of Taal's behavior or actions indicate a romantic nature?\n",
      "\tquestion too specific\n",
      "2.\tQuestion: When did Taal display romantic behavior?\n",
      "\tquestion too specific\n",
      "3.\tQuestion: Who is Taal, and what context is being referred to in the statement?\n",
      "\tquestion too specific\n",
      "4.\tQuestion: What cultural or historical context is referenced in the statement about Taal being romantic?\n",
      "\tquestion too specific\n",
      "5.\tQuestion: What is the basis for the assertion that Taal was romantic?\n",
      "\tThe basis for the assertion that *Taal* was romantic is that it is a Bollywood musical romantic drama film.\n",
      "\n",
      "The original verdict: The question-answer pairs provide context that the claim \"Taal was romantic\" refers to *Taal*, a Bollywood musical romantic drama film. The basis for the assertion is that it is a romantic film, which supports the claim. However, the pairs do not directly confirm or refute the claim with specific evidence about the film's romantic nature beyond its genre classification. Therefore, the question-answer pairs partially support the claim but do not fully confirm or disprove it. Inconclusive.\n",
      "\n",
      "This agent's feedback:\n",
      "- misrepresented factual details: The explanation implies that the question-answer pairs \"do not directly confirm or refute the claim with specific evidence about the film's romantic nature beyond its genre classification,\" which is accurate. However, it misrepresents the role of genre classification as insufficient evidence for the claim, which is a valid critique but not a factual misrepresentation of the evidence itself.\n",
      "\n",
      "- introduced/misrepresented events not in evidence: The explanation does not introduce or misrepresent events outside the evidence; it correctly focuses on the genre classification of the film as the basis for the claim.\n",
      "\n",
      "- introduced/misrepresented noun phrases altering meaning: The explanation does not alter the meaning of noun phrases from the evidence. It correctly refers to \"Taal\" as a Bollywood musical romantic drama film.\n",
      "\n",
      "- logic/reasoning inconsistencies: There is no significant logic or reasoning inconsistency in the explanation. It acknowledges the partial support from the question-answer pairs and the limitations of genre classification as evidence.\n",
      "\n",
      "- irrelevant or unsupported information: The explanation does not introduce irrelevant or unsupported information. It focuses on the relationship between the claim, the question-answer pairs, and the genre classification.\n",
      "\n",
      "- inadequate justification: The explanation provides a reasonable justification for its conclusion that the evidence is inconclusive, based on the limitations of genre classification as evidence for the romantic nature of the film. However, it could have more explicitly acknowledged the potential sufficiency of genre classification in this context.\n",
      "\n",
      "\n",
      "\n",
      "5. Bill Cosby was a comedian who did stand-up.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Who is being referred to as a comedian who did stand-up in the statement?\n",
      "\tIan Harvie\n",
      "2.\tQuestion: When did Bill Cosby start doing stand-up comedy?\n",
      "\tThe provided sources do not specify when Bill Cosby started doing stand-up comedy. Therefore, the answer is: question too specific.\n",
      "3.\tQuestion: What is the significance of Bill Cosby's stand-up comedy career?\n",
      "\tBill Cosby's stand-up comedy career is significant for pioneering a conversational style of comedy that influenced the sitcom \"The Cosby Show,\" and for producing works like the 1983 film \"Bill Cosby: Himself,\" regarded as a landmark in stand-up comedy. His routines also contributed to the evolution of comedy albums and live performance films.\n",
      "4.\tQuestion: What is the relationship between Bill Cosby's stand-up comedy and his role as a comedian?\n",
      "\tBill Cosby's stand-up comedy is directly related to his role as a comedian, as it showcases his comedic routines and conversational style, which are central to his career as a comedian.\n",
      "\n",
      "The original verdict: The question-answer pairs confirm that Bill Cosby was a comedian who did stand-up, as they describe his stand-up comedy career, its significance, and its relationship to his role as a comedian. While one question about the start date of his career is unanswered, the overall information supports the claim. Verdict: True.\n",
      "\n",
      "This agent's feedback:\n",
      "- misrepresented factual details: The explanation assumes that the question-answer pairs confirm Bill Cosby was a comedian who did stand-up without directly addressing the claim. The evidence provided in the Q&A pairs does not explicitly state that Bill Cosby was a comedian who did stand-up, but rather discusses his stand-up comedy career and its significance. The explanation conflates the discussion of his career with the assertion that he was a comedian who did stand-up.\n",
      "\n",
      "- introduced/misrepresented events not in evidence: The explanation references \"the provided sources do not specify when Bill Cosby started doing stand-up comedy\" as an unanswered question, but this is not an event or fact that is misrepresented. However, the explanation does not clarify that the lack of specificity does not negate the truth of the claim, which is a logical inconsistency.\n",
      "\n",
      "- logic/reasoning inconsistencies: The explanation concludes that the claim is \"True\" based on the Q&A pairs, but the evidence does not directly confirm the claim that Bill Cosby was a comedian who did stand-up. The Q&A pairs focus on his stand-up career and its significance, not on his role as a comedian in general. The conclusion assumes that the discussion of his stand-up career equates to him being a comedian who did stand-up, which may not be explicitly stated in the evidence.\n",
      "\n",
      "- irrelevant or unsupported information: The explanation does not address the specific claim that Bill Cosby was a comedian who did stand-up, but instead focuses on the discussion of his stand-up career. While the Q&A pairs do discuss his stand-up comedy, they do not directly state that he was a comedian who did stand-up, which is the core of the claim. The explanation therefore relies on inferred information rather than direct confirmation from the evidence.\n",
      "\n",
      "\n",
      "\n",
      "6. Peter Cetera was entirely absent from the album Chicago Transit Authority.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Did Peter Cetera contribute to any tracks on the album Chicago Transit Authority?\n",
      "\tYes, Peter Cetera contributed to tracks on the album *Chicago Transit Authority* by providing lead vocals on the song \"Questions 67 and 68.\"\n",
      "\n",
      "The original verdict: The question-answer pair directly contradicts the claim by confirming that Peter Cetera did contribute to the album *Chicago Transit Authority*, specifically by providing lead vocals on the song \"Questions 67 and 68.\" Therefore, the claim that Peter Cetera was entirely absent from the album is false. False.\n",
      "\n",
      "This agent's feedback:\n",
      "The explanation does not contain any errors in terms of the labels provided. The explanation correctly identifies the contradiction between the claim and the question-answer pair, and it accurately states that the claim is false based on the evidence. There is no misrepresented factual detail, no introduction of events not in evidence, no misrepresentation of noun phrases, no logic or reasoning inconsistencies, no irrelevant or unsupported information, and no inadequate justification. The explanation is consistent with the evidence and properly addresses the contradiction.\n",
      "\n",
      "\n",
      "\n",
      "7. Gustave Eiffel has yet to retire from engineering.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: When did Gustave Eiffel retire from engineering?\n",
      "\tGustave Eiffel retired from engineering after his contributions to the Eiffel Tower and the Statue of Liberty, but the exact date of his retirement is not specified in the provided sources.\n",
      "2.\tQuestion: What was Gustave Eiffel's role in the construction of the Statue of Liberty?\n",
      "\tGustave Eiffel contributed to the construction of the Statue of Liberty by designing the internal structure of the statue's framework.\n",
      "3.\tQuestion: What was Gustave Eiffel's role in the construction of the Eiffel Tower?\n",
      "\tGustave Eiffel was the designer of the Eiffel Tower.\n",
      "4.\tQuestion: What is the current status of Gustave Eiffel's retirement from engineering?\n",
      "\tquestion too specific\n",
      "5.\tQuestion: When did Gustave Eiffel retire from engineering?\n",
      "\tGustave Eiffel retired from engineering after his contributions to the construction of the Eiffel Tower and the Statue of Liberty, but the exact date of his retirement is not specified in the provided sources.\n",
      "\n",
      "The original verdict: The question-answer pairs do not fully confirm or disprove the claim that Gustave Eiffel has yet to retire from engineering. The provided information states that Gustave Eiffel retired from engineering after his contributions to the Eiffel Tower and the Statue of Liberty, but the exact date of his retirement is unspecified. Additionally, one answer indicates that the current status of his retirement is too specific to answer, suggesting a lack of available information. Since the claim refers to his current status, and the sources do not provide a definitive answer, the question-answer pairs are insufficient in explaining the claim. Inconclusive.\n",
      "\n",
      "This agent's feedback:\n",
      "- misrepresented factual details: The explanation misrepresents the content of the question-answer pairs by implying that the sources state Gustave Eiffel \"retired from engineering,\" when the sources only state that he \"retired from engineering after his contributions\" without confirming that he has \"yet to retire\" as claimed. The explanation conflates the retirement date being unspecified with the claim that he has \"yet to retire.\"\n",
      "\n",
      "- introduced/misrepresented events not in evidence: The explanation introduces the idea that the current status of his retirement is \"too specific to answer,\" which is a direct quote from the question-answer pairs. However, it misinterprets this as suggesting a lack of available information, rather than acknowledging that the question is too specific for an answer.\n",
      "\n",
      "- logic/reasoning inconsistencies: The explanation concludes that the question-answer pairs are \"insufficient in explaining the claim\" because the sources do not provide a definitive answer. However, the claim is about whether he \"has yet to retire,\" and the sources do not confirm or deny this, making the conclusion logically consistent with the evidence.\n",
      "\n",
      "- irrelevant or unsupported information: The explanation does not address the specific claim that Gustave Eiffel \"has yet to retire from engineering\" directly. Instead, it focuses on the lack of a specific retirement date, which is not the same as the current status of his retirement.\n",
      "\n",
      "\n",
      "\n",
      "8. Tequila is made from a plant primarily found around the woods.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What plant is tequila primarily made from, and where is it typically found?\n",
      "\tTequila is primarily made from the blue agave plant, which is typically found in the state of Jalisco, Mexico.\n",
      "\n",
      "The original verdict: The question-answer pair directly addresses the claim by specifying that tequila is made from the blue agave plant, which is typically found in Jalisco, Mexico. This confirms that the plant is not primarily found around general woods but in a specific region. Therefore, the claim is false.  \n",
      "Verdict: False\n",
      "\n",
      "This agent's feedback:\n",
      "- misrepresented factual details: The explanation refers to \"general woods\" in the claim but does not explicitly state that the claim misrepresents the plant's typical habitat. The explanation correctly identifies Jalisco as the region where blue agave is found, but it does not directly address the misrepresentation of \"woods\" as a general term.  \n",
      "\n",
      "- introduced/misrepresented events not in evidence: The explanation does not introduce any events not present in the evidence. It correctly focuses on the geographical location of the blue agave plant.  \n",
      "\n",
      "- introduced/misrepresented noun phrases altering meaning: The explanation does not alter the meaning of noun phrases from the evidence. It correctly identifies \"blue agave\" and \"Jalisco, Mexico\" as the specific plant and region.  \n",
      "\n",
      "- logic/reasoning inconsistencies: There are no logic or reasoning inconsistencies in the explanation. The reasoning flows logically from the evidence provided.  \n",
      "\n",
      "- irrelevant or unsupported information: The explanation does not include any irrelevant or unsupported information. It directly addresses the claim and provides a factual response based on the evidence.  \n",
      "\n",
      "- inadequate justification: The explanation provides a sufficient justification by directly addressing the claim and citing the specific plant and region, thereby refuting the claim that the plant is primarily found around \"woods.\"\n",
      "\n",
      "\n",
      "\n",
      "9. The People vs. Larry Flynt is an unproduced screenplay.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Is \"The People vs. Larry Flynt\" considered unproduced or has it been produced in any form?\n",
      "\tAnswer: \"The People vs. Larry Flynt\" has been produced; it is a 1996 American biographical drama film.\n",
      "\n",
      "The original verdict: The question-answer pair directly addresses the claim by stating that \"The People vs. Larry Flynt\" has been produced as a 1996 film, thereby disproving the claim that it is an unproduced screenplay. \n",
      "\n",
      "False.\n",
      "\n",
      "This agent's feedback:\n",
      "The explanation contains the following errors:\n",
      "\n",
      "- **introduced/misrepresented events not in evidence**: The explanation states that the film is a \"1996 American biographical drama film,\" but the evidence only provides the answer that it has been produced as a film without specifying the year or genre. The explanation adds details not present in the evidence.\n",
      "\n",
      "- **introduced/misrepresented noun phrases altering meaning**: The explanation refers to the work as a \"film,\" while the original claim refers to it as a \"screenplay.\" The explanation alters the meaning by misrepresenting the nature of the work.\n",
      "\n",
      "- **inadequate justification**: The explanation does not adequately justify why the film being a \"1996 American biographical drama film\" disproves the claim that it is an \"unproduced screenplay.\" The justification is based on an assumption rather than a direct comparison of the terms used in the claim and the answer.\n",
      "\n",
      "\n",
      "\n",
      "10. Matt Damon is not an actor.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Is Matt Damon recognized as an actor in the entertainment industry?\n",
      "\tAnswer: Yes.\n",
      "\n",
      "The original verdict: The question-answer pair directly addresses the claim by confirming that Matt Damon is recognized as an actor in the entertainment industry. This directly contradicts the claim that Matt Damon is not an actor. Therefore, the claim is false. \n",
      "\n",
      "Verdict: False\n",
      "\n",
      "This agent's feedback:\n",
      "- misrepresented factual details: The explanation assumes that the claim \"Matt Damon is not an actor\" is false based on the question-answer pair, but it does not provide any evidence beyond the question-answer pair to support this conclusion. The explanation relies on the answer to the question rather than independent verification of Matt Damon's status as an actor.\n",
      "\n",
      "- logic/reasoning inconsistencies: The explanation concludes that the claim is false based on the answer to a specific question, but it does not logically establish that the answer is definitive or that the claim is necessarily false without further context or evidence.\n",
      "\n",
      "- inadequate justification: The explanation does not provide any external or additional evidence to support the conclusion that the claim is false, other than the answer to the question itself. This makes the reasoning insufficient to determine the truth value of the original claim.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pprint_results(sample):\n",
    "    for i, s in enumerate(sample):\n",
    "        claim = s[\"claim\"]\n",
    "        qa_pairs = s[\"qa_pairs\"]\n",
    "        explanation = s[\"verdict_raw\"]\n",
    "        madr_agent = s[\"r\"].strip()\n",
    "        print(f\"{i+1}. {claim}\\n\")\n",
    "        print(f\"Available evidence for this claim, in the form of Q/A pairs:\")\n",
    "        for j, e in enumerate(qa_pairs):\n",
    "            print(f\"{j+1}.\\tQuestion: {e[0]}\\n\\t{e[1]}\")\n",
    "        print()\n",
    "        print(f\"The original verdict: {explanation}\\n\")\n",
    "        print(f\"This agent's feedback:\\n{madr_agent}\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "pprint_results(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a717ebe",
   "metadata": {},
   "source": [
    "## Debater 2 Freespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfaac8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate():\n",
    "    for c in sample:\n",
    "        claim = c[\"claim\"]\n",
    "        qa_pairs = c[\"qa_pairs\"]\n",
    "        verdict_raw = c[\"verdict_raw\"]\n",
    "        fb = client.send_prompt(\"init_fb2\", [claim, qa_pairs, verdict_raw])\n",
    "        c[\"r\"] = fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e734a821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Birth of the Dragon's principal photography began in Vancouver, Canada.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: When did the principal photography of *Birth of the Dragon* begin?\n",
      "\tThe principal photography of *Birth of the Dragon* began on November 17, 2015.\n",
      "\n",
      "The original verdict: The question-answer pair confirms that the principal photography of *Birth of the Dragon* began on November 17, 2015, but it does not specify the location. Therefore, the claim that it began in Vancouver, Canada, is not fully confirmed by the provided information. Inconclusive.\n",
      "\n",
      "This agent's feedback:\n",
      "The explanation contains the following problematic sentence: \"The question-answer pair confirms that the principal photography of *Birth of the Dragon* began on November 17, 2015, but it does not specify the location.\"\n",
      "\n",
      "This sentence is unfaithful because it inaccurately states that the question-answer pair \"does not specify the location,\" which is factually incorrect based on the provided evidence. The question-answer pair only provides the start date of principal photography and does not mention the location at all. However, the explanation incorrectly implies that the location is not specified, which is not a factual error but an omission. The explanation's failure to recognize that the location is not mentioned in the evidence is a logical inaccuracy, as it leads to the incorrect conclusion that the claim about Vancouver is \"not fully confirmed,\" when in fact the evidence does not address the location at all. \n",
      "\n",
      "Actional feedback: The explanation incorrectly assumes that the absence of location information in the evidence means the claim about Vancouver is unconfirmed, when in reality, the evidence is silent on the location. The explanation should acknowledge that the evidence does not provide information about the location, rather than asserting that the claim is \"not fully confirmed.\"\n",
      "\n",
      "\n",
      "\n",
      "2. Taal was romantic.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What specific aspects of Taal's behavior or actions indicate a romantic nature?\n",
      "\tquestion too specific\n",
      "2.\tQuestion: When did Taal display romantic behavior?\n",
      "\tquestion too specific\n",
      "3.\tQuestion: Who is Taal, and what context is being referred to in the statement?\n",
      "\tquestion too specific\n",
      "4.\tQuestion: What cultural or historical context is referenced in the statement about Taal being romantic?\n",
      "\tquestion too specific\n",
      "5.\tQuestion: What is the basis for the assertion that Taal was romantic?\n",
      "\tThe basis for the assertion that *Taal* was romantic is that it is a Bollywood musical romantic drama film.\n",
      "\n",
      "The original verdict: The question-answer pairs provide context that the claim \"Taal was romantic\" refers to *Taal*, a Bollywood musical romantic drama film. The basis for the assertion is that it is a romantic film, which supports the claim. However, the pairs do not directly confirm or refute the claim with specific evidence about the film's romantic nature beyond its genre classification. Therefore, the question-answer pairs partially support the claim but do not fully confirm or disprove it. Inconclusive.\n",
      "\n",
      "This agent's feedback:\n",
      "The explanation contains the following weaknesses that reduce faithfulness to the claim and evidence:\n",
      "\n",
      "1. **\"The basis for the assertion is that it is a romantic film, which supports the claim.\"**  \n",
      "   - **Why it is unfaithful:** This sentence is factually inaccurate. The evidence provided in the QA-pair explicitly states that the basis for the assertion is that *Taal* is a \"Bollywood musical romantic drama film,\" not simply that it is a \"romantic film.\" The explanation misrepresents the evidence by omitting the specific genre classification and simplifying the basis of the claim.\n",
      "\n",
      "2. **\"The question-answer pairs do not directly confirm or refute the claim with specific evidence about the film's romantic nature beyond its genre classification.\"**  \n",
      "   - **Why it is unfaithful:** This sentence is logically inaccurate. The evidence explicitly states that the basis for the assertion is the film's classification as a \"romantic drama,\" which is a specific type of romantic nature. The explanation incorrectly suggests that the evidence does not provide specific evidence about the film's romantic nature, contradicting the provided QA-pair evidence.\n",
      "\n",
      "\n",
      "\n",
      "3. Jet Li was a person.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Is Jet Li a person?\n",
      "\tYes, Jet Li is a person. He is a martial arts actor mentioned in the first source.\n",
      "\n",
      "The original verdict: The question-answer pair directly confirms the claim by explicitly stating that Jet Li is a person. The answer provides a clear affirmation of the claim and supports it with additional context. Therefore, the claim is fully confirmed.\n",
      "\n",
      "True\n",
      "\n",
      "This agent's feedback:\n",
      "None. The explanation is faithful to the claim and evidence. It correctly identifies that the question-answer pair confirms the claim and provides a reasonable justification for why the claim is supported by the evidence. There are no problematic sentences in the explanation that reduce faithfulness to the claim or evidence.\n",
      "\n",
      "\n",
      "\n",
      "4. Dangal (film) is set in Haryana.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Where is the film Dangal set?\n",
      "\tquestion too specific\n",
      "2.\tQuestion: What region in Haryana is the film Dangal set in?\n",
      "\tquestion too specific\n",
      "3.\tQuestion: What is the significance of setting Dangal in Haryana?\n",
      "\tThe significance of setting Bharat Kesri Dangal in Haryana is that it commemorates the martyrdom day of Shaheed Bhagat Singh, Rajguru, and Sukhdev, and it is organized by the Haryana Sports and Youth Affairs department to promote freestyle wrestling in the state.\n",
      "4.\tQuestion: What historical event is associated with the setting of Dangal in Haryana?\n",
      "\tThe historical event associated with the setting of Dangal in Haryana is the martyrdom day of Shaheed Bhagat Singh, Rajguru, and Sukhdev.\n",
      "\n",
      "The original verdict: The question-answer pairs provide information about the historical significance and event associated with the setting of Dangal in Haryana, but they do not directly confirm or deny the claim that the film is set in Haryana. While one answer mentions the film's setting in Haryana, it is accompanied by a note that the question is \"too specific,\" suggesting ambiguity. The other answers focus on the historical context rather than the film's actual setting. Therefore, the question-answer pairs are insufficient in definitively confirming or disproving the claim. Inconclusive.\n",
      "\n",
      "This agent's feedback:\n",
      "\"The question-answer pairs provide information about the historical significance and event associated with the setting of Dangal in Haryana, but they do not directly confirm or deny the claim that the film is set in Haryana.\"  \n",
      "- This sentence is unfaithful because it inaccurately states that the question-answer pairs \"do not directly confirm or deny\" the claim, when in fact, the answer to the question \"What region in Haryana is the film Dangal set in?\" is not provided, but the answer to \"What historical event is associated with the setting of Dangal in Haryana?\" does imply that the film is set in Haryana. The explanation incorrectly interprets the evidence as being inconclusive when it is not.  \n",
      "\n",
      "\"While one answer mentions the film's setting in Haryana, it is accompanied by a note that the question is 'too specific,' suggesting ambiguity.\"  \n",
      "- This sentence is unfaithful because it misrepresents the evidence. The note that the question is \"too specific\" is not about the film's setting, but about the question itself being too specific. The explanation incorrectly assumes that the note refers to the film's setting, which it does not.  \n",
      "\n",
      "\"The other answers focus on the historical context rather than the film's actual setting.\"  \n",
      "- This sentence is unfaithful because it is factually inaccurate. The answer that mentions the historical event does not focus on the film's actual setting, but the explanation incorrectly frames this as a focus on the historical context rather than the setting. The explanation confuses the content of the answer with the claim being made.\n",
      "\n",
      "\n",
      "\n",
      "5. Julia Louis-Dreyfus has a job.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What specific job does Julia Louis-Dreyfus currently hold?\n",
      "\tquestion too specific\n",
      "2.\tQuestion: When did Julia Louis-Dreyfus start working?\n",
      "\tThe sources provided do not specify when Julia Louis-Dreyfus started working. Therefore, the answer is: question too specific.\n",
      "\n",
      "The original verdict: The question-answer pairs do not provide sufficient information to confirm or disprove the claim that Julia Louis-Dreyfus has a job. While the claim is general, the provided answers are too vague or specific to determine its truthfulness definitively. Inconclusive.\n",
      "\n",
      "This agent's feedback:\n",
      "The explanation is faithful to the claim and evidence. However, there are no problematic sentences in the explanation that reduce its faithfulness to the claim/evidence. The explanation accurately reflects that the question-answer pairs do not provide sufficient information to confirm or disprove the claim, and it correctly identifies the answers as too vague or specific to determine the truthfulness of the claim. Therefore, there are no weaknesses in the explanation that reduce faithfulness to the claim or evidence.\n",
      "\n",
      "\n",
      "\n",
      "6. Independence Day: Resurgence is a drama film.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What genre does Independence Day: Resurgence belong to?\n",
      "\tThe question is too specific.\n",
      "2.\tQuestion: What type of film is Independence Day: Resurgence?\n",
      "\tAnswer: Science fiction action film.\n",
      "\n",
      "The original verdict: The claim that \"Independence Day: Resurgence is a drama film\" is false. The question-answer pairs provide information stating that the film is a \"science fiction action film,\" which directly contradicts the claim of it being a drama film. Therefore, the claim is disproven.  \n",
      "Verdict: False\n",
      "\n",
      "This agent's feedback:\n",
      "The explanation is faithful to the claim and evidence provided. There are no weaknesses in the explanation that reduce its faithfulness to the claim or the evidence. The explanation correctly identifies that the claim is false by referencing the question-answer pairs, which specify the film as a \"science fiction action film.\" The reasoning is logically accurate and consistent with the provided evidence.\n",
      "\n",
      "\n",
      "\n",
      "7. John Wick: Chapter 2 was theatrically released in AMC Theatres.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Did AMC Theatres exclusively release John Wick: Chapter 2?\n",
      "\tquestion too specific\n",
      "2.\tQuestion: When was John Wick: Chapter 2 theatrically released?\n",
      "\tFebruary 10, 2017.\n",
      "3.\tQuestion: Where was John Wick: Chapter 2 theatrically released besides AMC Theatres?\n",
      "\tAnswer: Los Angeles and New York City.\n",
      "4.\tQuestion: Who distributed John Wick: Chapter 2?\n",
      "\tThe distribution information for John Wick: Chapter 2 is not provided in the given sources.\n",
      "5.\tQuestion: What is the exact release scope of John Wick: Chapter 2 in terms of theaters?\n",
      "\tThe exact release scope of John Wick: Chapter 2 in terms of theaters is that it was theatrically released in the United States on February 10, 2017.\n",
      "\n",
      "The original verdict: The question-answer pairs provide information that John Wick: Chapter 2 was theatrically released in the United States on February 10, 2017, but do not explicitly confirm that it was released exclusively or primarily at AMC Theatres. While it mentions that the film was released in Los Angeles and New York City besides AMC Theatres, this does not definitively confirm or disprove the claim that AMC Theatres was the primary or exclusive distributor. Therefore, the claim cannot be fully confirmed or disproven based on the provided information. Inconclusive.\n",
      "\n",
      "This agent's feedback:\n",
      "The explanation contains the following problematic sentence: \"the film was released in Los Angeles and New York City besides AMC Theatres, this does not definitively confirm or disprove the claim that AMC Theatres was the primary or exclusive distributor.\"\n",
      "\n",
      "This sentence is unfaithful because it incorrectly attributes the release locations (Los Angeles and New York City) to the film being \"released besides AMC Theatres,\" which is factually inaccurate. The evidence states that these cities were \"where it was theatrically released besides AMC Theatres,\" implying that AMC Theatres was a different venue, not a distributor. The explanation misinterprets the evidence by suggesting that AMC Theatres is a distributor, when in fact, the evidence refers to AMC Theatres as a theater chain, not a distributor. This leads to a logical inaccuracy in the explanation. \n",
      "\n",
      "Actional feedback: The explanation incorrectly interprets AMC Theatres as a distributor rather than a theater chain, leading to a logical inaccuracy. The sentence should be revised to accurately reflect that AMC Theatres is a theater chain, not a distributor, and that the evidence does not provide information about distribution.\n",
      "\n",
      "\n",
      "\n",
      "8. Appian Way Productions is a film direction agency.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: What is the primary function or focus of Appian Way Productions?\n",
      "\tThe primary function or focus of Appian Way Productions is film production.\n",
      "\n",
      "The original verdict: The question-answer pair directly addresses the claim by stating that the primary function of Appian Way Productions is film production, which aligns with the claim that it is a film direction agency. However, \"film production\" and \"film direction agency\" are not entirely synonymous, as film production encompasses a broader range of activities including financing, casting, and post-production, while a film direction agency specifically focuses on directing. Therefore, the answer does not fully confirm the claim. Inconclusive.\n",
      "\n",
      "This agent's feedback:\n",
      "The explanation contains the following problematic sentence: \"film production encompasses a broader range of activities including financing, casting, and post-production, while a film direction agency specifically focuses on directing.\"\n",
      "\n",
      "This sentence is unfaithful because it introduces information not present in the evidence or the claim. The evidence only states that the primary function of Appian Way Productions is film production, and the claim is that it is a film direction agency. The explanation invents additional details about what film production and film direction agencies entail, which are not supported by the provided evidence. This introduces external knowledge that is not grounded in the given information, thereby reducing faithfulness to the claim and evidence.\n",
      "\n",
      "\n",
      "\n",
      "9. Ronaldo Maczinski's birthday is May 11.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: When was Ronaldo Maczinski actually born?\n",
      "\tAnswer: 11 September 1980.\n",
      "\n",
      "The original verdict: The question-answer pair directly contradicts the claim that Ronaldo Maczinski's birthday is May 11, as the answer states he was born on 11 September 1980. Therefore, the claim is false.  \n",
      "Verdict: False\n",
      "\n",
      "This agent's feedback:\n",
      "The explanation is faithful to the claim and evidence. There are no weaknesses in the explanation that reduce its faithfulness to the claim/evidence. The explanation correctly identifies the contradiction between the claim and the answer in the Q/A pair, and appropriately concludes that the claim is false. The reasoning is logically accurate and directly supports the verdict.\n",
      "\n",
      "\n",
      "\n",
      "10. Big Boi has yet to create music.\n",
      "\n",
      "Available evidence for this claim, in the form of Q/A pairs:\n",
      "1.\tQuestion: Is the statement accurate that Big Boi has not released any music?\n",
      "\tAnswer: No, the statement is not accurate. Big Boi (also known as Benjamin Bozeman) is a rapper and producer who has released numerous music projects, including albums and mixtapes.\n",
      "\n",
      "The original verdict: The question-answer pair directly addresses the claim by stating that Big Boi has released numerous music projects, thereby disproving the claim that he has yet to create music. The answer clearly contradicts the assertion in the claim. \n",
      "\n",
      "False.\n",
      "\n",
      "This agent's feedback:\n",
      "The explanation is unfaithful to the claim and evidence. The sentence \"The question-answer pair directly addresses the claim by stating that Big Boi has released numerous music projects, thereby disproving the claim that he has yet to create music.\" is factually inaccurate. The evidence provided in the Q/A pair does not state that Big Boi has \"released numerous music projects,\" but rather that he has \"released numerous music projects, including albums and mixtapes.\" The explanation incorrectly generalizes this specific information without accurately reflecting the content of the evidence. \n",
      "\n",
      "Additionally, the sentence \"thereby disproving the claim that he has yet to create music\" is logically inaccurate because the evidence only addresses whether he has released music, not whether he has \"created\" music. The explanation conflates \"released\" with \"created,\" which is a logical misstep. \n",
      "\n",
      "Actional feedback: The explanation should more accurately reflect the specific content of the evidence and avoid conflating \"released\" with \"created.\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = select_random_sample(corag_run)\n",
    "regenerate()\n",
    "pprint_results(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce506752",
   "metadata": {},
   "source": [
    "## Cross-Reference Freespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eebc07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate the debaters\n",
    "# for c in corag_run:\n",
    "#     claim = c[\"claim\"]\n",
    "#     qa_pairs = c[\"qa_pairs\"]\n",
    "#     verdict_raw = c[\"verdict_raw\"]\n",
    "#     fb1 = client.send_prompt(\"madr_init_fb1\", [claim, qa_pairs, verdict_raw])\n",
    "#     fb2 = client.send_prompt(\"madr_init_fb2\", [claim, qa_pairs, verdict_raw])\n",
    "#     c[\"fb1\"] = fb1\n",
    "#     c[\"fb2\"] = fb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cff7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "incomp_relative_path = os.path.join('assets', '20251201T170345--metrics_incomp__100.json')\n",
    "with open(relative_path, 'w') as f:\n",
    "    json.dump(corag_run, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32788368",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(relative_path, 'r', encoding='utf-8') as f:\n",
    "    incomp_run = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7215c636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim_id': 0,\n",
       "  'claim': 'In 2008, Jeff Ament released a solo record.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [[\"When was Jeff Ament's solo record released?\",\n",
       "    'September 16, 2008.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'The question-answer pair directly confirms the claim that Jeff Ament released a solo record in 2008, as it specifies the release date as September 16, 2008. Therefore, the claim is supported by the provided information. True.'},\n",
       " {'claim_id': 1,\n",
       "  'claim': 'Franklin D. Roosevelt had a family that originated in New York.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [[\"What is the origin of Franklin D. Roosevelt's family?\",\n",
       "    \"The origin of Franklin D. Roosevelt's family can be traced back to the Roosevelt family, a prominent American family with roots in Dutch heritage. The family's history in the United States dates back to the 17th century, with early ancestors arriving in the New World. The Roosevelt family became well-known in American politics and society, with Franklin D. Roosevelt being one of its most notable members.\"]],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': \"The question-answer pair directly supports the claim by stating that the Roosevelt family, including Franklin D. Roosevelt, has roots in Dutch heritage and a history in the United States dating back to the 17th century, with early ancestors arriving in the New World. This confirms that Franklin D. Roosevelt's family originated in New York, as New York was a significant settlement for early Dutch settlers. True.\",\n",
       "  'r': '- misrepresented factual details: The explanation assumes that the Dutch heritage and 17th-century origins necessarily mean the family originated in New York, which is not explicitly stated in the evidence. The evidence mentions Dutch heritage and early ancestors arriving in the New World, but does not specify that New York was the origin or that the family originated there.  \\n\\n- introduced/misrepresented noun phrases altering meaning: The explanation rephrases \"early ancestors arriving in the New World\" as \"the family originated in New York,\" which alters the meaning by assuming a specific location without explicit support from the evidence.  \\n\\n- logic/reasoning inconsistencies: The explanation draws a direct connection between the 17th-century arrival in the New World and the family originating in New York, which is a logical leap not supported by the evidence. The evidence does not specify the exact location of the family\\'s origin within the New World.'},\n",
       " {'claim_id': 2,\n",
       "  'claim': 'A phylogenetic tree is a branching diagram.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [['What is the primary purpose of a phylogenetic tree according to the statement?',\n",
       "    'The primary purpose of a phylogenetic tree is to represent a hypothesis about the evolutionary ancestry of a set of genes, species, or other taxa.'],\n",
       "   ['What type of diagram is a phylogenetic tree described as in the statement?',\n",
       "    'Answer: A branching diagram or \"tree.\"']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'The question-answer pairs confirm the claim that a phylogenetic tree is a branching diagram. The second answer explicitly states that a phylogenetic tree is described as a \"branching diagram or \\'tree\\'\". This directly supports the claim. The first question-answer pair provides context about the purpose of a phylogenetic tree, which is supplementary but does not contradict the claim. Therefore, the claim is fully confirmed. True.'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incomp_run[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a704260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Claim: {}\\n\\nQuestion-Answer Pairs:\\n{}\\n\\nExplanation:\\n{}\\n\\nPrimary feedback:\\n{}\\n\\nSecondary feedback:\\n{}',\n",
       " 'Given the following claim, QA-pairs, explanation, and primary and secondary feedback, revise the primary feedback.\\n\\nRemove any invalid points in the primary feedback, add only valid corrections supported by the secondary feedback, and ignore invalid secondary feedback. Use the provided claim, QA-pairs, and explanation as context.\\n\\nDo not copy secondary feedback verbatim. Output only the corrected primary feedback.')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that the user prompt include explanation now\n",
    "client._prompts[\"cross_fb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff5082fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_cross():\n",
    "    for i, c in enumerate(sample):\n",
    "        fb1 = c[\"fb1\"]\n",
    "        fb2 = c[\"fb2\"]\n",
    "        claim = c[\"claim\"]\n",
    "        qa_pairs = c[\"qa_pairs\"]\n",
    "        verdict_raw = c[\"verdict_raw\"]\n",
    "        if i >= 5:\n",
    "            # try opposite send order\n",
    "            cross = client.send_prompt(\"cross_fb\", [claim, qa_pairs, verdict_raw, fb2, fb1])\n",
    "        else:\n",
    "            cross = client.send_prompt(\"cross_fb\", [claim, qa_pairs, verdict_raw, fb1, fb2])\n",
    "        c[\"cross\"] = cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87765239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_results_cross(sample):\n",
    "    for i, s in enumerate(sample):\n",
    "        claim = s[\"claim\"]\n",
    "        qa_pairs = s[\"qa_pairs\"]\n",
    "        explanation = s[\"verdict_raw\"]\n",
    "        fb1 = s[\"fb1\"]\n",
    "        fb2 = s[\"fb2\"]\n",
    "        cross_agent = s[\"cross\"].strip()\n",
    "        print(f\"{i+1}. {claim}\")\n",
    "        print(f\"Available evidence for this claim, in the form of Q/A pairs:\")\n",
    "        for j, e in enumerate(qa_pairs):\n",
    "            print(f\"{j+1}.\\tQuestion: {e[0]}\\n\\t{e[1]}\")\n",
    "        print(f\"Explanation: {explanation}\\n\")\n",
    "        print(f\"Feedback for this explanation:\")\n",
    "        print(f\"\\ta. {fb1}\")\n",
    "        print(f\"\\tb. {fb2}\")\n",
    "        print()\n",
    "        print(f\"This agent's rewrite:\\n{cross_agent}\")\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e2953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = select_random_sample(incomp_run)\n",
    "regenerate_cross()\n",
    "pprint_results_cross(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ba69e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "/gnu/store/zmxsy7mhaq0yq9kh5w4yc6sq2g8kzb2x-python-3.11.14/bin/python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
