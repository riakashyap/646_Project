{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e072dbc",
   "metadata": {},
   "source": [
    "# MADR prompt evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bf871",
   "metadata": {},
   "source": [
    "This notebook is inspired by [this research](https://cacm.acm.org/research/from-prompt-engineering-to-prompt-science-with-humans-in-the-loop/) on prompt engineering with humans in the loop.\n",
    "\n",
    "We are faced with the issue: the existing prompts in the MADR paper are ill-defined and intended for a different complexity model. While fine-tuning is an enticing option, we lack a dataset of questions, evidence, veracity claim, and target feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3384cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822d5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_jupyter_notebook():\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        return shell == 'ZMQInteractiveShell'\n",
    "    except NameError:\n",
    "        return False\n",
    "IN_JUPYTER = in_jupyter_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581f57b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_clients import LlamaCppClient\n",
    "from src import config\n",
    "from src.madr import run_madr\n",
    "from src.parsers import parse_ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41daea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dir = config.PROMPTS_DIR / \"custom\"\n",
    "client = LlamaCppClient(prompts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45eb05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corag_run = None\n",
    "relative_path = os.path.join('assets', '20251123T192328--metrics__100.json')\n",
    "with open(relative_path, 'r', encoding='utf-8') as f:\n",
    "    corag_run = json.load(f)[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce502763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd3ffb4",
   "metadata": {},
   "source": [
    "## An initial codebook for Debater 1, 2, Cross-Agent, Judge, Refiner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b7f701",
   "metadata": {},
   "source": [
    "A 'codebook' (or criteria) in the above research encapsulates the desired outcomes for generation. It systematically allows any human-in-the-loop to quickly and methodically conclude when a generated response does or does not meet the specifications.\n",
    "\n",
    "The starting criteria, validated by multiple researchers for each model, is defined below:\n",
    "\n",
    "1. **Debater 1, given an explanation and evidence:**\n",
    "- Does distinguish between the following types of errors in the explanation:\n",
    "  1) misrepresentations of factual details in the evidence\n",
    "  2) introduces or misrepresents events not present in the evidence\n",
    "  3) introduces or misrepresents noun phrases in the evidence, changing semantic meaning\n",
    "  4) logic/reasoning inconsistencies in the explanation\n",
    "  5) includes information not relevant to the claim/evidence or beyond what the evidence supports\n",
    "- Does NOT attempt to correct errors in the explanation to align with evidence.\n",
    "- Identifies ALL such errors\n",
    "- Does NOT output non-existent errors.\n",
    "- Answers tersely\n",
    "\n",
    "*note, it is NOT a requirement the model names the error types.*\n",
    "\n",
    "2. **Debater 2, given an explanation and evidence:**\n",
    "- Identifies general weaknesses which reduce the faithfulness of an explanation to the claim/evidence.\n",
    "- Generates explanations for why identified issues are unfaithful:\n",
    "  1) Factually inaccurate\n",
    "  2) logically inaccurate\n",
    "  3) irrelevant\n",
    "  4) incomplete\n",
    "  5) incoherent\n",
    "- Does NOT attempt to correct errors in the explanation to align with evidence.\n",
    "- Does provide actionable feedback\n",
    "- Identifies ALL such errors\n",
    "- Does NOT output non-existent errors.\n",
    "- Answers tersely\n",
    "\n",
    "3. **Cross-Agent, given the original explanation, and a primary and secondary set of feedback:**\n",
    "- uses the original explanation to:\n",
    "  1) recognize invalid feedback in the primary feedback, and incorporate corrections from the secondary feedback.\n",
    "  2) does NOT include invalid feedback from the secondary feedback.\n",
    "- outputs ONLY the modified primary feedback\n",
    "- makes MINIMAL changes, rather than copying the secondary feedback\n",
    "\n",
    "*note, the input to this agent is up to debate. I am making the assumption that only including the explanation rather than the full claim and Q/A pairs balances the agent's ability to refine the feedback given a source of ground truth while keeping the prompt context small.*\n",
    "\n",
    "4. **Judge, given two sets of feedback:**\n",
    "- identifies major discrepancies between the input feedback:\n",
    "  1) identifies different errors or issues\n",
    "  2) provides inconsistent error descriptions\n",
    "  3) whether any suggested fixes are not equivalent\n",
    "- if discrepancies exist, output contains the word 'FALSE'\n",
    "- else, output contains the word 'TRUE'\n",
    "\n",
    "5. **Refinement Agent, given two sets of feedback and an explanation:**\n",
    "- rewrites the explanation to align with both sets of feedback\n",
    "- Does NOT output anything other than the refined explanation\n",
    "- Does NOT remove the component containing the verdict in the original explanation, but can 'flip' it, i.e.: true -> false."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d13f24",
   "metadata": {},
   "source": [
    "## Evaluating Base MADR Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bf970e",
   "metadata": {},
   "source": [
    "This marks our first iteration of refining the codebook. I have a set of 100 claims run through the baseline pipeline. When evaluating the MADR pipeline *I will assume that the answers generated by the CoRAG answering agent are always factual.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3e00c9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim_id': 0,\n",
       "  'claim': 'In 2008, Jeff Ament released a solo record.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [[\"When was Jeff Ament's solo record released?\",\n",
       "    'September 16, 2008.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'True.  \\nInconclusive'},\n",
       " {'claim_id': 1,\n",
       "  'claim': 'Franklin D. Roosevelt had a family that originated in New York.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [[\"What is the origin of Franklin D. Roosevelt's family?\",\n",
       "    \"Franklin D. Roosevelt's family originated from Dutch settlers in New York, with his ancestors tracing back to the 17th century. His direct lineage includes prominent figures such as his father, James Roosevelt, and his mother, Eleanor Roosevelt, who were part of influential American families.\"]],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'True. The question-answer pairs fully confirm the claim that Franklin D. Roosevelt had a family that originated in New York.'},\n",
       " {'claim_id': 2,\n",
       "  'claim': 'A phylogenetic tree is a branching diagram.',\n",
       "  'true_label': 'SUPPORTS',\n",
       "  'predicted_label': 'SUPPORTS',\n",
       "  'qa_pairs': [['What is the primary purpose of a phylogenetic tree according to the statement?',\n",
       "    'The primary purpose of a phylogenetic tree is to represent a hypothesis about the evolutionary ancestry of a set of genes, species, or other taxa.'],\n",
       "   ['What type of diagram is a phylogenetic tree described as in the statement?',\n",
       "    'A phylogenetic tree is described as a branching diagram or \"tree\" in the statement.']],\n",
       "  'verdict_int': 1,\n",
       "  'verdict_raw': 'The question-answer pairs fully confirm the claim.  \\nTrue'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corag_run[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca12e2a",
   "metadata": {},
   "source": [
    "### Debater 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22275805",
   "metadata": {},
   "source": [
    "To evaluate the debater, we need a reasonable number of responses. In this case, I will generate 20, for 10 random claims of each type (TRUE/FALSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60aa3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_sample(data, count=10):\n",
    "    supports = [i for i in data if i[\"true_label\"] == \"SUPPORTS\"]\n",
    "    refutes = [i for i in data if i[\"true_label\"] == \"REFUTES\"]\n",
    "\n",
    "    selection_supports = random.sample(supports, count)\n",
    "    selection_refutes = random.sample(refutes, count)\n",
    "\n",
    "    return selection_supports + selection_refutes\n",
    "sample = select_random_sample(corag_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a88d7",
   "metadata": {},
   "source": [
    "Now I'll generate and cache the debater's responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e2b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in sample:\n",
    "    claim = c[\"claim\"]\n",
    "    qa_pairs = c[\"qa_pairs\"]\n",
    "    verdict_raw = c[\"verdict_raw\"]\n",
    "    fb = client.send_prompt(\"madr_init_fb1\", [claim, qa_pairs, verdict_raw])\n",
    "    c[\"fb1\"] = fb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "/gnu/store/zmxsy7mhaq0yq9kh5w4yc6sq2g8kzb2x-python-3.11.14/bin/python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
