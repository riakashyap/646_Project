{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph-Based Reranking for Fact Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring graph reranking in 4 stages:\n",
    "1. **Basic Graph Reranker**: Simple semantic similarity graph\n",
    "2. **AMR + AMR-BERT**: Abstract Meaning Representation with specialized embeddings\n",
    "3. **Full GRAG**: AMR + Document reranker with RoBERTa/BERT\n",
    "4. **GRAG + Custom Weights**: Plugin architecture for weight functions\n",
    "\n",
    "Each implementation is encapsulated for easy integration into the CoRAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (3.8.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.0.14)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (8.3.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.12.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.2)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: wrapt in c:\\users\\shreya\\appdata\\roaming\\python\\python312\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: D:\\Jigoku\\Python\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shreya\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from typing import List, Dict, Tuple, Optional, Callable\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import amrlib\n",
    "import penman\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import INDEX_DIR, QRELS_PATH, CLAIMS_PATH\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: D:\\Celestia\\Projects\\646_Project\n",
      "Index directory: D:\\Celestia\\Projects\\646_Project\\data\\wiki\\index\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "INDEX_DIR = PROJECT_ROOT / \"data\" / \"wiki\" / \"index\"\n",
    "QRELS_PATH = PROJECT_ROOT / \"data\" / \"fever-qrel.json\"\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Index directory: {INDEX_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preparing qrels and fever dataset using BM25 to test reranker on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qrels already exist at D:\\Celestia\\Projects\\646_Project\\data\\fever-qrel.json\n",
      "\n",
      "Qrels loaded: 13332 claims\n",
      "  Sample IDs: ['137334', '111897', '89891', '181634', '219028']\n"
     ]
    }
   ],
   "source": [
    "# Qrel generation\n",
    "if not QRELS_PATH.exists():\n",
    "    print(\"GENERATING QRELS FROM LABELLED_DEV SPLIT\")\n",
    "    ds = load_dataset(\"fever\", \"v1.0\", split=\"labelled_dev\", trust_remote_code=True)\n",
    "    print(f\"Loaded {len(ds)} claims from labelled_dev split\")\n",
    "    \n",
    "    qrels = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    claims = []\n",
    "    added_claims = set()\n",
    "    \n",
    "    # Process each example\n",
    "    for ex in ds:\n",
    "        cid = str(ex[\"id\"])\n",
    "        label = ex[\"label\"]\n",
    "        \n",
    "        # Only use SUPPORTS/REFUTES (skip NEI)\n",
    "        if label not in (\"SUPPORTS\", \"REFUTES\"):\n",
    "            continue\n",
    "        \n",
    "        # Get evidence\n",
    "        page = ex.get(\"evidence_wiki_url\")\n",
    "        sent_id = ex.get(\"evidence_sentence_id\")\n",
    "        claim_text = ex.get(\"claim\")\n",
    "        \n",
    "        # Add to qrels if we have evidence\n",
    "        if page and sent_id is not None and claim_text:\n",
    "            qrels[cid][page] = 1\n",
    "            \n",
    "            if cid not in added_claims:\n",
    "                claims.append({\"id\": cid, \"input\": claim_text})\n",
    "                added_claims.add(cid)\n",
    "    \n",
    "    print(f\"Processed {len(ds)} examples\")\n",
    "    print(f\"Generated qrels for {len(qrels)} claims\")\n",
    "    print(f\"Claims with evidence: {len(claims)}\")\n",
    "    \n",
    "    # Save files\n",
    "    QRELS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(QRELS_PATH, \"w\", encoding=\"utf8\") as f:\n",
    "        json.dump(qrels, f, indent=2)\n",
    "    \n",
    "    with open(CLAIMS_PATH, \"w\", encoding=\"utf8\") as f:\n",
    "        json.dump(claims, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n     Saved qrels to {QRELS_PATH}\")\n",
    "    print(f\"Saved claims to {CLAIMS_PATH}\")\n",
    "else:\n",
    "    print(f\"Qrels already exist at {QRELS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Represents a retrieved document.\"\"\"\n",
    "    doc_id: str\n",
    "    text: str\n",
    "    score: float\n",
    "    metadata: Optional[Dict] = None\n",
    "\n",
    "@dataclass\n",
    "class RerankedResult:\n",
    "    \"\"\"Result after reranking.\"\"\"\n",
    "    doc_id: str\n",
    "    original_rank: int\n",
    "    new_rank: int\n",
    "    original_score: float\n",
    "    reranked_score: float\n",
    "    explanation: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'fever' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "\n",
      "2025-11-14 05:45:21,116 - datasets.load - ERROR - `trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'fever' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing FEVER data loader...\n",
      "Loading FEVER dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since fever couldn't be found on the Hugging Face Hub\n",
      "\n",
      "2025-11-14 05:45:21,389 - datasets.load - WARNING - Using the latest cached version of the dataset since fever couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'v1.0' at C:\\Users\\Shreya\\.cache\\huggingface\\datasets\\fever\\v1.0\\1.0.0\\7f8936e0558704771b08c7ce9cc202071b29a0050603374507ba61d23c00a58e (last modified on Mon Oct 20 18:04:46 2025).\n",
      "\n",
      "2025-11-14 05:45:21,392 - datasets.packaged_modules.cache.cache - WARNING - Found the latest cached dataset configuration 'v1.0' at C:\\Users\\Shreya\\.cache\\huggingface\\datasets\\fever\\v1.0\\1.0.0\\7f8936e0558704771b08c7ce9cc202071b29a0050603374507ba61d23c00a58e (last modified on Mon Oct 20 18:04:46 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded qrels with 13332 claims\n",
      " Loaded 28625 FEVER claims\n"
     ]
    }
   ],
   "source": [
    "class FEVERDataLoader:\n",
    "    \"\"\"Load real FEVER data and BM25 results for testing.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Initializing FEVER data loader...\")\n",
    "        \n",
    "        if not INDEX_DIR.exists():\n",
    "            raise FileNotFoundError(f\"Index not found at {INDEX_DIR}\")\n",
    "        \n",
    "        self.searcher = LuceneSearcher(str(INDEX_DIR))\n",
    "        self.searcher.set_bm25(1.2, 0.75)\n",
    "        \n",
    "        print(\"Loading FEVER dataset...\")\n",
    "        self.dataset = load_dataset(\"fever\", \"v1.0\", trust_remote_code=True)\n",
    "\n",
    "        full_dev_split = self.dataset[\"labelled_dev\"]\n",
    "        filtered_indices = [\n",
    "            i for i, ex in enumerate(full_dev_split) \n",
    "            if ex['label'] in ('SUPPORTS', 'REFUTES')\n",
    "        ]\n",
    "        self.dev_split = full_dev_split.select(filtered_indices)\n",
    "        \n",
    "        self.qrels = None\n",
    "        if QRELS_PATH.exists():\n",
    "            with open(QRELS_PATH, 'r') as f:\n",
    "                self.qrels = json.load(f)\n",
    "            print(f\"Loaded qrels with {len(self.qrels)} claims\")\n",
    "        else:\n",
    "            print(\"No qrels found - retrieval metrics will be limited\")\n",
    "            print(\" You can still evaluate ranking changes, but not P@3/R@3/MAP\")\n",
    "        \n",
    "        print(f\" Loaded {len(self.dev_split)} FEVER claims\")\n",
    "    \n",
    "    def get_claim(self, index: int) -> dict:\n",
    "        \"\"\"Get a single claim from FEVER.\"\"\"\n",
    "        claim_data = self.dev_split[index]\n",
    "        return {\n",
    "            'id': claim_data['id'],\n",
    "            'claim': claim_data['claim'],\n",
    "            'label': claim_data['label'],\n",
    "        }\n",
    "    \n",
    "    def get_bm25_results(self, claim: str, k: int = 10) -> List[Document]:\n",
    "        \"\"\"Get BM25 results for a claim.\"\"\"\n",
    "        hits = self.searcher.search(claim, k=k)\n",
    "        \n",
    "        documents = []\n",
    "        for hit in hits:\n",
    "            doc = self.searcher.doc(hit.docid)\n",
    "            \n",
    "            #  FIXED: Robust text extraction\n",
    "            text = \"\"\n",
    "            try:\n",
    "                # Try raw() + JSON parse (most common)\n",
    "                import json\n",
    "                raw_doc = doc.raw()\n",
    "                doc_dict = json.loads(raw_doc)\n",
    "                text = doc_dict.get('contents', '')\n",
    "            except:\n",
    "                # Fallback: try contents() method\n",
    "                try:\n",
    "                    text = doc.contents()\n",
    "                except:\n",
    "                    # Last resort: stringify\n",
    "                    text = str(doc)\n",
    "            \n",
    "            if not text:\n",
    "                print(f\" Warning: Empty text for doc {hit.docid}\")\n",
    "            \n",
    "            documents.append(Document(\n",
    "                doc_id=hit.docid,\n",
    "                text=text,\n",
    "                score=hit.score,\n",
    "                metadata={'rank': len(documents) + 1}\n",
    "            ))\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def get_test_batch(self, start: int = 0, n: int = 10) -> List[dict]:\n",
    "        \"\"\"Get a batch of test claims with BM25 results.\"\"\"\n",
    "        batch = []\n",
    "        for i in range(start, min(start + n, len(self.dev_split))):\n",
    "            claim_data = self.get_claim(i)\n",
    "            bm25_docs = self.get_bm25_results(claim_data['claim'], k=10)\n",
    "            \n",
    "            batch.append({\n",
    "                'claim_data': claim_data,\n",
    "                'bm25_docs': bm25_docs,\n",
    "            })\n",
    "        \n",
    "        return batch\n",
    "\n",
    "\n",
    "fever_loader = FEVERDataLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Basic Graph Reranker\n",
    "\n",
    "**Approach**: \n",
    "- Build a similarity graph using sentence embeddings (BERT Model)\n",
    "- Nodes = documents, edges = semantic similarity\n",
    "- Rerank using a combination of BM25 score + graph centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphReranker:\n",
    "    \"\"\"\n",
    "    Basic graph reranker using semantic similarity.\n",
    "\n",
    "    1. Compute document embeddings\n",
    "    2. Build a graph with edges weighted by cosine similarity\n",
    "    3. Compute PageRank centrality\n",
    "    4. Combine BM25 + centrality for final score\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = 'all-MiniLM-L6-v2',\n",
    "                 similarity_threshold: float = 0.3,\n",
    "                 alpha: float = 0.5): \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name: SentenceTransformer model for embeddings\n",
    "            similarity_threshold: Minimum similarity to create an edge\n",
    "            alpha: Balance between BM25 (alpha) and centrality (1-alpha)\n",
    "        \"\"\"\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def _build_similarity_graph(self, \n",
    "                                documents: List[Document],\n",
    "                                claim: str) -> nx.Graph:\n",
    "        \"\"\"\n",
    "        Prepares NetworkX graph with 'weight' on edges and 'bm25_score' on nodes\n",
    "        \"\"\"\n",
    "        # Encode all texts\n",
    "        texts = [claim] + [doc.text for doc in documents]\n",
    "        embeddings = self.encoder.encode(texts)\n",
    "        \n",
    "        claim_emb = embeddings[0]\n",
    "        doc_embs = embeddings[1:]\n",
    "        \n",
    "        # Compute pairwise similarities\n",
    "        sim_matrix = cosine_similarity(doc_embs)\n",
    "        claim_sims = cosine_similarity([claim_emb], doc_embs)[0]\n",
    "        \n",
    "        # Build graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes with attributes\n",
    "        for i, doc in enumerate(documents):\n",
    "            G.add_node(doc.doc_id, \n",
    "                      bm25_score=doc.score,\n",
    "                      claim_similarity=claim_sims[i],\n",
    "                      text=doc.text)\n",
    "        \n",
    "        # Add edges based on similarity\n",
    "        for i in range(len(documents)):\n",
    "            for j in range(i+1, len(documents)):\n",
    "                similarity = sim_matrix[i][j]\n",
    "                if similarity >= self.similarity_threshold:\n",
    "                    G.add_edge(documents[i].doc_id, \n",
    "                             documents[j].doc_id, \n",
    "                             weight=similarity)\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def rerank(self, \n",
    "               claim: str, \n",
    "               documents: List[Document],\n",
    "               top_k: int = 3) -> List[RerankedResult]:\n",
    "        \"\"\"\n",
    "        Rerank documents using graph centrality.\n",
    "        Args:\n",
    "            claim: The claim to verify\n",
    "            documents: BM25 retrieved documents\n",
    "            top_k: Number of documents to return\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        # Build graph\n",
    "        G = self._build_similarity_graph(documents, claim)\n",
    "        \n",
    "        # Compute PageRank centrality or use degree if the graph is disconnected\n",
    "        try:\n",
    "            centrality = nx.pagerank(G, weight='weight')\n",
    "        except:\n",
    "            centrality = nx.degree_centrality(G)\n",
    "        \n",
    "        # Normalize BM25 scores\n",
    "        max_bm25 = max(doc.score for doc in documents)\n",
    "        min_bm25 = min(doc.score for doc in documents)\n",
    "        bm25_range = max_bm25 - min_bm25 if max_bm25 != min_bm25 else 1\n",
    "        \n",
    "        # Combine scores\n",
    "        results = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            norm_bm25 = (doc.score - min_bm25) / bm25_range\n",
    "            cent_score = centrality.get(doc.doc_id, 0)\n",
    "            \n",
    "            final_score = self.alpha * norm_bm25 + (1 - self.alpha) * cent_score\n",
    "            \n",
    "            results.append(RerankedResult(\n",
    "                doc_id=doc.doc_id,\n",
    "                original_rank=i+1,\n",
    "                new_rank=0,  # Will be set after sorting\n",
    "                original_score=doc.score,\n",
    "                reranked_score=final_score,\n",
    "                explanation=f\"BM25: {norm_bm25:.3f}, Centrality: {cent_score:.3f}\"\n",
    "            ))\n",
    "        \n",
    "        # Sort and assign new ranks\n",
    "        results.sort(key=lambda x: x.reranked_score, reverse=True)\n",
    "        for i, result in enumerate(results):\n",
    "            result.new_rank = i + 1\n",
    "        \n",
    "        return results[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def visualize_graph(self, claim: str, documents: List[Document]):\n",
    "#     G = self._build_similarity_graph(documents, claim)\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     pos = nx.spring_layout(G)\n",
    "#     # Draw nodes\n",
    "#     node_sizes = [G.nodes[node]['bm25_score'] * 100 for node in G.nodes()]\n",
    "#     nx.draw_networkx_nodes(G, pos, node_size=node_sizes, \n",
    "#                           node_color='lightblue', alpha=0.7)\n",
    "#     # Draw edges\n",
    "#     edges = G.edges()\n",
    "#     weights = [G[u][v]['weight'] for u, v in edges]\n",
    "#     nx.draw_networkx_edges(G, pos, width=[w*3 for w in weights], \n",
    "#                           alpha=0.5)\n",
    "#     # Labels\n",
    "#     nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "#     plt.title(f\"Document Similarity Graph\\nClaim: {claim}\")\n",
    "#     plt.axis('off')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 50 claims...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "basic_reranker_s1 = GraphReranker(alpha=0.6)\n",
    "\n",
    "num_test_claims = 50\n",
    "test_batch = fever_loader.get_test_batch(start=0, n=num_test_claims)\n",
    "\n",
    "rankings_changed = 0\n",
    "total_tested = 0\n",
    "precision_at_3 = []\n",
    "recall_at_3 = []\n",
    "average_precisions = []\n",
    "\n",
    "print(f\"Testing on {num_test_claims} claims...\\n\")\n",
    "\n",
    "for idx, item in enumerate(test_batch):\n",
    "    claim = item['claim_data']['claim']\n",
    "    claim_id = str(item['claim_data']['id'])\n",
    "    bm25_docs = item['bm25_docs']\n",
    "    \n",
    "    if len(bm25_docs) < 3:\n",
    "        continue\n",
    "    \n",
    "    reranked = basic_reranker_s1.rerank(claim, bm25_docs, top_k=10)\n",
    "    \n",
    "    original_top3 = [doc.doc_id for doc in bm25_docs[:3]]\n",
    "    reranked_top3 = [r.doc_id for r in reranked[:3]]\n",
    "    \n",
    "    changed = original_top3 != reranked_top3\n",
    "    if changed:\n",
    "        rankings_changed += 1\n",
    "    \n",
    "    # Evaluate precision, recall, MAP\n",
    "    if fever_loader.qrels and claim_id in fever_loader.qrels:\n",
    "        relevant_docs = set(fever_loader.qrels[claim_id].keys())\n",
    "        \n",
    "        reranked_ids = [r.doc_id for r in reranked]\n",
    "        \n",
    "        top3_relevant = len(set(reranked_ids[:3]) & relevant_docs)\n",
    "        p_at_3 = top3_relevant / 3\n",
    "        precision_at_3.append(p_at_3)\n",
    "        \n",
    "        r_at_3 = top3_relevant / len(relevant_docs) if relevant_docs else 0\n",
    "        recall_at_3.append(r_at_3)\n",
    "        \n",
    "        num_relevant_seen = 0\n",
    "        precisions_at_relevant = []\n",
    "        for i, doc_id in enumerate(reranked_ids, 1):\n",
    "            if doc_id in relevant_docs:\n",
    "                num_relevant_seen += 1\n",
    "                precisions_at_relevant.append(num_relevant_seen / i)\n",
    "        \n",
    "        ap = np.mean(precisions_at_relevant) if precisions_at_relevant else 0\n",
    "        average_precisions.append(ap)\n",
    "    \n",
    "    total_tested += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Graph Reranker Summary\n",
      "Claims tested: 50\n",
      "Rankings changed: 9 (18.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic Graph Reranker Summary\")\n",
    "print(f\"Claims tested: {total_tested}\")\n",
    "print(f\"Rankings changed: {rankings_changed} ({rankings_changed/total_tested*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Metrics:\n",
      "Precision@3: 0.113 ± 0.158\n",
      "Recall@3: 0.300 ± 0.436\n",
      "MAP: 0.220 ± 0.334\n"
     ]
    }
   ],
   "source": [
    "if precision_at_3:\n",
    "    print(f\"Retrieval Metrics:\")\n",
    "    print(f\"Precision@3: {np.mean(precision_at_3):.3f} ± {np.std(precision_at_3):.3f}\")\n",
    "    print(f\"Recall@3: {np.mean(recall_at_3):.3f} ± {np.std(recall_at_3):.3f}\")\n",
    "    print(f\"MAP: {np.mean(average_precisions):.3f} ± {np.std(average_precisions):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: AMR Graph + AMR-BERT Reranker\n",
    "**Approach**:\n",
    "- Parse documents into Abstract Meaning Representation (AMR) graphs\n",
    "- Use AMR-BERT for specialized semantic embeddings\n",
    "- Build graph over AMR structures instead of raw text\n",
    "\n",
    "**Key Papers**:\n",
    "- AMR parsing: https://github.com/bjascob/amrlib\n",
    "- AMR-BERT: Specialized transformer for AMR graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dowloaded AMR model from: https://github.com/bjascob/amrlib-models/releases/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import amrlib\n",
    "import penman\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import re\n",
    "from collections import Counter\n",
    "        \n",
    "class AMRGraphReranker:\n",
    "    \"\"\"\n",
    "    Reranker using AMR graphs + AMR-BERT embeddings.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Parse text to AMR graph (amrlib)\n",
    "    2. Encode AMR to embeddings (AMR-BERT / RoBERTa)\n",
    "    3. Build similarity graph\n",
    "    4. Rerank using AMR structure + embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model_name: str = 'all-MiniLM-L6-v2',\n",
    "                 similarity_threshold: float = 0.2,\n",
    "                 alpha: float = 0.5,\n",
    "                 use_real_amr: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name: SentenceTransformer for text embeddings\n",
    "            similarity_threshold: Min similarity for edges\n",
    "            alpha: Balance BM25 vs AMR features\n",
    "            use_real_amr: If True, use amrlib; if False, use regex fallback\n",
    "        \"\"\"\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.alpha = alpha\n",
    "        self.use_real_amr = use_real_amr\n",
    "        self.amr_parser = None\n",
    "        \n",
    "        # Load AMR-BERT encoder (Using RoBERTa as alternative)\n",
    "        print(\"Loading AMR-BERT encoder...\")\n",
    "        self.amr_bert_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "        self.amr_bert_model = AutoModel.from_pretrained('roberta-base')\n",
    "        self.amr_bert_model.eval()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.amr_bert_model.to(self.device)\n",
    "        print(f\"AMR-BERT encoder loaded (RoBERTa-base on {self.device})\")\n",
    "        \n",
    "        if use_real_amr:\n",
    "            try:\n",
    "                print(\"Loading AMR parser from local model...\")\n",
    "                amr_model_path = Path.cwd() / \"model\" / \"amr\" / \"model_parse_xfm_bart_large-v0_1_0\"\n",
    "                if not amr_model_path.exists():\n",
    "                    raise FileNotFoundError(f\"AMR model not found at {amr_model_path}\")\n",
    "                \n",
    "                print(f\"Model path: {amr_model_path}\")\n",
    "                self.amr_parser = amrlib.load_stog_model(model_dir=str(amr_model_path))\n",
    "                \n",
    "                print(\"AMR parser loaded successfully!\")\n",
    "                print(\"  Model: STOG (String-to-Graph) - BART Large\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load AMR parser: {e}\")\n",
    "                print(f\"   Error type: {type(e).__name__}\")\n",
    "                print(\"Falling back to simplified feature extraction\")\n",
    "                self.use_real_amr = False\n",
    "    \n",
    "    def _encode_amr_with_bert(self, amr_string: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode AMR graph using AMR-BERT (RoBERTa).\n",
    "        \n",
    "        Takes the linearized AMR string and encodes it with RoBERTa,\n",
    "        treating it as structured text.\n",
    "        \n",
    "        Args: amr_string: Penman notation AMR string\n",
    "            \n",
    "        Returns: 768-dimensional embedding vector\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Tokenize AMR string\n",
    "            inputs = self.amr_bert_tokenizer(\n",
    "                amr_string,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                padding=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.amr_bert_model(**inputs)\n",
    "                # Use [CLS] token embedding (first token)\n",
    "                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]\n",
    "            \n",
    "            return embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"AMR-BERT encoding error: {e}\")\n",
    "            return np.zeros(768) # Fallback: return zero vector\n",
    "    \n",
    "    def _parse_to_amr_real(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Parse text to AMR graph using amrlib.\n",
    "        \n",
    "        Returns: Dict with AMR concepts, relations, edges, graph, and AMR-BERT embedding\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Limit text length for speed \n",
    "            text = text[:500]\n",
    "            \n",
    "            # Parse to AMR\n",
    "            amr_strings = self.amr_parser.parse_sents([text])\n",
    "            \n",
    "            if not amr_strings or len(amr_strings) == 0:\n",
    "                return self._empty_amr(text)\n",
    "            \n",
    "            # Get first AMR string\n",
    "            amr_string = amr_strings[0]\n",
    "            \n",
    "            # Parse with penman\n",
    "            graph = penman.decode(amr_string)\n",
    "            \n",
    "            # Extract concepts\n",
    "            concepts = []\n",
    "            for instance in graph.instances():\n",
    "                concepts.append(instance.target)\n",
    "            \n",
    "            # Extract relations and edges\n",
    "            relations = []\n",
    "            edges = []\n",
    "            for edge in graph.edges():\n",
    "                source, role, target = edge.source, edge.role, edge.target\n",
    "                relations.append(role)\n",
    "                edges.append((source, role, target))\n",
    "            \n",
    "            # Extract attributes\n",
    "            attributes = {}\n",
    "            for attr in graph.attributes():\n",
    "                source, role, value = attr.source, attr.role, attr.target\n",
    "                if role not in attributes:\n",
    "                    attributes[role] = []\n",
    "                attributes[role].append(value)\n",
    "            \n",
    "            # Encode AMR with AMR-BERT\n",
    "            amr_bert_embedding = self._encode_amr_with_bert(amr_string)\n",
    "            \n",
    "            return {\n",
    "                'concepts': concepts,\n",
    "                'relations': relations,\n",
    "                'edges': edges,\n",
    "                'attributes': attributes,\n",
    "                'graph': graph,\n",
    "                'amr_string': amr_string,\n",
    "                'amr_bert_embedding': amr_bert_embedding,  # ← NEW\n",
    "                'text': text\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"AMR parse error for text: {text[:50]}...\")\n",
    "            print(f\"   Error: {e}\")\n",
    "            return self._empty_amr(text)\n",
    "    \n",
    "    def _parse_to_amr_simple(self, text: str) -> Dict:\n",
    "        \"\"\"Fallback: regex-based feature extraction. Only if use_real_amr == FALSE\"\"\"\n",
    "       \n",
    "        entities = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', text)\n",
    "        verbs = re.findall(\n",
    "            r'\\b(?:is|was|are|were|be|been|being|has|have|had|'\n",
    "            r'do|does|did|can|could|will|would|shall|should|may|might|must|'\n",
    "            r'\\w+ed|\\w+ing)\\b',\n",
    "            text.lower()\n",
    "        )\n",
    "        \n",
    "        stop_words = {'the', 'a', 'an', 'is', 'was', 'are', 'were', 'in', 'on', \n",
    "                      'at', 'to', 'for', 'of', 'with', 'by', 'from', 'as'}\n",
    "        words = re.findall(r'\\b[a-z]+\\b', text.lower())\n",
    "        concepts = [w for w in words if w not in stop_words and len(w) > 3]\n",
    "        \n",
    "        # For simple mode, encode text directly with SentenceTransformer\n",
    "        amr_bert_embedding = self.encoder.encode(text)\n",
    "        \n",
    "        return {\n",
    "            'entities': entities,\n",
    "            'concepts': concepts,\n",
    "            'relations': verbs,\n",
    "            'concept_freq': Counter(concepts),\n",
    "            'entity_freq': Counter(entities),\n",
    "            'relation_freq': Counter(verbs),\n",
    "            'amr_bert_embedding': amr_bert_embedding,  \n",
    "            'text': text\n",
    "        }\n",
    "    \n",
    "    def _empty_amr(self, text: str) -> Dict:\n",
    "        \"\"\"Return empty AMR structure.\"\"\"\n",
    "        return {\n",
    "            'concepts': [],\n",
    "            'relations': [],\n",
    "            'edges': [],\n",
    "            'attributes': {},\n",
    "            'graph': None,\n",
    "            'amr_bert_embedding': np.zeros(768 if self.use_real_amr else 384),\n",
    "            'text': text\n",
    "        }\n",
    "    \n",
    "    def _parse_to_amr(self, text: str) -> Dict:\n",
    "        \"\"\"Route to real or simple AMR parsing.\"\"\"\n",
    "        if self.use_real_amr and self.amr_parser is not None:\n",
    "            return self._parse_to_amr_real(text)\n",
    "        else:\n",
    "            return self._parse_to_amr_simple(text)\n",
    "    \n",
    "    def _compute_amr_similarity(self, amr1: Dict, amr2: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Compute AMR graph similarity using BOTH:\n",
    "        1. Structural similarity (concepts, relations, edges)\n",
    "        2. AMR-BERT embedding similarity\n",
    "        \"\"\"\n",
    "        # 1. Structural similarity\n",
    "        concepts1 = set(amr1.get('concepts', []))\n",
    "        concepts2 = set(amr2.get('concepts', []))\n",
    "        \n",
    "        concept_sim = 0.0\n",
    "        if concepts1 or concepts2:\n",
    "            intersection = len(concepts1 & concepts2)\n",
    "            union = len(concepts1 | concepts2)\n",
    "            concept_sim = intersection / union if union > 0 else 0.0\n",
    "        \n",
    "        # Relation overlap\n",
    "        relations1 = set(amr1.get('relations', []))\n",
    "        relations2 = set(amr2.get('relations', []))\n",
    "        \n",
    "        relation_sim = 0.0\n",
    "        if relations1 or relations2:\n",
    "            intersection = len(relations1 & relations2)\n",
    "            union = len(relations1 | relations2)\n",
    "            relation_sim = intersection / union if union > 0 else 0.0\n",
    "        \n",
    "        # Edge overlap\n",
    "        edge_sim = 0.0\n",
    "        if self.use_real_amr and 'edges' in amr1 and 'edges' in amr2:\n",
    "            edges1 = set(amr1['edges'])\n",
    "            edges2 = set(amr2['edges'])\n",
    "            if edges1 or edges2:\n",
    "                intersection = len(edges1 & edges2)\n",
    "                union = len(edges1 | edges2)\n",
    "                edge_sim = intersection / union if union > 0 else 0.0\n",
    "        \n",
    "        # Structural similarity score\n",
    "        if self.use_real_amr:\n",
    "            structural_sim = 0.4 * concept_sim + 0.3 * relation_sim + 0.3 * edge_sim\n",
    "        else:\n",
    "            structural_sim = 0.6 * concept_sim + 0.4 * relation_sim\n",
    "        \n",
    "        # 2. AMR-BERT embedding similarity\n",
    "        emb1 = amr1.get('amr_bert_embedding')\n",
    "        emb2 = amr2.get('amr_bert_embedding')\n",
    "        \n",
    "        amr_bert_sim = 0.0\n",
    "        if emb1 is not None and emb2 is not None:\n",
    "            amr_bert_sim = cosine_similarity([emb1], [emb2])[0][0]\n",
    "        \n",
    "        # Combine structural + embedding similarity\n",
    "        # 60% AMR-BERT (dense, semantic) + 40% structural (explicit)\n",
    "        combined_similarity = 0.6 * amr_bert_sim + 0.4 * structural_sim\n",
    "        \n",
    "        return combined_similarity\n",
    "    \n",
    "    def _build_amr_graph(self, documents: List[Document], claim: str) -> nx.Graph:\n",
    "        \"\"\"Build graph using AMR features + AMR-BERT embeddings.\"\"\"\n",
    "        \n",
    "        # Parse claim\n",
    "        print(f\"  Parsing claim to AMR...\")\n",
    "        claim_amr = self._parse_to_amr(claim)\n",
    "        \n",
    "        # Parse documents\n",
    "        print(f\"  Parsing {len(documents)} documents...\")\n",
    "        doc_amrs = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            if self.use_real_amr and (i % 3 == 0 or i == len(documents)-1):\n",
    "                print(f\"  Progress: {i+1}/{len(documents)}\")\n",
    "            doc_amrs.append(self._parse_to_amr(doc.text))\n",
    "        \n",
    "        # Also compute text embeddings (for comparison)\n",
    "        claim_text_emb = self.encoder.encode(claim)\n",
    "        doc_text_embs = self.encoder.encode([doc.text[:500] for doc in documents])\n",
    "        \n",
    "        # Build graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes\n",
    "        for i, doc in enumerate(documents):\n",
    "            # AMR similarity (structural + AMR-BERT)\n",
    "            amr_sim = self._compute_amr_similarity(claim_amr, doc_amrs[i])\n",
    "            \n",
    "            # Text embedding similarity (baseline)\n",
    "            text_sim = cosine_similarity([claim_text_emb], [doc_text_embs[i]])[0][0]\n",
    "            \n",
    "            G.add_node(doc.doc_id,\n",
    "                      bm25_score=doc.score,\n",
    "                      amr_similarity=amr_sim,  # Combined structural + AMR-BERT\n",
    "                      text_similarity=text_sim,\n",
    "                      amr_features=doc_amrs[i])\n",
    "        \n",
    "        # Add edges\n",
    "        for i in range(len(documents)):\n",
    "            for j in range(i+1, len(documents)):\n",
    "                # AMR similarity (includes AMR-BERT)\n",
    "                amr_sim = self._compute_amr_similarity(doc_amrs[i], doc_amrs[j])\n",
    "                \n",
    "                # Text similarity\n",
    "                text_sim = cosine_similarity([doc_text_embs[i]], [doc_text_embs[j]])[0][0]\n",
    "                \n",
    "                # Edge weight: prioritize AMR similarity\n",
    "                edge_weight = 0.7 * amr_sim + 0.3 * text_sim\n",
    "                \n",
    "                if edge_weight >= self.similarity_threshold:\n",
    "                    G.add_edge(documents[i].doc_id, documents[j].doc_id, \n",
    "                             weight=edge_weight)\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def rerank(self, claim: str, documents: List[Document], \n",
    "               top_k: int = 3) -> List[RerankedResult]:\n",
    "        \"\"\"Rerank using AMR graph + AMR-BERT analysis.\"\"\"\n",
    "        \n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        # Build graph\n",
    "        G = self._build_amr_graph(documents, claim)\n",
    "        \n",
    "        # Compute centrality\n",
    "        try:\n",
    "            centrality = nx.pagerank(G, weight='weight')\n",
    "        except:\n",
    "            centrality = nx.degree_centrality(G)\n",
    "        \n",
    "        # Normalize BM25\n",
    "        max_bm25 = max(doc.score for doc in documents)\n",
    "        min_bm25 = min(doc.score for doc in documents)\n",
    "        bm25_range = max_bm25 - min_bm25 if max_bm25 != min_bm25 else 1\n",
    "        \n",
    "        # Score documents\n",
    "        results = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            norm_bm25 = (doc.score - min_bm25) / bm25_range\n",
    "            amr_sim = G.nodes[doc.doc_id]['amr_similarity']  # Includes AMR-BERT\n",
    "            text_sim = G.nodes[doc.doc_id]['text_similarity']\n",
    "            cent = centrality.get(doc.doc_id, 0)\n",
    "            \n",
    "            # Final score: weighted combination\n",
    "            final_score = (\n",
    "                self.alpha * norm_bm25 +\n",
    "                (1 - self.alpha) * (\n",
    "                    0.5 * amr_sim +      # AMR (structural + AMR-BERT)\n",
    "                    0.2 * text_sim +     # Text embedding\n",
    "                    0.3 * cent           # Graph centrality\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            results.append(RerankedResult(\n",
    "                doc_id=doc.doc_id,\n",
    "                original_rank=i+1,\n",
    "                new_rank=0,\n",
    "                original_score=doc.score,\n",
    "                reranked_score=final_score,\n",
    "                explanation=f\"AMR: {amr_sim:.3f}, Text: {text_sim:.3f}, Cent: {cent:.3f}\"\n",
    "            ))\n",
    "        \n",
    "        results.sort(key=lambda x: x.reranked_score, reverse=True)\n",
    "        for i, result in enumerate(results):\n",
    "            result.new_rank = i + 1\n",
    "        \n",
    "        return results[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def visualize_amr(self, text: str):\n",
    "#     \"\"\"Visualize AMR parse for debugging.\"\"\"\n",
    "#     amr = self._parse_to_amr(text)\n",
    "    \n",
    "#     print(f\"\\n{'='*70}\")\n",
    "#     print(f\"Text: {text[:100]}...\")\n",
    "#     print(f\"\\nAMR Features:\")\n",
    "#     print(f\"  Concepts: {amr.get('concepts', [])[:15]}\")\n",
    "#     print(f\"  Relations: {amr.get('relations', [])[:15]}\")\n",
    "#     print(f\"  Edges (first 5): {amr.get('edges', [])[:5]}\")\n",
    "    \n",
    "#     if 'amr_bert_embedding' in amr:\n",
    "#         emb = amr['amr_bert_embedding']\n",
    "#         print(f\"\\nAMR-BERT Embedding: {emb.shape} - [{emb[:5]}...]\")\n",
    "    \n",
    "#     if 'amr_string' in amr and amr['amr_string']:\n",
    "#         print(f\"\\nFull AMR Graph:\")\n",
    "#         print(amr['amr_string'])\n",
    "#     print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AMR-BERT encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMR-BERT encoder loaded (RoBERTa-base on cpu)\n",
      "Loading AMR parser from local model...\n",
      "Model path: D:\\Celestia\\Projects\\646_Project\\src\\model\\amr\\model_parse_xfm_bart_large-v0_1_0\n",
      "AMR parser loaded successfully!\n",
      "  Model: STOG (String-to-Graph) - BART Large\n",
      "\n",
      "Testing on 25 claims...\n"
     ]
    }
   ],
   "source": [
    "amr_reranker_s2 = AMRGraphReranker(use_real_amr=True, alpha=0.6)  # ← Added use_real_amr=True\n",
    "\n",
    "num_test_claims = 25\n",
    "test_batch = fever_loader.get_test_batch(start=0, n=num_test_claims)\n",
    "\n",
    "rankings_changed = 0\n",
    "total_tested = 0\n",
    "amr_concepts_found = 0\n",
    "amr_bert_embeddings_found = 0 \n",
    "precision_at_3 = []\n",
    "recall_at_3 = []\n",
    "average_precisions = []\n",
    "\n",
    "print(f\"\\nTesting on {num_test_claims} claims...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 06:27:57,696 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n",
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 06:36:12,601 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Claim: Fox 2000 Pictures released the film Soul Food....\n",
      "  AMR concepts: ['release-01', 'company', 'name', 'film', 'name']\n",
      "  AMR relations: [':ARG0', ':name', ':ARG1']\n",
      "  AMR-BERT embedding: (768,)-dim\n",
      "  BM25 top-3:     ['Soul_Food_-LRB-film-RRB-', 'List_of_20th_Century_Fox_films_-LRB-1935–99-RRB-', 'Maxine_Chadway']\n",
      "  Reranked top-3: ['Soul_Food_-LRB-film-RRB-', 'List_of_20th_Century_Fox_films_-LRB-1935–99-RRB-', 'Maxine_Chadway']\n",
      "  Changed: ✗ NO\n",
      "  Metrics: P@3=0.333, R@3=1.000, AP=1.000\n",
      "  Time: 883.5s\n",
      "\n",
      "[2/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 06:44:11,022 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n",
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 06:53:10,356 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Claim: Fox 2000 Pictures released the film Soul Food....\n",
      "  AMR concepts: ['release-01', 'company', 'name', 'film', 'name']\n",
      "  AMR relations: [':ARG0', ':name', ':ARG1']\n",
      "  AMR-BERT embedding: (768,)-dim\n",
      "  BM25 top-3:     ['Soul_Food_-LRB-film-RRB-', 'List_of_20th_Century_Fox_films_-LRB-1935–99-RRB-', 'Maxine_Chadway']\n",
      "  Reranked top-3: ['Soul_Food_-LRB-film-RRB-', 'List_of_20th_Century_Fox_films_-LRB-1935–99-RRB-', 'Maxine_Chadway']\n",
      "  Changed: ✗ NO\n",
      "  Metrics: P@3=0.333, R@3=1.000, AP=1.000\n",
      "  Time: 985.7s\n",
      "\n",
      "[3/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 07:00:12,682 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n",
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 07:06:52,984 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Claim: Fox 2000 Pictures released the film Soul Food....\n",
      "  AMR concepts: ['release-01', 'company', 'name', 'film', 'name']\n",
      "  AMR relations: [':ARG0', ':name', ':ARG1']\n",
      "  AMR-BERT embedding: (768,)-dim\n",
      "  BM25 top-3:     ['Soul_Food_-LRB-film-RRB-', 'List_of_20th_Century_Fox_films_-LRB-1935–99-RRB-', 'Maxine_Chadway']\n",
      "  Reranked top-3: ['Soul_Food_-LRB-film-RRB-', 'List_of_20th_Century_Fox_films_-LRB-1935–99-RRB-', 'Maxine_Chadway']\n",
      "  Changed: ✗ NO\n",
      "  Metrics: P@3=0.333, R@3=1.000, AP=1.000\n",
      "  Time: 786.7s\n",
      "\n",
      "[4/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 07:12:58,514 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n",
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 07:19:41,757 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "\n",
      "[5/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 07:25:53,714 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n",
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 07:31:45,534 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "\n",
      "[6/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 07:37:10,895 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 07:40:15,425 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Missing starting paren for node s3/stand-04\n",
      "\n",
      "2025-11-14 07:41:20,085 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but date-interval is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 07:42:55,299 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but slot is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 07:45:49,714 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 07:48:57,297 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Missing starting paren for node s3/stand-04\n",
      "\n",
      "2025-11-14 07:50:09,040 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but date-interval is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 07:51:27,389 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but slot is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[8/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 07:54:21,422 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 07:57:24,617 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Missing starting paren for node s3/stand-04\n",
      "\n",
      "2025-11-14 07:58:32,409 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but date-interval is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:00:01,335 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but slot is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[9/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:02:57,068 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:06:06,352 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Missing starting paren for node s3/stand-04\n",
      "\n",
      "2025-11-14 08:07:14,730 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but date-interval is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:08:37,064 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but slot is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[10/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:11:43,608 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:14:55,471 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Missing starting paren for node s3/stand-04\n",
      "\n",
      "2025-11-14 08:16:08,897 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but date-interval is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:17:34,573 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but slot is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[11/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:18:27,560 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:22:28,225 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but person is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:25:18,748 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but record-01 is not a new concept\n",
      "\n",
      "2025-11-14 08:26:22,496 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but ordinal-entity is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "\n",
      "[12/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:30:06,593 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but government-organization is not a new concept\n",
      "\n",
      "2025-11-14 08:30:06,594 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but organization is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n",
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:33:22,442 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but person is not a new concept\n",
      "\n",
      "2025-11-14 08:33:22,443 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but capital is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "\n",
      "[13/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:34:28,331 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but person is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:36:48,176 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:38:52,122 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but direct-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "\n",
      "[14/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:40:52,635 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but person is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:42:59,411 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:44:39,334 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but direct-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "\n",
      "[15/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:46:53,376 - penman.layout - WARNING - ignoring epigraph data for duplicate triple: ('b', ':mod', 'c')\n",
      "\n",
      "2025-11-14 08:46:53,378 - penman.layout - WARNING - ignoring epigraph data for duplicate triple: ('b', ':mod', 'c')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:49:58,099 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but bear-02 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:54:09,487 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but after is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[16/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:54:55,298 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-14 08:55:33,116 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 08:56:48,815 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:00:46,876 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-14 09:00:46,877 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but rudist is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[17/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:01:47,414 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-14 09:02:42,485 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:04:07,243 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:07:56,802 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-14 09:07:56,802 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but rudist is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[18/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:08:39,421 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-14 09:09:25,593 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:10:40,124 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:14:49,892 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-14 09:14:49,893 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but rudist is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[19/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:15:37,720 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-14 09:16:16,067 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:17:49,622 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:22:08,148 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-14 09:22:08,149 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but rudist is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[20/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:23:02,207 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-14 09:23:48,805 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:25:10,028 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:29:23,814 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-14 09:29:23,816 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but rudist is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[21/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:30:22,221 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but ordinal-entity is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:35:00,518 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but feature-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:36:55,584 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but feature-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "\n",
      "[22/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:43:45,124 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-14 09:43:45,125 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n",
      "\n",
      "[23/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 09:51:52,235 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-14 09:51:52,237 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n",
      "\n",
      "[24/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 10:00:09,548 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-14 10:00:09,550 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n",
      "\n",
      "[25/25] Processing claim...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 10:08:21,179 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-14 10:08:21,179 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "total_time = 0\n",
    "\n",
    "for idx, item in enumerate(test_batch):\n",
    "    claim = item['claim_data']['claim']\n",
    "    claim_id = str(item['claim_data']['id'])\n",
    "    bm25_docs = item['bm25_docs']\n",
    "    \n",
    "    if len(bm25_docs) < 3:\n",
    "        continue\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Rerank\n",
    "    print(f\"\\n[{idx+1}/{num_test_claims}] Processing claim...\")\n",
    "    reranked = amr_reranker_s2.rerank(claim, bm25_docs, top_k=10)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    total_time += elapsed\n",
    "    \n",
    "    # Compare rankings\n",
    "    original_top3 = [doc.doc_id for doc in bm25_docs[:3]]\n",
    "    reranked_top3 = [r.doc_id for r in reranked[:3]]\n",
    "    \n",
    "    changed = original_top3 != reranked_top3\n",
    "    if changed:\n",
    "        rankings_changed += 1\n",
    "    \n",
    "    # Check AMR features\n",
    "    claim_amr = amr_reranker_s2._parse_to_amr(claim)\n",
    "    if len(claim_amr.get('concepts', [])) > 0:\n",
    "        amr_concepts_found += 1\n",
    "    \n",
    "    # Check if AMR-BERT embedding exists\n",
    "    if 'amr_bert_embedding' in claim_amr and claim_amr['amr_bert_embedding'] is not None:\n",
    "        amr_bert_embeddings_found += 1\n",
    "    \n",
    "    # Compute retrieval metrics\n",
    "    if fever_loader.qrels and claim_id in fever_loader.qrels:\n",
    "        relevant_docs = set(fever_loader.qrels[claim_id].keys())\n",
    "        reranked_ids = [r.doc_id for r in reranked]\n",
    "        \n",
    "        # Precision@3\n",
    "        top3_relevant = len(set(reranked_ids[:3]) & relevant_docs)\n",
    "        p_at_3 = top3_relevant / 3\n",
    "        precision_at_3.append(p_at_3)\n",
    "        \n",
    "        # Recall@3\n",
    "        r_at_3 = top3_relevant / len(relevant_docs) if relevant_docs else 0\n",
    "        recall_at_3.append(r_at_3)\n",
    "        \n",
    "        # Average Precision\n",
    "        num_relevant_seen = 0\n",
    "        precisions_at_relevant = []\n",
    "        for i, doc_id in enumerate(reranked_ids, 1):\n",
    "            if doc_id in relevant_docs:\n",
    "                num_relevant_seen += 1\n",
    "                precisions_at_relevant.append(num_relevant_seen / i)\n",
    "        \n",
    "        ap = np.mean(precisions_at_relevant) if precisions_at_relevant else 0\n",
    "        average_precisions.append(ap)\n",
    "    \n",
    "    total_tested += 1\n",
    "    \n",
    "    if idx < 3:\n",
    "        print(f\"\\n{'─'*70}\")\n",
    "        print(f\"Claim: {claim[:70]}...\")\n",
    "        print(f\"  AMR concepts: {claim_amr.get('concepts', [])[:5]}\")\n",
    "        print(f\"  AMR relations: {claim_amr.get('relations', [])[:3]}\")\n",
    "        \n",
    "        if 'amr_bert_embedding' in claim_amr:\n",
    "            emb = claim_amr['amr_bert_embedding']\n",
    "            print(f\"  AMR-BERT embedding: {emb.shape if hasattr(emb, 'shape') else len(emb)}-dim\")\n",
    "        \n",
    "        print(f\"  BM25 top-3:     {original_top3}\")\n",
    "        print(f\"  Reranked top-3: {reranked_top3}\")\n",
    "        print(f\"  Changed: {'  YES' if changed else '✗ NO'}\")\n",
    "        \n",
    "        if fever_loader.qrels and claim_id in fever_loader.qrels:\n",
    "            print(f\"  Metrics: P@3={p_at_3:.3f}, R@3={r_at_3:.3f}, AP={ap:.3f}\")\n",
    "        \n",
    "        print(f\"  Time: {elapsed:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claims tested: 25\n",
      "Rankings changed: 2 (8.0%)\n",
      "AMR concepts detected: 25/25 (100.0%)\n",
      "AMR-BERT embeddings: 25/25 (100.0%)\n",
      "Total time: 13477.7s (avg: 539.1s per claim)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Claims tested: {total_tested}\")\n",
    "print(f\"Rankings changed: {rankings_changed} ({rankings_changed/total_tested*100:.1f}%)\")\n",
    "print(f\"AMR concepts detected: {amr_concepts_found}/{total_tested} ({amr_concepts_found/total_tested*100:.1f}%)\")\n",
    "print(f\"AMR-BERT embeddings: {amr_bert_embeddings_found}/{total_tested} ({amr_bert_embeddings_found/total_tested*100:.1f}%)\")  # ← NEW\n",
    "print(f\"Total time: {total_time:.1f}s (avg: {total_time/total_tested:.1f}s per claim)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval Metrics:\n",
      "  Precision@3: 0.187 ± 0.165\n",
      "  Recall@3:    0.480 ± 0.458\n",
      "  MAP:         0.356 ± 0.370\n"
     ]
    }
   ],
   "source": [
    "if precision_at_3:\n",
    "    print(f\"\\nRetrieval Metrics:\")\n",
    "    print(f\"  Precision@3: {np.mean(precision_at_3):.3f} ± {np.std(precision_at_3):.3f}\")\n",
    "    print(f\"  Recall@3:    {np.mean(recall_at_3):.3f} ± {np.std(recall_at_3):.3f}\")\n",
    "    print(f\"  MAP:         {np.mean(average_precisions):.3f} ± {np.std(average_precisions):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: Full GRAG Implementation\n",
    "\n",
    "**Components** (from GRAG paper):\n",
    "1. **AMR Graph Construction**: Parse documents to AMR\n",
    "2. **Document Encoder**: RoBERTa for document embeddings\n",
    "3. **Fact Encoder**: BERT for fact triplet embeddings\n",
    "4. **Cross-Document Reasoning**: Build fact-level graph across documents\n",
    "5. **Multi-stage Reranking**: AMR → Document → Fact\n",
    "\n",
    "**Key Innovation**: Extract factual triplets (subject-relation-object) and reason over them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses AMR Component from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "@dataclass\n",
    "class FactTriplet:\n",
    "    \"\"\"Represents a fact as (subject, relation, object) triplet.\"\"\"\n",
    "    subject: str\n",
    "    relation: str\n",
    "    obj: str\n",
    "    doc_id: str\n",
    "    confidence: float = 1.0\n",
    "    source_text: str = \"\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"({self.subject}, {self.relation}, {self.obj})\"\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.subject.lower(), self.relation.lower(), self.obj.lower()))\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return (self.subject.lower() == other.subject.lower() and\n",
    "                self.relation.lower() == other.relation.lower() and\n",
    "                self.obj.lower() == other.obj.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 217.9 kB/s eta 0:00:59\n",
      "     --------------------------------------- 0.1/12.8 MB 327.7 kB/s eta 0:00:39\n",
      "      --------------------------------------- 0.2/12.8 MB 1.0 MB/s eta 0:00:13\n",
      "     --- ------------------------------------ 1.0/12.8 MB 4.4 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 2.9/12.8 MB 10.4 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.4/12.8 MB 19.6 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.3/12.8 MB 40.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.1/12.8 MB 72.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 59.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 50.4 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: D:\\Jigoku\\Python\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRAGReranker:\n",
    "    \"\"\"\n",
    "    Full GRAG implementation with multi-stage graph reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 doc_encoder: str = 'roberta-base',\n",
    "                 fact_encoder: str = 'bert-base-uncased',\n",
    "                 use_real_amr: bool = True,\n",
    "                 device: str = 'cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            doc_encoder: Model for document embeddings (RoBERTa)\n",
    "            fact_encoder: Model for fact embeddings (BERT)\n",
    "            use_real_amr: Use real AMR parser or simplified version\n",
    "            device: 'cpu' or 'cuda'\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.use_real_amr = use_real_amr\n",
    "        \n",
    "        # Load document encoder (RoBERTa)\n",
    "        print(f\"Loading document encoder: {doc_encoder}...\")\n",
    "        self.doc_tokenizer = AutoTokenizer.from_pretrained(doc_encoder)\n",
    "        self.doc_model = AutoModel.from_pretrained(doc_encoder).to(device)\n",
    "        self.doc_model.eval()\n",
    "        \n",
    "        # Load fact encoder (BERT)\n",
    "        print(f\"Loading fact encoder: {fact_encoder}...\")\n",
    "        self.fact_tokenizer = AutoTokenizer.from_pretrained(fact_encoder)\n",
    "        self.fact_model = AutoModel.from_pretrained(fact_encoder).to(device)\n",
    "        self.fact_model.eval()\n",
    "        \n",
    "        # AMR component (reuse from Stage 2)\n",
    "        print(\"Loading AMR parser...\")\n",
    "        self.amr_reranker = AMRGraphReranker(use_real_amr=use_real_amr)\n",
    "        \n",
    "        # Load spaCy for fact extraction\n",
    "        try:\n",
    "            print(\"Loading spaCy for fact extraction...\")\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except:\n",
    "            print(\"  spaCy model not found. Installing...\")\n",
    "            import os\n",
    "            os.system(\"\")\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        print(\"GRAG Reranker initialized\")\n",
    "    \n",
    "    def _encode_text(self, text: str, encoder: str = 'doc') -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode text using specified encoder.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            encoder: 'doc' (RoBERTa) or 'fact' (BERT)\n",
    "            \n",
    "        Returns:\n",
    "            Embedding vector\n",
    "        \"\"\"\n",
    "        if encoder == 'doc':\n",
    "            tokenizer = self.doc_tokenizer\n",
    "            model = self.doc_model\n",
    "        else:\n",
    "            tokenizer = self.fact_tokenizer\n",
    "            model = self.fact_model\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Encode\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Use [CLS] token embedding\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def _extract_facts_spacy(self, text: str, doc_id: str) -> List[FactTriplet]:\n",
    "        \"\"\"\n",
    "        Extract fact triplets using spaCy dependency parsing.\n",
    "        \n",
    "        Extracts (subject, relation/verb, object) patterns:\n",
    "        - Subject-Verb-Object patterns from dependency tree\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text[:1000])  # Limit length for speed\n",
    "        facts = []\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            # Find verb as relation\n",
    "            for token in sent:\n",
    "                if token.pos_ == \"VERB\":\n",
    "                    # Find subject\n",
    "                    subjects = [child for child in token.children \n",
    "                               if child.dep_ in (\"nsubj\", \"nsubjpass\")]\n",
    "                    \n",
    "                    # Find object\n",
    "                    objects = [child for child in token.children \n",
    "                              if child.dep_ in (\"dobj\", \"pobj\", \"attr\")]\n",
    "                    \n",
    "                    # Create facts\n",
    "                    for subj in subjects:\n",
    "                        for obj in objects:\n",
    "                            # Get full noun phrases\n",
    "                            subj_text = \" \".join([t.text for t in subj.subtree])\n",
    "                            obj_text = \" \".join([t.text for t in obj.subtree])\n",
    "                            \n",
    "                            if len(subj_text) > 2 and len(obj_text) > 2:\n",
    "                                facts.append(FactTriplet(\n",
    "                                    subject=subj_text.strip(),\n",
    "                                    relation=token.lemma_,\n",
    "                                    obj=obj_text.strip(),\n",
    "                                    doc_id=doc_id,\n",
    "                                    source_text=sent.text\n",
    "                                ))\n",
    "        \n",
    "        return facts\n",
    "    \n",
    "    def _extract_facts_from_amr(self, amr_data: Dict, doc_id: str) -> List[FactTriplet]:\n",
    "        \"\"\"\n",
    "        Extract facts from AMR parse.\n",
    "        AMR edges are already (source, relation, target) triplets\n",
    "        \"\"\"\n",
    "        facts = []\n",
    "        \n",
    "        if 'edges' in amr_data and amr_data['edges']:\n",
    "            for edge in amr_data['edges']:\n",
    "                if len(edge) == 3:\n",
    "                    source, relation, target = edge\n",
    "                    facts.append(FactTriplet(\n",
    "                        subject=str(source),\n",
    "                        relation=str(relation),\n",
    "                        obj=str(target),\n",
    "                        doc_id=doc_id,\n",
    "                        source_text=amr_data.get('text', '')\n",
    "                    ))\n",
    "        \n",
    "        return facts\n",
    "    \n",
    "    def _extract_facts(self, text: str, doc_id: str) -> List[FactTriplet]:\n",
    "        \"\"\"\n",
    "        Extract facts using both spaCy and AMR.\n",
    "        \n",
    "        Combines:\n",
    "        1. spaCy dependency parsing (surface-level facts)\n",
    "        2. AMR graph edges (deep semantic facts)\n",
    "        \"\"\"\n",
    "        facts = []\n",
    "        \n",
    "        # Method 1: spaCy extraction\n",
    "        spacy_facts = self._extract_facts_spacy(text, doc_id)\n",
    "        facts.extend(spacy_facts)\n",
    "        \n",
    "        # Method 2: AMR extraction (if available)\n",
    "        if self.use_real_amr and self.amr_reranker.use_real_amr:\n",
    "            amr_data = self.amr_reranker._parse_to_amr(text)\n",
    "            amr_facts = self._extract_facts_from_amr(amr_data, doc_id)\n",
    "            facts.extend(amr_facts)\n",
    "        \n",
    "        # Deduplicate\n",
    "        unique_facts = list(set(facts))\n",
    "        \n",
    "        return unique_facts\n",
    "    \n",
    "    def _compute_fact_similarity(self, fact1: FactTriplet, fact2: FactTriplet) -> float:\n",
    "        \"\"\"\n",
    "        Compute similarity between two facts.\n",
    "        \n",
    "        Uses:\n",
    "        1. Entity overlap (subject/object matching)\n",
    "        2. Relation similarity\n",
    "        3. Embedding similarity\n",
    "        \"\"\"\n",
    "        if not hasattr(fact1, 'embedding'):\n",
    "            print(\"Fact1 not pre-encoded, encoding now\")\n",
    "            fact1.embedding = self._encode_text(...)\n",
    "        \n",
    "        if not hasattr(fact2, 'embedding'):\n",
    "            print(\"Fact2 not pre-encoded, encoding now\")\n",
    "            fact2.embedding = self._encode_text(...)\n",
    "    \n",
    "        # Entity overlap\n",
    "        entities1 = {fact1.subject.lower(), fact1.obj.lower()}\n",
    "        entities2 = {fact2.subject.lower(), fact2.obj.lower()}\n",
    "        entity_overlap = len(entities1 & entities2) / len(entities1 | entities2) if entities1 | entities2 else 0\n",
    "        \n",
    "        # Relation match\n",
    "        relation_match = 1.0 if fact1.relation.lower() == fact2.relation.lower() else 0.0\n",
    "        \n",
    "        # Embedding similarity\n",
    "        fact1_text = f\"{fact1.subject} {fact1.relation} {fact1.obj}\"\n",
    "        fact2_text = f\"{fact2.subject} {fact2.relation} {fact2.obj}\"\n",
    "        \n",
    "        emb1 = self._encode_text(fact1_text, encoder='fact')\n",
    "        emb2 = self._encode_text(fact2_text, encoder='fact')\n",
    "        emb_sim = cosine_similarity([emb1], [emb2])[0][0]\n",
    "        \n",
    "        # Weighted combination\n",
    "        similarity = 0.3 * entity_overlap + 0.2 * relation_match + 0.5 * emb_sim\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "    def _build_fact_graph(self, \n",
    "                     all_facts: List[FactTriplet],\n",
    "                     claim_facts: List[FactTriplet]) -> nx.Graph:\n",
    "        \"\"\"\n",
    "        Build cross-document fact graph.\n",
    "        \n",
    "        Nodes: Individual facts from all documents\n",
    "        Edges: Connect similar/related facts across documents\n",
    "        \"\"\"\n",
    "        G = nx.Graph()\n",
    "        print(f\"Pre-encoding {len(all_facts)} document facts...\")\n",
    "        for fact in all_facts:\n",
    "            fact_text = f\"{fact.subject} {fact.relation} {fact.obj}\"\n",
    "            fact.embedding = self._encode_text(fact_text, encoder='fact')\n",
    "        \n",
    "        print(f\"Pre-encoding {len(claim_facts)} claim facts...\")\n",
    "        for claim_fact in claim_facts:\n",
    "            claim_fact_text = f\"{claim_fact.subject} {claim_fact.relation} {claim_fact.obj}\"\n",
    "            claim_fact.embedding = self._encode_text(claim_fact_text, encoder='fact')\n",
    "        \n",
    "        print(f\"All facts encoded, building graph...\")\n",
    "        \n",
    "        # Add nodes (facts)\n",
    "        for i, fact in enumerate(all_facts):\n",
    "            fact_emb = fact.embedding \n",
    "\n",
    "            # Compute relevance to claim\n",
    "            claim_relevance = 0.0\n",
    "            if claim_facts:\n",
    "                claim_sims = []\n",
    "                for claim_fact in claim_facts:\n",
    "                    sim = self._compute_fact_similarity(fact, claim_fact)\n",
    "                    claim_sims.append(sim)\n",
    "                claim_relevance = max(claim_sims) if claim_sims else 0.0\n",
    "            \n",
    "            G.add_node(i,\n",
    "                      fact=fact,\n",
    "                      embedding=fact_emb,  \n",
    "                      doc_id=fact.doc_id,\n",
    "                      claim_relevance=claim_relevance)\n",
    "\n",
    "        # Add edges (fact-to-fact similarity)\n",
    "        for i in range(len(all_facts)):\n",
    "            for j in range(i+1, len(all_facts)):\n",
    "                fact_i = all_facts[i]\n",
    "                fact_j = all_facts[j]\n",
    "                \n",
    "                # Skip same document\n",
    "                if fact_i.doc_id == fact_j.doc_id:\n",
    "                    continue\n",
    "                \n",
    "                # Compute similarity \n",
    "                similarity = self._compute_fact_similarity(fact_i, fact_j)\n",
    "                \n",
    "                # Add edge if similar enough\n",
    "                if similarity > 0.4:\n",
    "                    G.add_edge(i, j, weight=similarity)\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def _build_multilevel_graph(self,\n",
    "                               documents: List[Document],\n",
    "                               claim: str) -> Dict[str, nx.Graph]:\n",
    "        \"\"\"\n",
    "        Build multi-level graph structure (GRAG key innovation).\n",
    "        \n",
    "        Returns:\n",
    "            Dict with 'amr', 'document', 'fact' graphs\n",
    "        \"\"\"\n",
    "        print(\"Building multi-level graph...\")\n",
    "        \n",
    "        # Level 1: AMR Graph\n",
    "        print(\"Level 1: AMR graph...\")\n",
    "        amr_graph = self.amr_reranker._build_amr_graph(documents, claim)\n",
    "        \n",
    "        # Level 2: Document Graph\n",
    "        print(\"Level 2: Document graph...\")\n",
    "        doc_graph = nx.Graph()\n",
    "        claim_emb = self._encode_text(claim, encoder='doc')\n",
    "        \n",
    "        doc_embeddings = {}\n",
    "        for doc in documents:\n",
    "            doc_emb = self._encode_text(doc.text[:500], encoder='doc')\n",
    "            doc_embeddings[doc.doc_id] = doc_emb\n",
    "            \n",
    "            claim_sim = cosine_similarity([claim_emb], [doc_emb])[0][0]\n",
    "            doc_graph.add_node(doc.doc_id, embedding=doc_emb, claim_sim=claim_sim)\n",
    "        \n",
    "        # Add doc-doc edges\n",
    "        for i, doc_i in enumerate(documents):\n",
    "            for doc_j in documents[i+1:]:\n",
    "                sim = cosine_similarity(\n",
    "                    [doc_embeddings[doc_i.doc_id]],\n",
    "                    [doc_embeddings[doc_j.doc_id]]\n",
    "                )[0][0]\n",
    "                if sim > 0.3:\n",
    "                    doc_graph.add_edge(doc_i.doc_id, doc_j.doc_id, weight=sim)\n",
    "        \n",
    "        # Level 3: Fact Graph\n",
    "        print(\"Level 3: Fact graph (cross-document reasoning)...\")\n",
    "        claim_facts = self._extract_facts(claim, \"claim\")\n",
    "        all_facts = []\n",
    "        for doc in documents:\n",
    "            facts = self._extract_facts(doc.text[:500], doc.doc_id)\n",
    "            all_facts.extend(facts)\n",
    "        \n",
    "        fact_graph = self._build_fact_graph(all_facts, claim_facts) if all_facts else nx.Graph()\n",
    "        \n",
    "        return {\n",
    "            'amr': amr_graph,\n",
    "            'document': doc_graph,\n",
    "            'fact': fact_graph,\n",
    "            'all_facts': all_facts\n",
    "        }\n",
    "    \n",
    "    def rerank(self,\n",
    "               claim: str,\n",
    "               documents: List[Document],\n",
    "               top_k: int = 3) -> List[RerankedResult]:\n",
    "        \"\"\"\n",
    "        Multi-stage GRAG reranking with cross-document reasoning.\n",
    "        \n",
    "        Follows GRAG paper architecture\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        print(\"  GRAG Multi-Stage Reranking:\")\n",
    "        \n",
    "        # Build multi-level graph\n",
    "        graphs = self._build_multilevel_graph(documents, claim)\n",
    "        amr_graph = graphs['amr']\n",
    "        doc_graph = graphs['document']\n",
    "        fact_graph = graphs['fact']\n",
    "        all_facts = graphs['all_facts']\n",
    "        \n",
    "        # Compute centrality scores\n",
    "        print(\"    Computing graph centrality...\")\n",
    "        try:\n",
    "            amr_centrality = nx.pagerank(amr_graph, weight='weight') if amr_graph.number_of_edges() > 0 else {}\n",
    "            doc_centrality = nx.pagerank(doc_graph, weight='weight') if doc_graph.number_of_edges() > 0 else {}\n",
    "            fact_centrality = nx.pagerank(fact_graph, weight='weight') if fact_graph.number_of_edges() > 0 else {}\n",
    "        except:\n",
    "            amr_centrality = {}\n",
    "            doc_centrality = {}\n",
    "            fact_centrality = {}\n",
    "        \n",
    "        # Aggregate fact scores per document\n",
    "        print(\"    Aggregating scores...\")\n",
    "        fact_scores = defaultdict(float)\n",
    "        if all_facts and fact_centrality:\n",
    "            for node_id, cent in fact_centrality.items():\n",
    "                if node_id in fact_graph.nodes:\n",
    "                    doc_id = fact_graph.nodes[node_id]['doc_id']\n",
    "                    claim_rel = fact_graph.nodes[node_id].get('claim_relevance', 0)\n",
    "                    # Weight by both centrality and claim relevance\n",
    "                    fact_scores[doc_id] += cent * (1 + claim_rel)\n",
    "        \n",
    "        # Normalize BM25 scores\n",
    "        max_bm25 = max(doc.score for doc in documents)\n",
    "        min_bm25 = min(doc.score for doc in documents)\n",
    "        bm25_range = max_bm25 - min_bm25 if max_bm25 != min_bm25 else 1\n",
    "        \n",
    "        # Final scoring\n",
    "        results = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            # Get all scores\n",
    "            norm_bm25 = (doc.score - min_bm25) / bm25_range\n",
    "            \n",
    "            amr_score = amr_centrality.get(doc.doc_id, 0) if doc.doc_id in amr_graph.nodes else 0\n",
    "            if doc.doc_id in amr_graph.nodes:\n",
    "                amr_score += amr_graph.nodes[doc.doc_id].get('amr_similarity', 0)\n",
    "            amr_score = min(amr_score, 1.0)\n",
    "            \n",
    "            doc_score = doc_centrality.get(doc.doc_id, 0) if doc.doc_id in doc_graph.nodes else 0\n",
    "            if doc.doc_id in doc_graph.nodes:\n",
    "                doc_score += doc_graph.nodes[doc.doc_id].get('claim_sim', 0)\n",
    "            doc_score = min(doc_score, 1.0)\n",
    "            \n",
    "            fact_score = fact_scores.get(doc.doc_id, 0)\n",
    "            # Normalize fact score\n",
    "            if fact_scores:\n",
    "                max_fact = max(fact_scores.values())\n",
    "                fact_score = fact_score / max_fact if max_fact > 0 else 0\n",
    "            \n",
    "            # Weighted combination (GRAG paper weights)\n",
    "            final_score = (\n",
    "                0.15 * norm_bm25 +      # BM25 baseline\n",
    "                0.25 * amr_score +      # AMR structural match\n",
    "                0.30 * doc_score +      # Document semantic + centrality\n",
    "                0.30 * fact_score       # Fact reasoning + centrality\n",
    "            )\n",
    "            \n",
    "            results.append(RerankedResult(\n",
    "                doc_id=doc.doc_id,\n",
    "                original_rank=i+1,\n",
    "                new_rank=0,\n",
    "                original_score=doc.score,\n",
    "                reranked_score=final_score,\n",
    "                explanation=(\n",
    "                    f\"BM25:{norm_bm25:.2f} AMR:{amr_score:.2f} \"\n",
    "                    f\"Doc:{doc_score:.2f} Fact:{fact_score:.2f}\"\n",
    "                )\n",
    "            ))\n",
    "        \n",
    "        # Sort and assign ranks\n",
    "        results.sort(key=lambda x: x.reranked_score, reverse=True)\n",
    "        for i, result in enumerate(results):\n",
    "            result.new_rank = i + 1\n",
    "        \n",
    "        return results[:top_k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading document encoder: roberta-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fact encoder: bert-base-uncased...\n",
      "Loading AMR parser...\n",
      "Loading AMR-BERT encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMR-BERT encoder loaded (RoBERTa-base on cpu)\n",
      "Loading AMR parser from local model...\n",
      "Model path: D:\\Celestia\\Projects\\646_Project\\src\\model\\amr\\model_parse_xfm_bart_large-v0_1_0\n",
      "AMR parser loaded successfully!\n",
      "  Model: STOG (String-to-Graph) - BART Large\n",
      "Loading spaCy for fact extraction...\n",
      "GRAG Reranker initialized\n",
      "\n",
      "Testing on 25 claims...\n"
     ]
    }
   ],
   "source": [
    "# Initialize GRAG reranker\n",
    "grag_reranker_s3 = GRAGReranker(\n",
    "    doc_encoder='roberta-base',\n",
    "    fact_encoder='bert-base-uncased',\n",
    "    use_real_amr=True,\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "# Test configuration\n",
    "num_test_claims = 25 \n",
    "test_batch = fever_loader.get_test_batch(start=0, n=num_test_claims)\n",
    "\n",
    "# Metrics storage\n",
    "rankings_changed = 0\n",
    "total_tested = 0\n",
    "facts_extracted = 0\n",
    "total_facts = 0\n",
    "precision_at_3 = []\n",
    "recall_at_3 = []\n",
    "average_precisions = []\n",
    "\n",
    "stage3_improvements = [] \n",
    "\n",
    "print(f\"\\nTesting on {num_test_claims} claims...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[1/25] Claim: Fox 2000 Pictures released the film Soul Food....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 15:55:01,717 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n",
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 15:57:54,114 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 16:00:37,946 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n",
      "\n",
      "2025-11-14 16:03:48,185 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 405 document facts...\n",
      "Pre-encoding 5 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 16:58:53,749 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Results:\n",
      "  BM25 top-3:     ['Soul_Food_-LRB-film-RRB-', 'List_of_20th_Century_Fox_films_-LRB-1935–99-RRB-', 'Maxine_Chadway']\n",
      "  GRAG top-3:     ['Soul_Food_-LRB-film-RRB-', 'Ways_to_Live_Forever_-LRB-film-RRB-', 'Bootmen']\n",
      "  Changed:        ✓ YES\n",
      "\n",
      "  Metrics:\n",
      "\tPrecision@3: 0.333\n",
      "\tRecall@3:    1.000\n",
      "\tAP:          1.000\n",
      "\tImprovement: 0 relevant docs in top-3\n",
      "\n",
      "  Facts Extracted:\n",
      "Claim facts: 5\n",
      "      - (r, :ARG0, c)\n",
      "      - (f, :name, n2)\n",
      "      - (c, :name, n)\n",
      "Document facts: 206 total\n",
      "(First 3: [FactTriplet(subject='f', relation=':name', obj='n', doc_id='Soul_Food_-LRB-film-RRB-', confidence=1.0, source_text=\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family ,\"), FactTriplet(subject='f2', relation=':mod', obj='c2', doc_id='Soul_Food_-LRB-film-RRB-', confidence=1.0, source_text=\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family ,\"), FactTriplet(subject='p12', relation=':name', obj='n14', doc_id='Soul_Food_-LRB-film-RRB-', confidence=1.0, source_text=\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family ,\")])\n",
      "\n",
      "  Top-3 Scores & Explanations:\n",
      "1. Soul_Food_-LRB-film-RRB-\n",
      " Score: 0.9370\n",
      "BM25:1.00 AMR:0.75 Doc:1.00 Fact:1.00\n",
      "2. Ways_to_Live_Forever_-LRB-film-RRB-\n",
      " Score: 0.7679\n",
      "BM25:0.00 AMR:0.74 Doc:1.00 Fact:0.95\n",
      "3. Bootmen\n",
      " Score: 0.7513\n",
      "BM25:0.03 AMR:0.74 Doc:1.00 Fact:0.87\n",
      "\n",
      "  Processing time: 3835.1s\n",
      "\n",
      "================================================================================\n",
      "[2/25] Claim: Fox 2000 Pictures released the film Soul Food....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 17:01:55,009 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n",
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 17:04:47,088 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 17:07:32,769 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n",
      "\n",
      "2025-11-14 17:10:33,123 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 405 document facts...\n",
      "Pre-encoding 5 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 17:59:08,223 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Results:\n",
      "  BM25 top-3:     ['Soul_Food_-LRB-film-RRB-', 'List_of_20th_Century_Fox_films_-LRB-1935–99-RRB-', 'Maxine_Chadway']\n",
      "  GRAG top-3:     ['Soul_Food_-LRB-film-RRB-', 'Ways_to_Live_Forever_-LRB-film-RRB-', 'Bootmen']\n",
      "  Changed:        ✓ YES\n",
      "\n",
      "  Metrics:\n",
      "\tPrecision@3: 0.333\n",
      "\tRecall@3:    1.000\n",
      "\tAP:          1.000\n",
      "\tImprovement: 0 relevant docs in top-3\n",
      "\n",
      "  Facts Extracted:\n",
      "Claim facts: 5\n",
      "      - (r, :ARG0, c)\n",
      "      - (f, :name, n2)\n",
      "      - (c, :name, n)\n",
      "Document facts: 206 total\n",
      "(First 3: [FactTriplet(subject='f', relation=':name', obj='n', doc_id='Soul_Food_-LRB-film-RRB-', confidence=1.0, source_text=\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family ,\"), FactTriplet(subject='f2', relation=':mod', obj='c2', doc_id='Soul_Food_-LRB-film-RRB-', confidence=1.0, source_text=\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family ,\"), FactTriplet(subject='p12', relation=':name', obj='n14', doc_id='Soul_Food_-LRB-film-RRB-', confidence=1.0, source_text=\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family ,\")])\n",
      "\n",
      "  Top-3 Scores & Explanations:\n",
      "1. Soul_Food_-LRB-film-RRB-\n",
      " Score: 0.9370\n",
      "BM25:1.00 AMR:0.75 Doc:1.00 Fact:1.00\n",
      "2. Ways_to_Live_Forever_-LRB-film-RRB-\n",
      " Score: 0.7679\n",
      "BM25:0.00 AMR:0.74 Doc:1.00 Fact:0.95\n",
      "3. Bootmen\n",
      " Score: 0.7513\n",
      "BM25:0.03 AMR:0.74 Doc:1.00 Fact:0.87\n",
      "\n",
      "  Processing time: 3427.2s\n",
      "\n",
      "================================================================================\n",
      "[3/25] Claim: Fox 2000 Pictures released the film Soul Food....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 18:02:19,365 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n",
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 18:05:16,081 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 18:08:05,068 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n",
      "\n",
      "2025-11-14 18:11:00,889 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 405 document facts...\n",
      "Pre-encoding 5 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 19:08:20,876 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Results:\n",
      "  BM25 top-3:     ['Soul_Food_-LRB-film-RRB-', 'List_of_20th_Century_Fox_films_-LRB-1935–99-RRB-', 'Maxine_Chadway']\n",
      "  GRAG top-3:     ['Soul_Food_-LRB-film-RRB-', 'Ways_to_Live_Forever_-LRB-film-RRB-', 'Bootmen']\n",
      "  Changed:        ✓ YES\n",
      "\n",
      "  Metrics:\n",
      "\tPrecision@3: 0.333\n",
      "\tRecall@3:    1.000\n",
      "\tAP:          1.000\n",
      "\tImprovement: 0 relevant docs in top-3\n",
      "\n",
      "  Facts Extracted:\n",
      "Claim facts: 5\n",
      "      - (r, :ARG0, c)\n",
      "      - (f, :name, n2)\n",
      "      - (c, :name, n)\n",
      "Document facts: 206 total\n",
      "(First 3: [FactTriplet(subject='f', relation=':name', obj='n', doc_id='Soul_Food_-LRB-film-RRB-', confidence=1.0, source_text=\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family ,\"), FactTriplet(subject='f2', relation=':mod', obj='c2', doc_id='Soul_Food_-LRB-film-RRB-', confidence=1.0, source_text=\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family ,\"), FactTriplet(subject='p12', relation=':name', obj='n14', doc_id='Soul_Food_-LRB-film-RRB-', confidence=1.0, source_text=\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family ,\")])\n",
      "\n",
      "  Top-3 Scores & Explanations:\n",
      "1. Soul_Food_-LRB-film-RRB-\n",
      " Score: 0.9370\n",
      "BM25:1.00 AMR:0.75 Doc:1.00 Fact:1.00\n",
      "2. Ways_to_Live_Forever_-LRB-film-RRB-\n",
      " Score: 0.7679\n",
      "BM25:0.00 AMR:0.74 Doc:1.00 Fact:0.95\n",
      "3. Bootmen\n",
      " Score: 0.7513\n",
      "BM25:0.03 AMR:0.74 Doc:1.00 Fact:0.87\n",
      "\n",
      "  Processing time: 3952.7s\n",
      "\n",
      "================================================================================\n",
      "[4/25] Claim: Fox 2000 Pictures released the film Soul Food....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 19:11:48,132 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n",
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 19:15:02,515 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 19:18:16,907 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n",
      "\n",
      "2025-11-14 19:21:38,724 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 405 document facts...\n",
      "Pre-encoding 5 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 20:19:11,668 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 4044.4s\n",
      "\n",
      "================================================================================\n",
      "[5/25] Claim: Fox 2000 Pictures released the film Soul Food....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 20:22:17,243 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n",
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 20:25:19,376 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 20:28:10,461 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n",
      "\n",
      "2025-11-14 20:31:06,810 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 405 document facts...\n",
      "Pre-encoding 5 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 21:20:06,582 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 3453.8s\n",
      "\n",
      "================================================================================\n",
      "[6/25] Claim: Telemundo is a English-language television network....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 21:23:15,069 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 21:24:45,386 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Missing starting paren for node s3/stand-04\n",
      "\n",
      "2025-11-14 21:25:20,388 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but date-interval is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 21:26:00,052 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but slot is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 21:27:29,531 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n",
      "\n",
      "2025-11-14 21:29:00,857 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Missing starting paren for node s3/stand-04\n",
      "\n",
      "2025-11-14 21:29:41,619 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but date-interval is not a new concept\n",
      "\n",
      "2025-11-14 21:30:25,607 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but slot is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 381 document facts...\n",
      "Pre-encoding 5 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 22:23:42,066 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 3617.3s\n",
      "\n",
      "================================================================================\n",
      "[7/25] Claim: Telemundo is a English-language television network....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 22:25:37,743 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 22:27:15,346 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Missing starting paren for node s3/stand-04\n",
      "\n",
      "2025-11-14 22:28:06,200 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but date-interval is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 22:28:53,969 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but slot is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 22:30:27,770 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n",
      "\n",
      "2025-11-14 22:32:27,116 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Missing starting paren for node s3/stand-04\n",
      "\n",
      "2025-11-14 22:33:08,260 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but date-interval is not a new concept\n",
      "\n",
      "2025-11-14 22:33:57,010 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but slot is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 381 document facts...\n",
      "Pre-encoding 5 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 23:30:10,024 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 3881.4s\n",
      "\n",
      "================================================================================\n",
      "[8/25] Claim: Telemundo is a English-language television network....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 23:32:00,727 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 23:33:48,039 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Missing starting paren for node s3/stand-04\n",
      "\n",
      "2025-11-14 23:34:32,003 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but date-interval is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 23:35:23,300 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but slot is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 23:37:07,112 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n",
      "\n",
      "2025-11-14 23:38:53,927 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Missing starting paren for node s3/stand-04\n",
      "\n",
      "2025-11-14 23:39:34,004 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but date-interval is not a new concept\n",
      "\n",
      "2025-11-14 23:40:20,508 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but slot is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 381 document facts...\n",
      "Pre-encoding 5 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 00:32:47,676 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 3643.0s\n",
      "\n",
      "================================================================================\n",
      "[9/25] Claim: Telemundo is a English-language television network....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 00:34:39,332 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 00:36:26,034 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Missing starting paren for node s3/stand-04\n",
      "\n",
      "2025-11-15 00:37:06,000 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but date-interval is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 00:37:53,968 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but slot is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 00:39:35,641 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n",
      "\n",
      "2025-11-15 00:41:22,328 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Missing starting paren for node s3/stand-04\n",
      "\n",
      "2025-11-15 00:42:06,756 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but date-interval is not a new concept\n",
      "\n",
      "2025-11-15 00:42:55,837 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but slot is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 381 document facts...\n",
      "Pre-encoding 5 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 01:43:28,076 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 4074.2s\n",
      "\n",
      "================================================================================\n",
      "[10/25] Claim: Telemundo is a English-language television network....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 01:46:05,374 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 01:48:39,431 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Missing starting paren for node s3/stand-04\n",
      "\n",
      "2025-11-15 01:49:51,676 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but date-interval is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 01:50:53,092 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but slot is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 01:53:21,171 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n",
      "\n",
      "2025-11-15 01:55:38,875 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Missing starting paren for node s3/stand-04\n",
      "\n",
      "2025-11-15 01:56:19,269 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but date-interval is not a new concept\n",
      "\n",
      "2025-11-15 01:57:15,696 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but slot is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 381 document facts...\n",
      "Pre-encoding 5 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 02:51:31,287 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but network is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 3968.6s\n",
      "\n",
      "================================================================================\n",
      "[11/25] Claim: Damon Albarn's debut album was released in 2011....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 02:52:13,934 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 02:54:48,486 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but person is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 02:56:38,059 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but record-01 is not a new concept\n",
      "\n",
      "2025-11-15 02:57:20,293 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but ordinal-entity is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 02:59:14,443 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-15 03:01:45,398 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but person is not a new concept\n",
      "\n",
      "2025-11-15 03:03:31,040 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but record-01 is not a new concept\n",
      "\n",
      "2025-11-15 03:04:10,851 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but ordinal-entity is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 439 document facts...\n",
      "Pre-encoding 5 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 04:08:12,560 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-15 04:10:20,567 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but person is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 4566.8s\n",
      "\n",
      "================================================================================\n",
      "[12/25] Claim: There is a capital called Mogadishu....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 04:11:40,155 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but government-organization is not a new concept\n",
      "\n",
      "2025-11-15 04:11:40,156 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but organization is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n",
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 04:13:33,077 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but person is not a new concept\n",
      "\n",
      "2025-11-15 04:13:33,078 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but capital is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 04:15:17,938 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but government-organization is not a new concept\n",
      "\n",
      "2025-11-15 04:15:17,940 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but organization is not a new concept\n",
      "\n",
      "2025-11-15 04:17:19,333 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but person is not a new concept\n",
      "\n",
      "2025-11-15 04:17:19,336 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but capital is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 286 document facts...\n",
      "Pre-encoding 3 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 04:45:27,107 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but government-organization is not a new concept\n",
      "\n",
      "2025-11-15 04:45:27,108 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but organization is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 2027.3s\n",
      "\n",
      "================================================================================\n",
      "[13/25] Claim: Savages was exclusively a German film....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 04:46:10,826 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but person is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 04:47:34,292 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 04:48:44,849 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but direct-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 04:49:57,057 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but person is not a new concept\n",
      "\n",
      "2025-11-15 04:51:26,064 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-15 04:52:39,565 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but direct-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 268 document facts...\n",
      "Pre-encoding 4 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 05:18:24,816 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but person is not a new concept\n",
      "\n",
      "2025-11-15 05:19:46,991 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 1931.7s\n",
      "\n",
      "================================================================================\n",
      "[14/25] Claim: Savages was exclusively a German film....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 05:20:37,654 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but person is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 05:22:01,134 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 05:23:13,591 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but direct-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 05:24:19,695 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but person is not a new concept\n",
      "\n",
      "2025-11-15 05:25:41,732 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-15 05:26:52,103 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but direct-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 268 document facts...\n",
      "Pre-encoding 4 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 05:52:16,312 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but person is not a new concept\n",
      "\n",
      "2025-11-15 05:53:38,519 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 1898.3s\n",
      "\n",
      "================================================================================\n",
      "[15/25] Claim: Andrew Kevin Walker is only Chinese....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 05:54:48,239 - penman.layout - WARNING - ignoring epigraph data for duplicate triple: ('b', ':mod', 'c')\n",
      "\n",
      "2025-11-15 05:54:48,242 - penman.layout - WARNING - ignoring epigraph data for duplicate triple: ('b', ':mod', 'c')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 05:56:57,558 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but bear-02 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 05:59:55,921 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but after is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 06:00:34,119 - penman.layout - WARNING - ignoring epigraph data for duplicate triple: ('b', ':mod', 'c')\n",
      "\n",
      "2025-11-15 06:00:34,122 - penman.layout - WARNING - ignoring epigraph data for duplicate triple: ('b', ':mod', 'c')\n",
      "\n",
      "2025-11-15 06:02:38,486 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but bear-02 is not a new concept\n",
      "\n",
      "2025-11-15 06:05:37,385 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but after is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 396 document facts...\n",
      "Pre-encoding 5 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 06:59:54,426 - penman.layout - WARNING - ignoring epigraph data for duplicate triple: ('b', ':mod', 'c')\n",
      "\n",
      "2025-11-15 06:59:54,430 - penman.layout - WARNING - ignoring epigraph data for duplicate triple: ('b', ':mod', 'c')\n",
      "\n",
      "2025-11-15 07:02:01,906 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but bear-02 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 3906.4s\n",
      "\n",
      "================================================================================\n",
      "[16/25] Claim: The Cretaceous ended....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 07:02:31,175 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-15 07:02:56,389 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 07:03:49,184 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 07:06:23,915 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-15 07:06:23,916 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but rudist is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 07:06:52,414 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-15 07:07:17,986 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n",
      "\n",
      "2025-11-15 07:08:12,154 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n",
      "\n",
      "2025-11-15 07:10:49,063 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-15 07:10:49,064 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but rudist is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 353 document facts...\n",
      "Pre-encoding 2 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 07:53:04,767 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-15 07:53:30,239 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n",
      "\n",
      "2025-11-15 07:54:19,497 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 3036.9s\n",
      "\n",
      "================================================================================\n",
      "[17/25] Claim: The Cretaceous ended....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 07:54:53,305 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-15 07:55:19,716 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 07:56:09,602 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 07:58:45,463 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-15 07:58:45,464 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but rudist is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 07:59:17,605 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-15 07:59:46,583 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n",
      "\n",
      "2025-11-15 08:00:40,888 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n",
      "\n",
      "2025-11-15 08:03:13,881 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-15 08:03:13,883 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but rudist is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 353 document facts...\n",
      "Pre-encoding 2 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 08:46:10,049 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-15 08:46:35,494 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n",
      "\n",
      "2025-11-15 08:47:24,379 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 3075.8s\n",
      "\n",
      "================================================================================\n",
      "[18/25] Claim: The Cretaceous ended....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 08:48:01,223 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-15 08:48:29,425 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 08:49:21,391 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 08:52:10,790 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-15 08:52:10,791 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but rudist is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 08:52:41,293 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-15 08:53:09,154 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n",
      "\n",
      "2025-11-15 08:53:59,712 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n",
      "\n",
      "2025-11-15 08:56:43,384 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-15 08:56:43,388 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but rudist is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 353 document facts...\n",
      "Pre-encoding 2 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 09:37:53,711 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-15 09:38:18,632 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n",
      "\n",
      "2025-11-15 09:39:08,463 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 2995.4s\n",
      "\n",
      "================================================================================\n",
      "[19/25] Claim: The Cretaceous ended....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 09:39:48,608 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-15 09:40:13,301 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 09:41:01,794 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 09:43:33,260 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-15 09:43:33,261 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but rudist is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 09:44:01,841 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-15 09:44:27,632 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n",
      "\n",
      "2025-11-15 09:45:17,527 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n",
      "\n",
      "2025-11-15 09:47:51,707 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-15 09:47:51,708 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but rudist is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 353 document facts...\n",
      "Pre-encoding 2 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 10:28:49,201 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-15 10:29:17,507 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n",
      "\n",
      "2025-11-15 10:30:02,832 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 2947.1s\n",
      "\n",
      "================================================================================\n",
      "[20/25] Claim: The Cretaceous ended....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 10:30:40,969 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-15 10:31:05,841 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 10:31:51,777 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 10:34:24,044 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-15 10:34:24,045 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but rudist is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 10:34:53,424 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-15 10:35:18,259 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n",
      "\n",
      "2025-11-15 10:36:10,771 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n",
      "\n",
      "2025-11-15 10:38:44,705 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but and is not a new concept\n",
      "\n",
      "2025-11-15 10:38:44,706 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but rudist is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 353 document facts...\n",
      "Pre-encoding 2 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 11:18:41,451 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but extinct is not a new concept\n",
      "\n",
      "2025-11-15 11:19:04,419 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but series is not a new concept\n",
      "\n",
      "2025-11-15 11:19:50,198 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but include-91 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 2887.2s\n",
      "\n",
      "================================================================================\n",
      "[21/25] Claim: Murda Beatz's real name is Marshall Mathers....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 11:20:28,258 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but ordinal-entity is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 11:23:00,185 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but feature-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 11:24:04,510 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but feature-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 11:25:44,246 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but ordinal-entity is not a new concept\n",
      "\n",
      "2025-11-15 11:28:20,473 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but feature-01 is not a new concept\n",
      "\n",
      "2025-11-15 11:29:22,749 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but feature-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 380 document facts...\n",
      "Pre-encoding 4 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 12:15:53,157 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but ordinal-entity is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 3325.1s\n",
      "\n",
      "================================================================================\n",
      "[22/25] Claim: Nicholas Brody is a character on Homeland....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 12:20:27,673 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-15 12:20:27,673 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 12:24:56,416 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-15 12:24:56,417 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 370 document facts...\n",
      "Pre-encoding 3 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 13:11:07,439 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-15 13:11:07,440 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 3032.2s\n",
      "\n",
      "================================================================================\n",
      "[23/25] Claim: Nicholas Brody is a character on Homeland....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 13:13:48,437 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-15 13:13:48,439 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 13:17:58,337 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-15 13:17:58,338 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 370 document facts...\n",
      "Pre-encoding 3 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 16:11:14,207 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-15 16:11:14,208 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 4440.2s\n",
      "\n",
      "================================================================================\n",
      "[24/25] Claim: Nicholas Brody is a character on Homeland....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 16:14:02,423 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-15 16:14:02,424 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 16:18:18,767 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-15 16:18:18,768 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 370 document facts...\n",
      "Pre-encoding 3 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 17:06:10,426 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-15 17:06:10,428 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 3114.6s\n",
      "\n",
      "================================================================================\n",
      "[25/25] Claim: Nicholas Brody is a character on Homeland....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n",
      "  Progress: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 17:09:04,328 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-15 17:09:04,330 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 7/10\n",
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 17:13:14,079 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-15 17:13:14,080 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding 370 document facts...\n",
      "Pre-encoding 3 claim facts...\n",
      "All facts encoded, building graph...\n",
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-15 17:57:30,300 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but compose-02 is not a new concept\n",
      "\n",
      "2025-11-15 17:57:30,301 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but work-of-art is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed in 2893.0s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "total_time = 0\n",
    "\n",
    "for idx, item in enumerate(test_batch):\n",
    "    claim = item['claim_data']['claim']\n",
    "    claim_id = str(item['claim_data']['id'])\n",
    "    bm25_docs = item['bm25_docs']\n",
    "    \n",
    "    if len(bm25_docs) < 3:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[{idx+1}/{num_test_claims}] Claim: {claim[:70]}...\")\n",
    "    print('='*80)\n",
    "    \n",
    "    # Track time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Rerank with GRAG\n",
    "    reranked = grag_reranker_s3.rerank(claim, bm25_docs, top_k=10)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    total_time += elapsed\n",
    "    \n",
    "    # Compare rankings\n",
    "    original_top3 = [doc.doc_id for doc in bm25_docs[:3]]\n",
    "    reranked_top3 = [r.doc_id for r in reranked[:3]]\n",
    "    \n",
    "    changed = original_top3 != reranked_top3\n",
    "    if changed:\n",
    "        rankings_changed += 1\n",
    "    \n",
    "    # Count facts extracted\n",
    "    try:\n",
    "        claim_facts = grag_reranker_s3._extract_facts(claim, \"claim\")\n",
    "        doc_facts = []\n",
    "        for doc in bm25_docs[:5]:  # Check first 5 docs\n",
    "            facts = grag_reranker_s3._extract_facts(doc.text[:500], doc.doc_id)\n",
    "            doc_facts.extend(facts)\n",
    "        \n",
    "        if claim_facts or doc_facts:\n",
    "            facts_extracted += 1\n",
    "        total_facts += len(doc_facts)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Compute retrieval metrics\n",
    "    if fever_loader.qrels and claim_id in fever_loader.qrels:\n",
    "        relevant_docs = set(fever_loader.qrels[claim_id].keys())\n",
    "        reranked_ids = [r.doc_id for r in reranked]\n",
    "        \n",
    "        # Precision@3\n",
    "        top3_relevant = len(set(reranked_ids[:3]) & relevant_docs)\n",
    "        p_at_3 = top3_relevant / 3\n",
    "        precision_at_3.append(p_at_3)\n",
    "        \n",
    "        # Recall@3\n",
    "        r_at_3 = top3_relevant / len(relevant_docs) if relevant_docs else 0\n",
    "        recall_at_3.append(r_at_3)\n",
    "        \n",
    "        # Average Precision\n",
    "        num_relevant_seen = 0\n",
    "        precisions_at_relevant = []\n",
    "        for i, doc_id in enumerate(reranked_ids, 1):\n",
    "            if doc_id in relevant_docs:\n",
    "                num_relevant_seen += 1\n",
    "                precisions_at_relevant.append(num_relevant_seen / i)\n",
    "        \n",
    "        ap = np.mean(precisions_at_relevant) if precisions_at_relevant else 0\n",
    "        average_precisions.append(ap)\n",
    "        \n",
    "        # Track improvement\n",
    "        original_top3_relevant = len(set(original_top3) & relevant_docs)\n",
    "        improvement = top3_relevant - original_top3_relevant\n",
    "        stage3_improvements.append(improvement)\n",
    "    \n",
    "    total_tested += 1\n",
    "    \n",
    "    # Print results for first 3 examples\n",
    "    if idx < 3:\n",
    "        print(f\"\\n{'─'*70}\")\n",
    "        print(f\"Results:\")\n",
    "        print(f\"  BM25 top-3:     {original_top3}\")\n",
    "        print(f\"  GRAG top-3:     {reranked_top3}\")\n",
    "        print(f\"  Changed:        {'✓ YES' if changed else '✗ NO'}\")\n",
    "        \n",
    "        if fever_loader.qrels and claim_id in fever_loader.qrels:\n",
    "            print(f\"\\n  Metrics:\")\n",
    "            print(f\"\\tPrecision@3: {p_at_3:.3f}\")\n",
    "            print(f\"\\tRecall@3:    {r_at_3:.3f}\")\n",
    "            print(f\"\\tAP:          {ap:.3f}\")\n",
    "            print(f\"\\tImprovement: {'+' if improvement > 0 else ''}{improvement} relevant docs in top-3\")\n",
    "        \n",
    "        print(f\"\\n  Facts Extracted:\")\n",
    "        if claim_facts:\n",
    "            print(f\"Claim facts: {len(claim_facts)}\")\n",
    "            for f in claim_facts[:3]:\n",
    "                print(f\"      - {f}\")\n",
    "        if doc_facts:\n",
    "            print(f\"Document facts: {len(doc_facts)} total\")\n",
    "            print(f\"(First 3: {doc_facts[:3]})\")\n",
    "        \n",
    "        print(f\"\\n  Top-3 Scores & Explanations:\")\n",
    "        for r in reranked[:3]:\n",
    "            print(f\"{r.new_rank}. {r.doc_id}\")\n",
    "            print(f\" Score: {r.reranked_score:.4f}\")\n",
    "            print(f\"{r.explanation}\")\n",
    "        \n",
    "        print(f\"\\n  Processing time: {elapsed:.1f}s\")\n",
    "    else:\n",
    "        print(f\" Processed in {elapsed:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[1/25] Claim: Fox 2000 Pictures released the film Soul Food....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 11:24:43,980 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n",
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 11:27:42,238 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 11:30:27,973 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n",
      "\n",
      "2025-11-14 11:33:18,858 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Computing graph centrality...\n",
      "    Aggregating scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 15:29:39,542 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "Results:\n",
      "  BM25 top-3:     ['Soul_Food_-LRB-film-RRB-', 'List_of_20th_Century_Fox_films_-LRB-1935–99-RRB-', 'Maxine_Chadway']\n",
      "  GRAG top-3:     ['Soul_Food_-LRB-film-RRB-', 'Ways_to_Live_Forever_-LRB-film-RRB-', 'Bootmen']\n",
      "  Changed:        ✓ YES\n",
      "\n",
      "  Metrics:\n",
      "\tPrecision@3: 0.333\n",
      "\tRecall@3:    1.000\n",
      "\tAP:          1.000\n",
      "\tImprovement: 0 relevant docs in top-3\n",
      "\n",
      "  Facts Extracted:\n",
      "Claim facts: 5\n",
      "      - (r, :ARG0, c)\n",
      "      - (f, :name, n2)\n",
      "      - (c, :name, n)\n",
      "Document facts: 206 total\n",
      "(First 3: [FactTriplet(subject='f', relation=':name', obj='n', doc_id='Soul_Food_-LRB-film-RRB-', confidence=1.0, source_text=\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family ,\"), FactTriplet(subject='f2', relation=':mod', obj='c2', doc_id='Soul_Food_-LRB-film-RRB-', confidence=1.0, source_text=\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family ,\"), FactTriplet(subject='p12', relation=':name', obj='n14', doc_id='Soul_Food_-LRB-film-RRB-', confidence=1.0, source_text=\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family ,\")])\n",
      "\n",
      "  Top-3 Scores & Explanations:\n",
      "1. Soul_Food_-LRB-film-RRB-\n",
      " Score: 0.9370\n",
      "BM25:1.00 AMR:0.75 Doc:1.00 Fact:1.00\n",
      "2. Ways_to_Live_Forever_-LRB-film-RRB-\n",
      " Score: 0.7679\n",
      "BM25:0.00 AMR:0.74 Doc:1.00 Fact:0.95\n",
      "3. Bootmen\n",
      " Score: 0.7513\n",
      "BM25:0.03 AMR:0.74 Doc:1.00 Fact:0.87\n",
      "\n",
      "  Processing time: 14709.2s\n",
      "\n",
      "================================================================================\n",
      "[2/25] Claim: Fox 2000 Pictures released the film Soul Food....\n",
      "================================================================================\n",
      "  GRAG Multi-Stage Reranking:\n",
      "Building multi-level graph...\n",
      "Level 1: AMR graph...\n",
      "  Parsing claim to AMR...\n",
      "  Parsing 10 documents...\n",
      "  Progress: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 15:32:46,571 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 4/10\n",
      "  Progress: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 15:35:32,527 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 10/10\n",
      "Level 2: Document graph...\n",
      "Level 3: Fact graph (cross-document reasoning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-11-14 15:38:11,849 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but list-01 is not a new concept\n",
      "\n",
      "2025-11-14 15:41:00,123 - amrlib.models.parse_xfm.penman_serializer - WARNING - gid=x Start paren present but film is not a new concept\n"
     ]
    }
   ],
   "source": [
    "# ## Left for reference \n",
    "\n",
    "# Results:\n",
    "#   BM25 top-3:     ['Soul_Food_-LRB-film-RRB-', 'List_of_20th_Century_Fox_films_-LRB-1935–99-RRB-', 'Maxine_Chadway']\n",
    "#   GRAG top-3:     ['Soul_Food_-LRB-film-RRB-', 'Ways_to_Live_Forever_-LRB-film-RRB-', 'Bootmen']\n",
    "#   Changed:        ✓ YES\n",
    "\n",
    "#   Metrics:\n",
    "# \tPrecision@3: 0.333\n",
    "# \tRecall@3:    1.000\n",
    "# \tAP:          1.000\n",
    "# \tImprovement: 0 relevant docs in top-3\n",
    "\n",
    "#   Facts Extracted:\n",
    "# Claim facts: 5\n",
    "#       - (r, :ARG0, c)\n",
    "#       - (f, :name, n2)\n",
    "#       - (c, :name, n)\n",
    "# Document facts: 206 total\n",
    "# (First 3: [FactTriplet(subject='f', relation=':name', obj='n', doc_id='Soul_Food_-LRB-film-RRB-', confidence=1.0, source_text=\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family ,\"), FactTriplet(subject='f2', relation=':mod', obj='c2', doc_id='Soul_Food_-LRB-film-RRB-', confidence=1.0, source_text=\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family ,\"), FactTriplet(subject='p12', relation=':name', obj='n14', doc_id='Soul_Food_-LRB-film-RRB-', confidence=1.0, source_text=\"Soul Food is a 1997 American comedy-drama film produced by Kenneth `` Babyface '' Edmonds , Tracey Edmonds and Robert Teitel and released by Fox 2000 Pictures . Featuring an ensemble cast , the film stars Vanessa L. Williams , Vivica A. Fox , Nia Long , Michael Beach , Mekhi Phifer , Jeffrey D. Sams , Irma P. Hall , Gina Ravera and Brandon Hammond . Written and directed by George Tillman , Jr. -- in his major studio debut -- the film centers on the trials of an extended African-American family ,\")])\n",
    "\n",
    "#   Top-3 Scores & Explanations:\n",
    "# 1. Soul_Food_-LRB-film-RRB-\n",
    "#  Score: 0.9370\n",
    "# BM25:1.00 AMR:0.75 Doc:1.00 Fact:1.00\n",
    "# 2. Ways_to_Live_Forever_-LRB-film-RRB-\n",
    "#  Score: 0.7679\n",
    "# BM25:0.00 AMR:0.74 Doc:1.00 Fact:0.95\n",
    "# 3. Bootmen\n",
    "#  Score: 0.7513\n",
    "# BM25:0.03 AMR:0.74 Doc:1.00 Fact:0.87\n",
    "\n",
    "#   Processing time: 14709.2s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic Statistics:\n",
      "  Claims tested: 25\n",
      "  Rankings changed: 25 (100.0%)\n",
      "  Facts extracted: 25/25 claims (100.0%)\n",
      "  Total facts: 4445 (avg: 177.8 per claim)\n",
      "  Total time: 83975.5s\n",
      "  Avg time per claim: 3359.0s\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBasic Statistics:\")\n",
    "print(f\"  Claims tested: {total_tested}\")\n",
    "print(f\"  Rankings changed: {rankings_changed} ({rankings_changed/total_tested*100:.1f}%)\")\n",
    "print(f\"  Facts extracted: {facts_extracted}/{total_tested} claims ({facts_extracted/total_tested*100:.1f}%)\")\n",
    "print(f\"  Total facts: {total_facts} (avg: {total_facts/total_tested:.1f} per claim)\")\n",
    "print(f\"  Total time: {total_time:.1f}s\")\n",
    "print(f\"  Avg time per claim: {total_time/total_tested:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval Metrics:\n",
      "  Precision@3: 0.200 ± 0.163\n",
      "  Recall@3: 0.520 ± 0.458\n",
      "  MAP: 0.479 ± 0.431\n",
      "\n",
      "Improvement over BM25:\n",
      "  Avg change in relevant docs (top-3): +0.04\n",
      "  Improved:   1/25 (4.0%)\n",
      "  Unchanged:  24/25 (96.0%)\n",
      "  Degraded:   0/25 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "if precision_at_3:\n",
    "    print(f\"\\nRetrieval Metrics:\")\n",
    "    print(f\"  Precision@3: {np.mean(precision_at_3):.3f} ± {np.std(precision_at_3):.3f}\")\n",
    "    print(f\"  Recall@3: {np.mean(recall_at_3):.3f} ± {np.std(recall_at_3):.3f}\")\n",
    "    print(f\"  MAP: {np.mean(average_precisions):.3f} ± {np.std(average_precisions):.3f}\")\n",
    "    \n",
    "    if stage3_improvements:\n",
    "        avg_improvement = np.mean(stage3_improvements)\n",
    "        print(f\"\\nImprovement over BM25:\")\n",
    "        print(f\"  Avg change in relevant docs (top-3): {avg_improvement:+.2f}\")\n",
    "        improved = sum(1 for x in stage3_improvements if x > 0)\n",
    "        unchanged = sum(1 for x in stage3_improvements if x == 0)\n",
    "        degraded = sum(1 for x in stage3_improvements if x < 0)\n",
    "        print(f\"  Improved:   {improved}/{len(stage3_improvements)} ({improved/len(stage3_improvements)*100:.1f}%)\")\n",
    "        print(f\"  Unchanged:  {unchanged}/{len(stage3_improvements)} ({unchanged/len(stage3_improvements)*100:.1f}%)\")\n",
    "        print(f\"  Degraded:   {degraded}/{len(stage3_improvements)} ({degraded/len(stage3_improvements)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 4: GRAG + Custom Weight Functions\n",
    "\n",
    "**Extension**: Plugin architecture for custom knowledge graph weighting.\n",
    "\n",
    "**Weight Functions Defined**:\n",
    "1. **Temporal weighting**: Prioritize recent facts\n",
    "2. **Coherence**: Reward mutually supporting facts and penalizes contradictory facts\n",
    "3. **Source reliability**: If metadata available, weight by source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalWeightMetric:\n",
    "    \"\"\"Relative recency within document set.\n",
    "    \n",
    "    Normalizes dates relative to the retrieved documents:\n",
    "    - Newest doc in set: 1.5x\n",
    "    - Oldest doc in set: 0.7x\n",
    "    - Linear interpolation between.\"\"\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsensusWeightMetric:\n",
    "    \"\"\"Consensus detection with contradiction penalty.\n",
    "    \n",
    "    - Facts supported by many similar facts: boosted\n",
    "    - Facts that contradict neighbours: penalised (but not removed)\n",
    "    - Isolated facts: neutral\n",
    "    - Threshold to define similarity between the encodings\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the source of the document\n",
    "## Only possible if we go with duckduckgo api extension\n",
    "### Will not work on further beyond the following framework as that extension is not added yet\n",
    "\n",
    "class SourceCredibilityWeight:\n",
    "    \"\"\"\n",
    "    Rules on what makes a website credible (stanford) + LLM ambiguity\n",
    "    \n",
    "    Combines:\n",
    "    - Fast rule-based filtering (whitelist/blacklist)\n",
    "    - LLM evaluation for ambiguous cases\n",
    "    - Confidence weighting between the two\n",
    "    \n",
    "    Use case: Best of both worlds - fast + nuanced\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRAGWithWeights(GRAGReranker):\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation \n",
    "Testing successful approaches with the generation aspect of QwenB as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 : basic_reranker_s1\n",
    "2 : amr_reranker_s2\n",
    "3 : grag_reranker_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline with Qwen-8B\n",
    "from src.model_clients import LlamaCppClient\n",
    "from src.ragar_corag import RagarCorag\n",
    "\n",
    "# Setup Qwen-8B\n",
    "mc = LlamaCppClient(user_prompts_dir, sys_prompts_dir)\n",
    "\n",
    "# Setup CoRAG with reranker\n",
    "corag = RagarCorag(\n",
    "    mc=mc,\n",
    "    use_reranker=True,\n",
    "    reranker_stage='grag'\n",
    ")\n",
    "\n",
    "# Run full pipeline\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for item in test_batch:\n",
    "    claim = item['claim_data']['claim']\n",
    "    true_label = item['claim_data']['label']\n",
    "    \n",
    "    # Full pipeline: BM25 → Rerank → Qwen-8B → Verdict\n",
    "    result = corag.run(claim)\n",
    "    verdict = result['verdict']  # 0=REFUTES, 1=SUPPORTS, 2=NEI\n",
    "    \n",
    "    fever_labels = [\"REFUTES\", \"SUPPORTS\", \"NOT ENOUGH INFO\"]\n",
    "    pred_label = fever_labels[verdict] if verdict is not None else \"NEI\"\n",
    "    \n",
    "    predictions.append(pred_label)\n",
    "    labels.append(true_label)\n",
    "\n",
    "# Compute classification metrics\n",
    "accuracy = sum(p == l for p, l in zip(predictions, labels)) / len(labels)\n",
    "print(f\"End-to-end Accuracy: {accuracy:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
